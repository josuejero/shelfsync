This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
.github/
  ISSUE_TEMPLATE/
    bug_report.yml
    feature_request.yml
  workflows/
    ci.yml
    codeql.yml
  dependabot.yml
  PULL_REQUEST_TEMPLATE.md
apps/
  web/
    public/
      .gitkeep
    src/
      app/
        api/
          proxy/
            sse/
              route.ts
        books/
          [id]/
            page.tsx
        dashboard/
          matches/
            page.tsx
          page.tsx
        notifications/
          page.tsx
        settings/
          goodreads/
            page.tsx
          page.tsx
        signin/
          page.tsx
        signup/
          page.tsx
        globals.css
        layout.tsx
        page.test.tsx
        page.tsx
      components/
        AuthGuard.tsx
        NotificationBell.tsx
        TryDemoButton.tsx
      hooks/
        useSyncRun.ts
      lib/
        api.ts
        readNext.ts
        syncRuns.ts
      test/
        msw/
          handlers.ts
          server.ts
        setup.ts
    test-results/
      .last-run.json
    .env.local.example
    .eslintrc.json
    next-env.d.ts
    next.config.mjs
    package.json
    postcss.config.js
    tailwind.config.js
    tsconfig.json
    vercel.json
    vitest.config.ts
docs/
  adr/
    0001-tech-stack.md
    0002-goodreads-ingestion.md
  data-quality/
    issue-log/
      data_quality_issue_log_example.csv
      data_quality_issue_log_template.csv
      README.md
      taxonomy.md
  openapi/
    goodreads-mock.yaml
  architecture.md
infra/
  docker-compose.full.yml
  docker-compose.prod.yml
  docker-compose.yml
  goodreads-mock.yaml
mock/
  goodreads/
    export.csv
    shelf.xml
scripts/
  dev-env.sh
  dev.sh
services/
  api/
    alembic/
      versions/
        06ccc00c800a_phase7_notifications.py
        156614e6d7d6_phase4_seed_libraries.py
        32191e629780_add_sync_runs_table.py
        7b7d94f4f3e1_create_sync_runs_and_notification_events.py
        8399dceac96e_phase1_init_schema.py
        8f008b42145c_phase2_ingestion_fields.py
        b2c2190162bb_phase3_catalog_matching.py
        bedca43b8c03_phase3_catalog_matching.py
        c7a5b2b4af1e_add_missing_shelf_columns.py
      env.py
      README
      script.py.mako
    app/
      api/
        routes/
          auth.py
          books.py
          dashboard.py
          health.py
          libraries.py
          matching.py
          notifications.py
          settings.py
          shelf_items.py
          shelf_sources.py
          sync_runs.py
        __init__.py
        deps.py
        rate_limit.py
        router.py
      core/
        config.py
        otel.py
        redis_client.py
        redis.py
        security.py
      crud/
        availability.py
        catalog_matches.py
        notifications.py
        shelf_items.py
        sync_runs.py
      db/
        session.py
      domain/
        normalize.py
      fixtures/
        catalog_fixture.json
      ingestion/
        csv_import.py
        fetch.py
        rss.py
      middleware/
        request_id.py
      models/
        __init__.py
        availability_snapshot.py
        base.py
        catalog_item.py
        catalog_match.py
        library.py
        notification_event.py
        shelf_item.py
        shelf_source.py
        sync_run.py
        user_settings.py
        user.py
      providers/
        factory.py
        fixture_provider.py
        types.py
      schemas/
        auth.py
        books.py
        dashboard.py
        library.py
        matching.py
        notifications.py
        settings.py
        shelf.py
        sync_run.py
      services/
        catalog/
          factory.py
          fixture_provider.py
          overdrive_provider.py
          provider.py
          types.py
        matching/
          matcher.py
          persist.py
        read_next/
          scoring.py
        availability_cache.py
        goodreads_csv.py
        goodreads_rss.py
        import_service.py
        normalization.py
        read_next_scoring.py
        shelf_import.py
      workers/
        async_utils.py
        events.py
        jobs.py
        queue.py
        redis_conn.py
      __init__.py
      main.py
    bin/
      alembic
    tests/
      conftest.py
      test_auth.py
      test_goodreads_csv_parser.py
      test_goodreads_rss_parser.py
      test_health.py
      test_matching.py
      test_notifications.py
      test_read_next_scoring.py
      test_shelf_import_idempotent.py
      test_sync_runs.py
    .coverage
    .dockerignore
    .env.example
    alembic.ini
    Dockerfile
    requirements-dev.txt
    requirements.txt
shelfsync-api/
  src/
    index.ts
  test/
    env.d.ts
    index.spec.ts
    tsconfig.json
  .editorconfig
  .gitignore
  .prettierrc
  package.json
  tsconfig.json
  vitest.config.mts
  wrangler.jsonc
.editorconfig
.gitignore
.nvmrc
.python-version
.repomixignore
CODE_OF_CONDUCT.md
cookies.txt
LICENSE
Makefile
pyproject.toml
README.md
SECURITY.md
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="shelfsync-api/src/index.ts">
/**
 * Welcome to Cloudflare Workers! This is your first worker.
 *
 * - Run `npm run dev` in your terminal to start a development server
 * - Open a browser tab at http://localhost:8787/ to see your worker in action
 * - Run `npm run deploy` to publish your worker
 *
 * Bind resources to your worker in `wrangler.jsonc`. After adding bindings, a type definition for the
 * `Env` object can be regenerated with `npm run cf-typegen`.
 *
 * Learn more at https://developers.cloudflare.com/workers/
 */

export default {
	async fetch(request, env, ctx): Promise<Response> {
		return new Response('Hello World!');
	},
} satisfies ExportedHandler<Env>;
</file>

<file path="shelfsync-api/test/env.d.ts">
declare module 'cloudflare:test' {
	interface ProvidedEnv extends Env {}
}
</file>

<file path="shelfsync-api/test/index.spec.ts">
import { env, createExecutionContext, waitOnExecutionContext, SELF } from 'cloudflare:test';
import { describe, it, expect } from 'vitest';
import worker from '../src/index';

// For now, you'll need to do something like this to get a correctly-typed
// `Request` to pass to `worker.fetch()`.
const IncomingRequest = Request<unknown, IncomingRequestCfProperties>;

describe('Hello World worker', () => {
	it('responds with Hello World! (unit style)', async () => {
		const request = new IncomingRequest('http://example.com');
		// Create an empty context to pass to `worker.fetch()`.
		const ctx = createExecutionContext();
		const response = await worker.fetch(request, env, ctx);
		// Wait for all `Promise`s passed to `ctx.waitUntil()` to settle before running test assertions
		await waitOnExecutionContext(ctx);
		expect(await response.text()).toMatchInlineSnapshot(`"Hello World!"`);
	});

	it('responds with Hello World! (integration style)', async () => {
		const response = await SELF.fetch('https://example.com');
		expect(await response.text()).toMatchInlineSnapshot(`"Hello World!"`);
	});
});
</file>

<file path="shelfsync-api/test/tsconfig.json">
{
	"extends": "../tsconfig.json",
	"compilerOptions": {
		"types": ["@cloudflare/vitest-pool-workers"]
	},
	"include": ["./**/*.ts", "../worker-configuration.d.ts"],
	"exclude": []
}
</file>

<file path="shelfsync-api/.editorconfig">
# http://editorconfig.org
root = true

[*]
indent_style = tab
end_of_line = lf
charset = utf-8
trim_trailing_whitespace = true
insert_final_newline = true

[*.yml]
indent_style = space
</file>

<file path="shelfsync-api/.gitignore">
# Logs

logs
_.log
npm-debug.log_
yarn-debug.log*
yarn-error.log*
lerna-debug.log*
.pnpm-debug.log*

# Diagnostic reports (https://nodejs.org/api/report.html)

report.[0-9]_.[0-9]_.[0-9]_.[0-9]_.json

# Runtime data

pids
_.pid
_.seed
\*.pid.lock

# Directory for instrumented libs generated by jscoverage/JSCover

lib-cov

# Coverage directory used by tools like istanbul

coverage
\*.lcov

# nyc test coverage

.nyc_output

# Grunt intermediate storage (https://gruntjs.com/creating-plugins#storing-task-files)

.grunt

# Bower dependency directory (https://bower.io/)

bower_components

# node-waf configuration

.lock-wscript

# Compiled binary addons (https://nodejs.org/api/addons.html)

build/Release

# Dependency directories

node_modules/
jspm_packages/

# Snowpack dependency directory (https://snowpack.dev/)

web_modules/

# TypeScript cache

\*.tsbuildinfo

# Optional npm cache directory

.npm

# Optional eslint cache

.eslintcache

# Optional stylelint cache

.stylelintcache

# Microbundle cache

.rpt2_cache/
.rts2_cache_cjs/
.rts2_cache_es/
.rts2_cache_umd/

# Optional REPL history

.node_repl_history

# Output of 'npm pack'

\*.tgz

# Yarn Integrity file

.yarn-integrity

# parcel-bundler cache (https://parceljs.org/)

.cache
.parcel-cache

# Next.js build output

.next
out

# Nuxt.js build / generate output

.nuxt
dist

# Gatsby files

.cache/

# Comment in the public line in if your project uses Gatsby and not Next.js

# https://nextjs.org/blog/next-9-1#public-directory-support

# public

# vuepress build output

.vuepress/dist

# vuepress v2.x temp and cache directory

.temp
.cache

# Docusaurus cache and generated files

.docusaurus

# Serverless directories

.serverless/

# FuseBox cache

.fusebox/

# DynamoDB Local files

.dynamodb/

# TernJS port file

.tern-port

# Stores VSCode versions used for testing VSCode extensions

.vscode-test

# yarn v2

.yarn/cache
.yarn/unplugged
.yarn/build-state.yml
.yarn/install-state.gz
.pnp.\*

# wrangler project

.dev.vars*
!.dev.vars.example
.env*
!.env.example
.wrangler/
</file>

<file path="shelfsync-api/.prettierrc">
{
	"printWidth": 140,
	"singleQuote": true,
	"semi": true,
	"useTabs": true
}
</file>

<file path="shelfsync-api/package.json">
{
	"name": "shelfsync-api",
	"version": "0.0.0",
	"private": true,
	"scripts": {
		"deploy": "wrangler deploy",
		"dev": "wrangler dev",
		"start": "wrangler dev",
		"test": "vitest",
		"cf-typegen": "wrangler types"
	},
	"devDependencies": {
		"@cloudflare/vitest-pool-workers": "^0.12.4",
		"typescript": "^5.5.2",
		"vitest": "~3.2.0",
		"wrangler": "4.61.1"
	}
}
</file>

<file path="shelfsync-api/tsconfig.json">
{
	"compilerOptions": {
		/* Visit https://aka.ms/tsconfig.json to read more about this file */

		/* Set the JavaScript language version for emitted JavaScript and include compatible library declarations. */
		"target": "es2024",
		/* Specify a set of bundled library declaration files that describe the target runtime environment. */
		"lib": ["es2024"],
		/* Specify what JSX code is generated. */
		"jsx": "react-jsx",

		/* Specify what module code is generated. */
		"module": "es2022",
		/* Specify how TypeScript looks up a file from a given module specifier. */
		"moduleResolution": "Bundler",
		/* Enable importing .json files */
		"resolveJsonModule": true,

		/* Allow JavaScript files to be a part of your program. Use the `checkJS` option to get errors from these files. */
		"allowJs": true,
		/* Enable error reporting in type-checked JavaScript files. */
		"checkJs": false,

		/* Disable emitting files from a compilation. */
		"noEmit": true,

		/* Ensure that each file can be safely transpiled without relying on other imports. */
		"isolatedModules": true,
		/* Allow 'import x from y' when a module doesn't have a default export. */
		"allowSyntheticDefaultImports": true,
		/* Ensure that casing is correct in imports. */
		"forceConsistentCasingInFileNames": true,

		/* Enable all strict type-checking options. */
		"strict": true,

		/* Skip type checking all .d.ts files. */
		"skipLibCheck": true
	},
	"exclude": ["test"],
	"include": ["worker-configuration.d.ts", "src/**/*.ts"]
}
</file>

<file path="shelfsync-api/vitest.config.mts">
import { defineWorkersConfig } from '@cloudflare/vitest-pool-workers/config';

export default defineWorkersConfig({
	test: {
		poolOptions: {
			workers: {
				wrangler: { configPath: './wrangler.jsonc' },
			},
		},
	},
});
</file>

<file path="shelfsync-api/wrangler.jsonc">
{
  "name": "shelfsync-api",
  "main": "src/index.ts",
  "compatibility_date": "2026-01-30"
}
</file>

<file path=".repomixignore">
tsconfig.tsbuildinfo
</file>

<file path=".github/ISSUE_TEMPLATE/bug_report.yml">
name: Bug report
description: Report a problem
body:
  - type: textarea
    id: repro
    attributes:
      label: Repro steps
      description: Steps to reproduce
    validations:
      required: true
  - type: textarea
    id: expected
    attributes:
      label: Expected
    validations:
      required: true
  - type: textarea
    id: actual
    attributes:
      label: Actual
    validations:
      required: true
</file>

<file path=".github/ISSUE_TEMPLATE/feature_request.yml">
name: Feature request
description: Propose a feature for ShelfSync
body:
  - type: textarea
    id: problem
    attributes:
      label: Problem
      description: What user problem does this solve?
    validations:
      required: true
  - type: textarea
    id: proposal
    attributes:
      label: Proposal
      description: What should we build?
    validations:
      required: true
</file>

<file path=".github/workflows/codeql.yml">
name: CodeQL

on:
  push:
    branches: [main]
  pull_request:
  schedule:
    - cron: "0 6 * * 1"

permissions:
  contents: read

jobs:
  analyze:
    name: Analyze
    runs-on: ubuntu-latest
    permissions:
      actions: read
      contents: read
      security-events: write
    strategy:
      fail-fast: false
      matrix:
        language: ["javascript", "python"]
    steps:
      - uses: actions/checkout@v5
      - uses: github/codeql-action/init@v3
        with:
          languages: ${{ matrix.language }}
      - uses: github/codeql-action/autobuild@v3
      - uses: github/codeql-action/analyze@v3
</file>

<file path=".github/dependabot.yml">
version: 2
updates:
  - package-ecosystem: "npm"
    directory: "/apps/web"
    schedule:
      interval: "weekly"
  - package-ecosystem: "pip"
    directory: "/services/api"
    schedule:
      interval: "weekly"
  - package-ecosystem: "github-actions"
    directory: "/"
    schedule:
      interval: "weekly"
</file>

<file path=".github/PULL_REQUEST_TEMPLATE.md">
## Summary

## Acceptance criteria
- [ ] Phase 0 CI passes
- [ ] Includes tests or explains why not

## Testing
- [ ] API: pytest
- [ ] Web: npm run lint && npm run build

## Notes
</file>

<file path="apps/web/public/.gitkeep">
Placeholder so Vercel's configured `outputDirectory` (public) exists.
</file>

<file path="apps/web/src/app/dashboard/matches/page.tsx">
"use client";

import { apiFetch } from "@/lib/api";
import { useEffect, useState } from "react";

type Availability = {
  format: string;
  status: string;
  copies_available: number | null;
  copies_total: number | null;
  holds: number | null;
  deep_link: string | null;
  last_checked_at: string;
};

type MatchRow = {
  shelf_item_id: string;
  method: string;
  confidence: number;
  evidence: Record<string, unknown>;
  catalog_item: {
    id: string;
    provider: string;
    provider_item_id: string;
    title: string;
    author: string | null;
    isbn10: string | null;
    isbn13: string | null;
    asin: string | null;
  };
  availability: Availability[];
};

type JobStatus = {
  id: string;
  status: string;
  result: Record<string, unknown> | null;
  exc_info: string | null;
};

export default function MatchesPage() {
  const [rows, setRows] = useState<MatchRow[]>([]);
  const [loading, setLoading] = useState(false);
  const [job, setJob] = useState<JobStatus | null>(null);
  const [error, setError] = useState<string | null>(null);

  async function loadRows() {
    const data = await apiFetch<MatchRow[]>("/v1/matches");
    setRows(data);
  }

  useEffect(() => {
    loadRows().catch((e) => setError(String(e)));
  }, []);

  async function refresh() {
    setError(null);
    setLoading(true);
    try {
      const { job_id } = await apiFetch<{ job_id: string }>("/v1/matching/refresh", {
        method: "POST",
      });

      // Poll status. Keep it simple; a websocket can come later.
      for (let i = 0; i < 30; i++) {
        const st = await apiFetch<JobStatus>(`/v1/matching/refresh/${job_id}`);
        setJob(st);
        if (st.status === "finished" || st.status === "failed") break;
        await new Promise((r) => setTimeout(r, 1000));
      }

      await loadRows();
    } catch (e) {
      setError(String(e));
    } finally {
      setLoading(false);
    }
  }

  return (
    <div className="p-6 space-y-4">
      <div className="flex items-center justify-between">
        <h1 className="text-2xl font-semibold">Matches</h1>
        <button
          className="px-3 py-2 rounded bg-black text-white disabled:opacity-50"
          onClick={refresh}
          disabled={loading}
        >
          {loading ? "Refreshing‚Ä¶" : "Refresh matches"}
        </button>
      </div>

      {error && <div className="p-3 rounded bg-red-50 text-red-800">{error}</div>}

      {job && (
        <div className="p-3 rounded bg-gray-50 text-sm">
          <div>
            Job: <span className="font-mono">{job.id}</span> ({job.status})
          </div>
          {job.exc_info && <div className="text-red-700">{job.exc_info}</div>}
          {job.result && <pre className="mt-2 overflow-auto">{JSON.stringify(job.result, null, 2)}</pre>}
        </div>
      )}

      <div className="space-y-3">
        {rows.length === 0 ? (
          <div className="text-gray-600">No matches yet. Import shelf items (Phase 2) and refresh.</div>
        ) : (
          rows.map((r) => (
            <div key={r.shelf_item_id} className="border rounded p-4">
              <div className="flex items-start justify-between gap-4">
                <div>
                  <div className="font-medium">{r.catalog_item.title}</div>
                  {r.catalog_item.author && <div className="text-sm text-gray-600">{r.catalog_item.author}</div>}
                  <div className="text-xs text-gray-500">
                    {r.method} ¬∑ confidence {r.confidence.toFixed(2)} ¬∑ {r.catalog_item.provider}:{r.catalog_item.provider_item_id}
                  </div>
                </div>
              </div>

              <div className="mt-3 grid grid-cols-1 md:grid-cols-2 gap-2">
                {r.availability.map((a) => (
                  <div key={a.format} className="rounded bg-gray-50 p-3 text-sm">
                    <div className="font-medium">{a.format}</div>
                    <div>Status: {a.status}</div>
                    {typeof a.copies_total === "number" && (
                      <div>
                        Copies: {a.copies_available ?? "?"}/{a.copies_total}
                      </div>
                    )}
                    {typeof a.holds === "number" && <div>Holds: {a.holds}</div>}
                    {a.deep_link && (
                      <a className="text-blue-700 underline" href={a.deep_link} target="_blank" rel="noreferrer">
                        Open
                      </a>
                    )}
                  </div>
                ))}
              </div>

              <details className="mt-3">
                <summary className="cursor-pointer text-sm text-gray-700">Why this match?</summary>
                <pre className="mt-2 overflow-auto text-xs bg-gray-50 p-3 rounded">{JSON.stringify(r.evidence, null, 2)}</pre>
              </details>
            </div>
          ))
        )}
      </div>
    </div>
  );
}
</file>

<file path="apps/web/src/app/notifications/page.tsx">
"use client";

import Link from "next/link";
import { useEffect, useState } from "react";

import { AuthGuard } from "@/components/AuthGuard";
import { apiFetch } from "@/lib/api";

type NotificationOut = {
  id: string;
  created_at: string;
  read_at: string | null;
  shelf_item_id: string;
  title: string;
  author: string | null;
  format: string;
  old_status: string;
  new_status: string;
  deep_link: string | null;
};

type NotificationListOut = {
  page: { total: number; limit: number; offset: number };
  items: NotificationOut[];
};

export default function NotificationsPage() {
  const [data, setData] = useState<NotificationListOut | null>(null);
  const [loading, setLoading] = useState(true);
  const [error, setError] = useState<string | null>(null);
  const [unreadOnly, setUnreadOnly] = useState(true);

  async function load() {
    setLoading(true);
    setError(null);
    try {
      const res = await apiFetch<NotificationListOut>(
        `/v1/notifications?limit=100&offset=0&unread_only=${unreadOnly ? "true" : "false"}`
      );
      setData(res);
    } catch (e) {
      setError(String(e));
    } finally {
      setLoading(false);
    }
  }

  useEffect(() => {
    load();
    // eslint-disable-next-line react-hooks/exhaustive-deps
  }, [unreadOnly]);

  async function markRead(id: string) {
    await apiFetch<void>(`/v1/notifications/${id}/read`, { method: "POST" });
    await load();
  }

  async function markAllRead() {
    await apiFetch<{ updated: number }>(`/v1/notifications/mark-all-read`, { method: "POST" });
    await load();
  }

  return (
    <AuthGuard>
      <main className="p-6 max-w-4xl mx-auto">
        <div className="flex items-center justify-between gap-4">
          <div>
            <Link href="/dashboard" className="text-sm text-blue-600 hover:underline">
              ‚Üê Back to dashboard
            </Link>
            <h1 className="text-2xl font-semibold mt-2">Notifications</h1>
            <p className="text-sm text-gray-600">Items that became available recently.</p>
          </div>
          <button
            onClick={markAllRead}
            className="rounded border px-3 py-2 text-sm hover:bg-gray-50"
          >
            Mark all read
          </button>
        </div>

        <div className="mt-4 flex items-center gap-2 text-sm">
          <label className="flex items-center gap-2">
            <input
              type="checkbox"
              checked={unreadOnly}
              onChange={(e) => setUnreadOnly(e.target.checked)}
            />
            Show unread only
          </label>
        </div>

        {loading ? <div className="mt-6 text-sm text-gray-600">Loading‚Ä¶</div> : null}
        {error ? <div className="mt-6 text-sm text-red-600">{error}</div> : null}

        {!loading && data && data.items.length === 0 ? (
          <div className="mt-6 rounded border p-4 text-sm text-gray-600">No notifications yet.</div>
        ) : null}

        <div className="mt-6 space-y-3">
          {data?.items.map((n) => (
            <div key={n.id} className="rounded border p-4">
              <div className="flex items-start justify-between gap-4">
                <div>
                  <div className="text-sm text-gray-500">
                    {new Date(n.created_at).toLocaleString()} ‚Ä¢ {n.format}
                  </div>
                  <div className="mt-1 font-medium">
                    <Link href={`/books/${n.shelf_item_id}`} className="hover:underline">
                      {n.title}
                    </Link>
                  </div>
                  {n.author ? <div className="text-sm text-gray-600">{n.author}</div> : null}

                  <div className="mt-2 text-sm">
                    <span className="font-medium">Status:</span> {n.old_status} ‚Üí {n.new_status}
                  </div>

                  {n.deep_link ? (
                    <div className="mt-2">
                      <a
                        href={n.deep_link}
                        target="_blank"
                        rel="noreferrer"
                        className="text-sm text-blue-600 hover:underline"
                      >
                        Open in library
                      </a>
                    </div>
                  ) : null}
                </div>

                {n.read_at ? (
                  <span className="text-xs text-gray-500">Read</span>
                ) : (
                  <button
                    onClick={() => markRead(n.id)}
                    className="rounded border px-3 py-2 text-sm hover:bg-gray-50"
                  >
                    Mark read
                  </button>
                )}
              </div>
            </div>
          ))}
        </div>
      </main>
    </AuthGuard>
  );
}
</file>

<file path="apps/web/src/app/settings/goodreads/page.tsx">
"use client";

import { apiFetch } from "@/lib/api";
import { useEffect, useState } from "react";

type ShelfSource = {
  id: string;
  source_type: string;
  provider: string;
  source_ref: string;
  meta: Record<string, unknown>;
  is_active: boolean;
  last_synced_at: string | null;
  last_sync_status: string | null;
  last_sync_error: string | null;
};

type ShelfItem = {
  id: string;
  title: string;
  author: string;
  isbn10: string | null;
  isbn13: string | null;
  asin: string | null;
  shelf: string | null;
  needs_fuzzy_match: boolean;
};

type ImportSummary = {
  created: number;
  updated: number;
  skipped: number;
  errors: { key: string; error: string }[];
};

export default function GoodreadsSettingsPage() {
  const [rssUrl, setRssUrl] = useState("http://localhost:4010/mock/goodreads/shelf/demo/read/rss");
  const [shelf, setShelf] = useState("to-read");

  const [rssSource, setRssSource] = useState<ShelfSource | null>(null);
  const [items, setItems] = useState<ShelfItem[]>([]);
  const [summary, setSummary] = useState<ImportSummary | null>(null);

  const [csvFile, setCsvFile] = useState<File | null>(null);

  const [busy, setBusy] = useState(false);
  const [error, setError] = useState<string | null>(null);

  async function refreshItems(sourceId?: string) {
    const qs = new URLSearchParams();
    if (sourceId) qs.set("source_id", sourceId);
    const rows = await apiFetch<ShelfItem[]>(`/v1/shelf-items?${qs.toString()}`);
    setItems(rows);
  }

  async function connectRss() {
    setBusy(true);
    setError(null);
    setSummary(null);
    try {
      const src = await apiFetch<ShelfSource>("/v1/shelf-sources/rss", {
        method: "POST",
        body: JSON.stringify({ rss_url: rssUrl, shelf, sync_now: true }),
      });
      setRssSource(src);

      // Let the worker run; poll once after a short delay.
      setTimeout(() => {
        refreshItems(src.id).catch(() => undefined);
      }, 750);
    } catch (e: any) {
      setError(e?.message || String(e));
    } finally {
      setBusy(false);
    }
  }

  async function syncNow() {
    if (!rssSource) return;
    setBusy(true);
    setError(null);
    try {
      await apiFetch<{ job_id: string }>(`/v1/shelf-sources/${rssSource.id}/sync`, { method: "POST" });
      setTimeout(() => {
        refreshItems(rssSource.id).catch(() => undefined);
      }, 750);
    } catch (e: any) {
      setError(e?.message || String(e));
    } finally {
      setBusy(false);
    }
  }

  async function disconnect() {
    if (!rssSource) return;
    setBusy(true);
    setError(null);
    try {
      await apiFetch<void>(`/v1/shelf-sources/${rssSource.id}`, { method: "DELETE" });
      setRssSource(null);
      setItems([]);
    } catch (e: any) {
      setError(e?.message || String(e));
    } finally {
      setBusy(false);
    }
  }

  async function uploadCsv() {
    if (!csvFile) return;
    setBusy(true);
    setError(null);
    setSummary(null);
    try {
      const fd = new FormData();
      fd.append("file", csvFile);
      const res = await apiFetch<ImportSummary>("/v1/shelf-sources/csv", {
        method: "POST",
        body: fd,
      });
      setSummary(res);
      await refreshItems();
    } catch (e: any) {
      setError(e?.message || String(e));
    } finally {
      setBusy(false);
    }
  }

  useEffect(() => {
    // Load items on first open (works even before connecting)
    refreshItems().catch(() => undefined);
  }, []);

  return (
    <main className="min-h-screen p-8">
      <div className="mx-auto max-w-3xl space-y-8">
        <header className="space-y-2">
          <h1 className="text-3xl font-bold">Goodreads</h1>
          <p className="text-gray-600">Connect an RSS shelf or upload a Goodreads CSV export.</p>
        </header>

        {error ? (
          <div className="rounded-xl border border-red-200 bg-red-50 p-4 text-sm text-red-800">
            {error}
          </div>
        ) : null}

        <section className="rounded-2xl border p-6 shadow-sm space-y-4">
          <h2 className="text-xl font-semibold">Connect RSS</h2>

          <div className="space-y-2">
            <label className="block text-sm font-medium">RSS URL (or path)</label>
            <input
              className="w-full rounded-xl border px-3 py-2"
              value={rssUrl}
              onChange={(e) => setRssUrl(e.target.value)}
              placeholder="https://www.goodreads.com/review/list_rss/..."
            />
          </div>

          <div className="space-y-2">
            <label className="block text-sm font-medium">Shelf label (stored as metadata)</label>
            <input className="w-full rounded-xl border px-3 py-2" value={shelf} onChange={(e) => setShelf(e.target.value)} />
          </div>

          <div className="flex flex-wrap gap-2">
            <button
              className="rounded-xl bg-black px-4 py-2 text-white disabled:opacity-60"
              onClick={connectRss}
              disabled={busy}
            >
              Connect & Import
            </button>

            <button
              className="rounded-xl border px-4 py-2 disabled:opacity-60"
              onClick={syncNow}
              disabled={busy || !rssSource}
            >
              Re-import (Sync)
            </button>

            <button
              className="rounded-xl border px-4 py-2 disabled:opacity-60"
              onClick={disconnect}
              disabled={busy || !rssSource}
            >
              Disconnect
            </button>
          </div>

          {rssSource ? (
            <div className="text-sm text-gray-700">
              <div>
                <span className="font-medium">Connected:</span> {rssSource.source_ref}
              </div>
              <div>
                <span className="font-medium">Last sync:</span> {rssSource.last_synced_at || "(not yet)"}
              </div>
              {rssSource.last_sync_status === "error" ? (
                <div className="text-red-700">{rssSource.last_sync_error}</div>
              ) : null}
            </div>
          ) : (
            <div className="text-sm text-gray-500">No RSS source connected yet.</div>
          )}
        </section>

        <section className="rounded-2xl border p-6 shadow-sm space-y-4">
          <h2 className="text-xl font-semibold">CSV Upload (fallback)</h2>
          <p className="text-sm text-gray-600">
            Export from Goodreads: <span className="font-medium">My Books ‚Üí Import and Export ‚Üí Export Library</span>.
          </p>

          <input
            type="file"
            accept=".csv"
            onChange={(e) => setCsvFile(e.target.files?.[0] || null)}
          />

          <button
            className="rounded-xl bg-black px-4 py-2 text-white disabled:opacity-60"
            onClick={uploadCsv}
            disabled={busy || !csvFile}
          >
            Upload & Import
          </button>

          {summary ? (
            <div className="rounded-xl border bg-gray-50 p-4 text-sm">
              <div className="font-medium">Import summary</div>
              <div>Created: {summary.created}</div>
              <div>Updated: {summary.updated}</div>
              <div>Skipped: {summary.skipped}</div>
              {summary.errors?.length ? (
                <div className="mt-2">
                  <div className="font-medium">Errors</div>
                  <ul className="list-disc pl-5">
                    {summary.errors.slice(0, 20).map((e) => (
                      <li key={`${e.key}-${e.error}`}>{e.key}: {e.error}</li>
                    ))}
                  </ul>
                </div>
              ) : null}
            </div>
          ) : null}
        </section>

        <section className="rounded-2xl border p-6 shadow-sm space-y-4">
          <h2 className="text-xl font-semibold">Imported shelf items</h2>
          <div className="text-sm text-gray-600">Showing most recently updated first.</div>

          <div className="divide-y rounded-xl border">
            {items.length === 0 ? (
              <div className="p-4 text-sm text-gray-500">No items yet. Connect RSS or upload CSV.</div>
            ) : (
              items.map((b) => (
                <div key={b.id} className="p-4">
                  <div className="font-medium">{b.title}</div>
                  <div className="text-sm text-gray-700">{b.author}</div>
                  <div className="mt-1 text-xs text-gray-500">
                    {b.isbn13 ? `ISBN13: ${b.isbn13}` : b.isbn10 ? `ISBN: ${b.isbn10}` : b.asin ? `ASIN: ${b.asin}` : "No ISBN (fuzzy match later)"}
                    {b.shelf ? ` ¬∑ shelf: ${b.shelf}` : ""}
                    {b.needs_fuzzy_match ? " ¬∑ needs match" : ""}
                  </div>
                </div>
              ))
            )}
          </div>
        </section>
      </div>
    </main>
  );
}
</file>

<file path="apps/web/src/app/settings/page.tsx">
"use client";

import Link from "next/link";
import { useEffect, useMemo, useState } from "react";

import { AuthGuard } from "@/components/AuthGuard";
import { apiFetch } from "@/lib/api";

type SettingsOut = { library_system: string | null; preferred_formats: string[]; updated_at: string };
type LibraryOut = { id: string; name: string };

export default function SettingsPage() {
  const [settings, setSettings] = useState<SettingsOut | null>(null);
  const [libraries, setLibraries] = useState<LibraryOut[]>([]);
  const [saving, setSaving] = useState(false);
  const [error, setError] = useState<string | null>(null);
  const [ok, setOk] = useState<string | null>(null);

  const [librarySystem, setLibrarySystem] = useState<string>("");
  const [formats, setFormats] = useState<{ ebook: boolean; audiobook: boolean }>({ ebook: true, audiobook: false });

  useEffect(() => {
    (async () => {
      try {
        const [s, libs] = await Promise.all([
          apiFetch<SettingsOut>("/v1/settings"),
          apiFetch<LibraryOut[]>("/v1/libraries"),
        ]);
        setSettings(s);
        setLibraries(libs);
        setLibrarySystem(s.library_system ?? "");
        setFormats({ ebook: s.preferred_formats.includes("ebook"), audiobook: s.preferred_formats.includes("audiobook") });
      } catch (e) {
        const msg = e instanceof Error ? e.message : "Failed to load settings.";
        setError(msg);
      }
    })();
  }, []);

  const preferredFormats = useMemo(() => {
    const out: string[] = [];
    if (formats.ebook) out.push("ebook");
    if (formats.audiobook) out.push("audiobook");
    // Always keep at least one format selected
    return out.length ? out : ["ebook"];
  }, [formats]);

  async function save() {
    setOk(null);
    setError(null);
    setSaving(true);
    try {
      const res = await apiFetch<SettingsOut>("/v1/settings", {
        method: "PATCH",
        body: JSON.stringify({
          library_system: librarySystem || null,
          preferred_formats: preferredFormats,
        }),
      });
      setSettings(res);
      setOk("Saved.");
    } catch (e) {
      const msg = e instanceof Error ? e.message : "Failed to save.";
      setError(msg);
    } finally {
      setSaving(false);
    }
  }

  return (
    <AuthGuard>
      <main className="min-h-screen px-6 py-10">
        <div className="mx-auto flex max-w-3xl flex-col gap-6">
          <header className="rounded-3xl border border-black/10 bg-white/70 p-6 shadow-sm">
            <div className="flex items-center justify-between gap-4">
              <div>
                <h1 className="font-display text-3xl tracking-tight">Preferences</h1>
                <p className="text-sm text-black/60">Choose your library and formats.</p>
              </div>
              <div className="flex items-center gap-2 text-sm">
                <Link className="rounded-full border border-black/10 px-4 py-2 hover:bg-black/5" href="/dashboard">
                  Back
                </Link>
                <Link className="rounded-full border border-black/10 px-4 py-2 hover:bg-black/5" href="/settings/goodreads">
                  Import/Connect
                </Link>
              </div>
            </div>
          </header>

          <section className="rounded-3xl border border-black/10 bg-white/70 p-6 shadow-sm">
            {error ? (
              <div className="mb-4 rounded-2xl border border-red-200 bg-red-50 p-4 text-sm text-red-900">{error}</div>
            ) : null}
            {ok ? (
              <div className="mb-4 rounded-2xl border border-emerald-200 bg-emerald-50 p-4 text-sm text-emerald-900">
                {ok}
              </div>
            ) : null}

            <div className="space-y-5">
              <div>
                <label className="mb-1 block text-sm font-medium">Library</label>
                <select
                  value={librarySystem}
                  onChange={(e) => setLibrarySystem(e.target.value)}
                  className="w-full rounded-2xl border border-black/10 bg-white px-4 py-2 text-sm"
                >
                  <option value="">‚Äî Select a library ‚Äî</option>
                  {libraries.map((l) => (
                    <option key={l.id} value={l.id}>
                      {l.name} ({l.id})
                    </option>
                  ))}
                </select>
                <p className="mt-2 text-xs text-black/60">Library selection is used to scope availability lookups and caching.</p>
              </div>

              <div>
                <label className="mb-1 block text-sm font-medium">Preferred formats</label>
                <div className="flex flex-wrap gap-3 text-sm">
                  <label className="flex items-center gap-2">
                    <input
                      type="checkbox"
                      checked={formats.ebook}
                      onChange={(e) => setFormats((f) => ({ ...f, ebook: e.target.checked }))}
                    />
                    Ebook
                  </label>
                  <label className="flex items-center gap-2">
                    <input
                      type="checkbox"
                      checked={formats.audiobook}
                      onChange={(e) => setFormats((f) => ({ ...f, audiobook: e.target.checked }))}
                    />
                    Audiobook
                  </label>
                </div>
                <p className="mt-2 text-xs text-black/60">At least one format must be selected.</p>
              </div>

              <div className="flex items-center justify-between">
                <div className="text-xs text-black/60">
                  Last updated: {settings?.updated_at ? new Date(settings.updated_at).toLocaleString() : "‚Äî"}
                </div>
                <button
                  onClick={save}
                  disabled={saving}
                  className="rounded-full bg-black px-5 py-2 text-sm text-white disabled:opacity-60"
                >
                  {saving ? "Saving‚Ä¶" : "Save"}
                </button>
              </div>
            </div>
          </section>
        </div>
      </main>
    </AuthGuard>
  );
}
</file>

<file path="apps/web/src/app/signin/page.tsx">
"use client";

import { apiFetch } from "@/lib/api";
import { useRouter } from "next/navigation";
import { useState } from "react";

type UserOut = { id: string; email: string };

export default function SignInPage() {
  const router = useRouter();
  const [email, setEmail] = useState("");
  const [password, setPassword] = useState("");
  const [error, setError] = useState<string | null>(null);
  const [loading, setLoading] = useState(false);

  async function onSubmit(e: React.FormEvent) {
    e.preventDefault();
    setError(null);
    setLoading(true);
    try {
      await apiFetch<UserOut>("/v1/auth/login", {
        method: "POST",
        body: JSON.stringify({ email, password }),
      });
      router.push("/dashboard");
    } catch (err: any) {
      setError(err?.message ?? "Sign in failed");
    } finally {
      setLoading(false);
    }
  }

  return (
    <main className="min-h-screen p-8">
      <div className="mx-auto max-w-md space-y-6">
        <h1 className="text-3xl font-bold">Sign in</h1>

        {error && (
          <div className="rounded-xl border border-red-200 bg-red-50 p-3 text-sm">
            {error}
          </div>
        )}

        <form onSubmit={onSubmit} className="space-y-4">
          <label className="block space-y-1">
            <span className="text-sm font-medium">Email</span>
            <input
              className="w-full rounded-lg border p-2"
              value={email}
              onChange={(e) => setEmail(e.target.value)}
              type="email"
              required
              autoComplete="email"
            />
          </label>

          <label className="block space-y-1">
            <span className="text-sm font-medium">Password</span>
            <input
              className="w-full rounded-lg border p-2"
              value={password}
              onChange={(e) => setPassword(e.target.value)}
              type="password"
              required
              minLength={8}
              autoComplete="current-password"
            />
          </label>

          <button
            disabled={loading}
            className="w-full rounded-lg bg-black px-4 py-2 text-white disabled:opacity-50"
          >
            {loading ? "Signing in‚Ä¶" : "Sign in"}
          </button>
        </form>

        <p className="text-sm text-gray-600">
          New here? <a className="underline" href="/signup">Create an account</a>
        </p>
      </div>
    </main>
  );
}
</file>

<file path="apps/web/src/app/signup/page.tsx">
"use client";

import { apiFetch } from "@/lib/api";
import { useRouter } from "next/navigation";
import { useState } from "react";

type UserOut = { id: string; email: string };

export default function SignUpPage() {
  const router = useRouter();
  const [email, setEmail] = useState("");
  const [password, setPassword] = useState("");
  const [error, setError] = useState<string | null>(null);
  const [loading, setLoading] = useState(false);

  async function onSubmit(e: React.FormEvent) {
    e.preventDefault();
    setError(null);
    setLoading(true);
    try {
      await apiFetch<UserOut>("/v1/auth/signup", {
        method: "POST",
        body: JSON.stringify({ email, password }),
      });
      router.push("/dashboard");
    } catch (err: any) {
      setError(err?.message ?? "Sign up failed");
    } finally {
      setLoading(false);
    }
  }

  return (
    <main className="min-h-screen p-8">
      <div className="mx-auto max-w-md space-y-6">
        <h1 className="text-3xl font-bold">Create account</h1>

        {error && (
          <div className="rounded-xl border border-red-200 bg-red-50 p-3 text-sm">
            {error}
          </div>
        )}

        <form onSubmit={onSubmit} className="space-y-4">
          <label className="block space-y-1">
            <span className="text-sm font-medium">Email</span>
            <input
              className="w-full rounded-lg border p-2"
              value={email}
              onChange={(e) => setEmail(e.target.value)}
              type="email"
              required
              autoComplete="email"
            />
          </label>

          <label className="block space-y-1">
            <span className="text-sm font-medium">Password</span>
            <input
              className="w-full rounded-lg border p-2"
              value={password}
              onChange={(e) => setPassword(e.target.value)}
              type="password"
              required
              minLength={8}
              autoComplete="new-password"
            />
          </label>

          <button
            disabled={loading}
            className="w-full rounded-lg bg-black px-4 py-2 text-white disabled:opacity-50"
          >
            {loading ? "Creating‚Ä¶" : "Create account"}
          </button>
        </form>

        <p className="text-sm text-gray-600">
          Already have an account? <a className="underline" href="/signin">Sign in</a>
        </p>
      </div>
    </main>
  );
}
</file>

<file path="apps/web/src/app/page.test.tsx">
/// <reference types="vitest" />

import { render, screen } from "@testing-library/react";

beforeAll(() => {
  process.env.NEXT_PUBLIC_API_BASE_URL = "http://localhost:8000";
});

test("renders the hero title", async () => {
  const { default: Home } = await import("./page");
  render(<Home />);

  expect(screen.getByText("ShelfSync")).toBeInTheDocument();
});
</file>

<file path="apps/web/src/components/AuthGuard.tsx">
"use client";

import { apiFetch } from "@/lib/api";
import { useRouter } from "next/navigation";
import { useEffect, useState } from "react";

type UserOut = { id: string; email: string };

export function AuthGuard({ children }: { children: React.ReactNode }) {
  const router = useRouter();
  const [loading, setLoading] = useState(true);
  const [user, setUser] = useState<UserOut | null>(null);

  useEffect(() => {
    let alive = true;
    (async () => {
      try {
        const me = await apiFetch<UserOut>("/v1/auth/me");
        if (!alive) return;
        setUser(me);
      } catch {
        router.push("/signin");
      } finally {
        if (alive) setLoading(false);
      }
    })();
    return () => {
      alive = false;
    };
  }, [router]);

  if (loading) {
    return <div className="p-8 text-sm text-gray-600">Checking session‚Ä¶</div>;
  }

  if (!user) return null;

  return <>{children}</>;
}
</file>

<file path="apps/web/src/components/NotificationBell.tsx">
"use client";

import Link from "next/link";
import { useEffect, useRef, useState } from "react";

import { apiFetch } from "@/lib/api";

type UnreadCountOut = { unread: number };

export function NotificationBell() {
  const [unread, setUnread] = useState<number>(0);
  const startedRef = useRef(false);

  async function loadCount() {
    try {
      const res = await apiFetch<UnreadCountOut>("/v1/notifications/unread-count");
      setUnread(res.unread);
    } catch {
      // ignore
    }
  }

  useEffect(() => {
    if (startedRef.current) return;
    startedRef.current = true;

    let alive = true;
    let pollTimer: ReturnType<typeof setInterval> | null = null;

    loadCount();

    // Live updates (SSE). If it fails, fall back to polling.
    const url = `/api/proxy/sse?path=${encodeURIComponent("/v1/notifications/events")}`;
    const es = new EventSource(url);

    es.onmessage = (evt) => {
      if (!alive) return;
      try {
        const msg = JSON.parse(evt.data);
        if (msg?.type === "notification") {
          setUnread((n) => n + 1);
        }
      } catch {
        // ignore
      }
    };

    es.onerror = () => {
      if (!alive) return;
      es.close();

      // fallback polling every 30s
      if (!pollTimer) {
        pollTimer = setInterval(() => {
          if (!alive) return;
          loadCount();
        }, 30_000);
      }
    };

    return () => {
      alive = false;
      es.close();
      if (pollTimer) clearInterval(pollTimer);
    };
  }, []);

  return (
    <Link
      href="/notifications"
      className="relative inline-flex items-center justify-center rounded border px-3 py-2 text-sm hover:bg-gray-50"
      aria-label="Notifications"
      title="Notifications"
    >
      <span aria-hidden>üîî</span>
      {unread > 0 ? (
        <span className="absolute -top-2 -right-2 min-w-[1.25rem] h-5 px-1 rounded-full bg-red-600 text-white text-xs flex items-center justify-center">
          {unread}
        </span>
      ) : null}
    </Link>
  );
}
</file>

<file path="apps/web/src/components/TryDemoButton.tsx">
"use client";

import { apiFetch } from "@/lib/api";
import { useRouter } from "next/navigation";
import { useState } from "react";

const DEMO_EMAIL = "demo@example.com";
const DEMO_PASSWORD = "password123";
const DEMO_RSS_URL = "https://www.goodreads.com/shelf/rss?user=demo&shelf=read";

type TryDemoButtonProps = {
  className?: string;
};

export function TryDemoButton({ className }: TryDemoButtonProps) {
  const router = useRouter();
  const [pending, setPending] = useState(false);
  const [error, setError] = useState<string | null>(null);

  const label = pending ? "Loading demo..." : "Try demo data";

  async function handleClick() {
    setError(null);
    setPending(true);

    try {
      await apiFetch("/v1/auth/login", {
        method: "POST",
        body: JSON.stringify({ email: DEMO_EMAIL, password: DEMO_PASSWORD }),
      });

      await apiFetch("/v1/shelf-sources/rss", {
        method: "POST",
        body: JSON.stringify({ rss_url: DEMO_RSS_URL, shelf_name: "Demo shelf" }),
      });

      router.push("/dashboard");
    } catch (err) {
      const message = err instanceof Error ? err.message : "Unable to load demo data.";
      setError(message);
    } finally {
      setPending(false);
    }
  }

  const buttonClassName = `${className ?? ""} disabled:opacity-60 disabled:cursor-not-allowed`;

  return (
    <div className="space-y-2">
      <button className={buttonClassName} disabled={pending} onClick={handleClick}>
        {label}
      </button>
      {error ? <p className="text-xs text-[var(--accent)]">{error}</p> : null}
    </div>
  );
}
</file>

<file path="apps/web/src/hooks/useSyncRun.ts">
"use client";

import { getSyncRun, startAvailabilityRefresh, subscribeSyncRun, SyncRun } from "@/lib/syncRuns";
import { useCallback, useEffect, useRef, useState } from "react";

export function useSyncRun() {
  const [run, setRun] = useState<SyncRun | null>(null);
  const [error, setError] = useState<string | null>(null);
  const unsubscribeRef = useRef<null | (() => void)>(null);

  const stop = useCallback(() => {
    unsubscribeRef.current?.();
    unsubscribeRef.current = null;
  }, []);

  const start = useCallback(async () => {
    setError(null);
    stop();

    const created = await startAvailabilityRefresh();
    setRun(created);

    unsubscribeRef.current = subscribeSyncRun(created.id, (evt) => {
      const t = evt.type;
      if (t === "progress") {
        setRun((r) =>
          r
            ? {
                ...r,
                status: "running",
                progress_current: evt.payload.current,
                progress_total: evt.payload.total,
              }
            : r
        );
      }
      if (t === "failed") {
        setRun((r) => (r ? { ...r, status: "failed", error_message: evt.payload.message } : r));
        stop();
      }
      if (t === "succeeded") {
        setRun((r) => (r ? { ...r, status: "succeeded", progress_current: r.progress_total } : r));
        stop();
      }
    });

    // Fallback polling in case SSE disconnects
    const poll = async () => {
      try {
        const fresh = await getSyncRun(created.id);
        setRun(fresh);
        if (fresh.status === "succeeded" || fresh.status === "failed") return;
        setTimeout(poll, 2500);
      } catch {
        setTimeout(poll, 2500);
      }
    };

    setTimeout(poll, 2500);
  }, [stop]);

  useEffect(() => {
    return () => stop();
  }, [stop]);

  return { run, error, start, stop };
}
</file>

<file path="apps/web/src/lib/readNext.ts">
export type ReadNext = {
  score: number;
  tier: string;
  best_format: string | null;
  hold_ratio: number | null;
  reasons: string[];
};

export function compareReadNext(
  a: { title: string; author?: string | null; shelf_item_id?: string; read_next?: ReadNext },
  b: { title: string; author?: string | null; shelf_item_id?: string; read_next?: ReadNext },
) {
  const sa = a.read_next?.score ?? -Infinity;
  const sb = b.read_next?.score ?? -Infinity;
  if (sa !== sb) return sb - sa; // desc

  const ta = (a.title ?? "").toLowerCase();
  const tb = (b.title ?? "").toLowerCase();
  if (ta < tb) return -1;
  if (ta > tb) return 1;

  const aa = (a.author ?? "").toLowerCase();
  const ab = (b.author ?? "").toLowerCase();
  if (aa < ab) return -1;
  if (aa > ab) return 1;

  const ida = a.shelf_item_id ?? "";
  const idb = b.shelf_item_id ?? "";
  if (ida < idb) return -1;
  if (ida > idb) return 1;

  return 0;
}

export function readNextTooltip(rn?: ReadNext) {
  if (!rn) return "No Read Next score";
  const lines = [`Score: ${rn.score.toFixed(1)}`, ...rn.reasons];
  return lines.join("\n");
}
</file>

<file path="apps/web/src/lib/syncRuns.ts">
import { apiFetch } from "@/lib/api";

export type SyncRun = {
  id: string;
  kind: string;
  status: "queued" | "running" | "succeeded" | "failed";
  progress_current: number;
  progress_total: number;
  error_message?: string | null;
  started_at?: string | null;
  finished_at?: string | null;
  created_at: string;
};

export async function startAvailabilityRefresh(): Promise<SyncRun> {
  return apiFetch<SyncRun>("/v1/sync-runs", {
    method: "POST",
    body: JSON.stringify({ kind: "availability_refresh" }),
  });
}

export async function getSyncRun(id: string): Promise<SyncRun> {
  return apiFetch<SyncRun>(`/v1/sync-runs/${id}`);
}

export function subscribeSyncRun(id: string, onEvent: (evt: any) => void) {
  const es = new EventSource(`/api/proxy/sse?path=/v1/sync-runs/${id}/events`);

  es.addEventListener("sync", (e) => {
    try {
      onEvent(JSON.parse((e as MessageEvent).data));
    } catch {
      // ignore
    }
  });

  es.onerror = () => {
    // Let caller decide fallback.
  };

  return () => es.close();
}
</file>

<file path="apps/web/src/test/msw/handlers.ts">
import { http, HttpResponse } from "msw";

export const handlers = [
  http.get("*/health", () => HttpResponse.json({ ok: true })),
];
</file>

<file path="apps/web/src/test/msw/server.ts">
import { setupServer } from "msw/node";

import { handlers } from "./handlers";

export const server = setupServer(...handlers);
</file>

<file path="apps/web/src/test/setup.ts">
import "@testing-library/jest-dom/vitest";
import { afterAll, afterEach, beforeAll, vi } from "vitest";

import { server } from "./msw/server";

vi.mock("next/navigation", () => ({
  useRouter: () => ({
    push: vi.fn(),
  }),
}));

beforeAll(() => server.listen({ onUnhandledRequest: "error" }));
afterEach(() => server.resetHandlers());
afterAll(() => server.close());
</file>

<file path="apps/web/test-results/.last-run.json">
{
  "status": "failed",
  "failedTests": []
}
</file>

<file path="apps/web/.env.local.example">
NEXT_PUBLIC_API_BASE_URL=http://localhost:8000
</file>

<file path="apps/web/.eslintrc.json">
{
  "extends": ["next/core-web-vitals"]
}
</file>

<file path="apps/web/next-env.d.ts">
/// <reference types="next" />
/// <reference types="next/image-types/global" />

// NOTE: This file should not be edited
// see https://nextjs.org/docs/app/building-your-application/configuring/typescript for more information.
</file>

<file path="apps/web/next.config.mjs">
/** @type {import('next').NextConfig} */
const nextConfig = {
  reactStrictMode: true
};

export default nextConfig;
</file>

<file path="apps/web/postcss.config.js">
module.exports = {
  plugins: {
    tailwindcss: {},
    autoprefixer: {}
  }
};
</file>

<file path="apps/web/tailwind.config.js">
/** @type {import('tailwindcss').Config} */
module.exports = {
  content: ["./src/**/*.{js,ts,jsx,tsx,mdx}"],
  theme: {
    extend: {}
  },
  plugins: []
};
</file>

<file path="apps/web/vercel.json">
{
  "rewrites": [
    {
      "source": "/api/:path*",
      "destination": "https://YOUR_TRYCLOUDFLARE_SUBDOMAIN.trycloudflare.com/:path*"
    }
  ]
}
</file>

<file path="apps/web/vitest.config.ts">
import react from "@vitejs/plugin-react";
import path from "node:path";
import { defineConfig } from "vitest/config";

export default defineConfig({
  plugins: [react()],
  resolve: {
    alias: {
      "@": path.resolve(__dirname, "./src"),
    },
  },
  test: {
    environment: "jsdom",
    setupFiles: ["src/test/setup.ts"],
    globals: true,
    css: true,
  },
});
</file>

<file path="docs/adr/0001-tech-stack.md">
# ADR 0001: Tech stack

## Status
Accepted

## Context
ShelfSync needs a fast-to-demo full-stack portfolio project with modern tooling, strong local dev ergonomics, and a clear path to deployment.

## Decision
- Web: Next.js + React + TypeScript + Tailwind
- API: FastAPI + Uvicorn + Pydantic
- Data: Postgres + SQLAlchemy + Alembic
- Caching/queue: Redis
- HTTP: httpx

## Consequences
- Great developer experience and hiring-manager familiarity.
- Python backend supports data parsing and automation easily.
- Clear hosting story: Vercel + Koyeb + Supabase + Upstash.
</file>

<file path="docs/adr/0002-goodreads-ingestion.md">
# 0002: Goodreads ingestion strategy

## Status
Accepted

## Context
We need a reliable way to ingest a user‚Äôs Goodreads shelves into ShelfSync.

## Decision
Use RSS as the primary ingestion mechanism and CSV as a fallback.

## Consequences
- RSS allows periodic background sync.
- CSV supports users who cannot access RSS.
- Idempotency is enforced by external IDs when present.
</file>

<file path="docs/data-quality/issue-log/data_quality_issue_log_example.csv">
issue_id,title,description,dataset_system,table_entity,column_field,dq_dimension,severity,business_impact,detected_by,detection_method,evidence_link_or_query,status,owner,reported_date,target_fix_date,resolved_date,root_cause_category,remediation_summary,fix_reference,verification_steps,preventive_control,notes
DQ-001,EU order totals missing,Order total is NULL for EU orders, but dashboards expect revenue per transaction,orders-db,orders,total_amount,Completeness,S2,"EU revenue dashboards show zeroed totals for July",Billing alert (QLIK),Automated completeness check,https://monitoring.example.com/dq/DQ-001,Verified,@data-eng,2024-08-01,2024-08-03,2024-08-02,Transformation bug,Updated the EU order aggregation to coalesce totals and trim NULL rows,https://github.com/org/repo/pull/123,"Re-ran completeness check and confirmed dashboards now show totals",Added EU-specific completeness alert and automated test for total_amount,Recurring from July; follow up with partner about feed stability
DQ-002,Duplicate customer records,Customer IDs appear twice with diff emails so marketing sends duplicate messages,crm,customers,customer_id,Uniqueness,S3,"Marketing automation sends duplicates; analytics double-counts customer lifetime value",DataOps manual audit,Ad-hoc SQL scan,"SELECT customer_id, COUNT(*) FROM crm.customers GROUP BY 1 HAVING COUNT(*) > 1",In Progress,@data-ops,2024-08-05,2024-08-10,,Transformation bug,Built temporary flag on duplicates and preparing dedup job,DQ-002-TICKET-45,"Run dedupe job, confirm no duplicates, and notify marketing",Added dedup monitoring query and alert when duplicates exceed threshold,Recurring regression; upstream loader review pending
DQ-003,Invalid inventory status,Inventory.status includes SOLDOUT but schema only allows IN_STOCK, BACKORDER,inventory,inventory,status,Validity,S4,"UI shows blank state for items marked SOLDOUT, but stock counts remain accurate",Ops on-call,Stakeholder report,https://support.example.com/ticket/789,Triaged,@ops,2024-08-07,2024-08-14,,Definition mismatch,Mapped SOLDOUT to OUT_OF_STOCK during ingestion and documented new contract,https://github.com/org/repo/pull/130,"Confirmed enumerations via SQL audit and UI spot-check",Added enum constraint check and documented allowed statuses,Documented contract expectation in taxonomy guide
</file>

<file path="docs/data-quality/issue-log/data_quality_issue_log_template.csv">
issue_id,title,description,dataset_system,table_entity,column_field,dq_dimension,severity,business_impact,detected_by,detection_method,evidence_link_or_query,status,owner,reported_date,target_fix_date,resolved_date,root_cause_category,remediation_summary,fix_reference,verification_steps,preventive_control,notes
</file>

<file path="docs/data-quality/issue-log/README.md">
# Data Quality Issue Log

This folder captures the workflow for logging, triaging, resolving, and reporting data quality issues affecting ShelfSync. Keep the log in `data_quality_issue_log_template.csv` and update `data_quality_issue_log_example.csv` with representative cases before any reporting cycle.

## Purpose
- Track every known data problem from detection through verification so nothing slips between teams.
- Assign accountability and severity so stakeholders know what to expect and who is working the fix.
- Build a governance-friendly record for retrospective metrics (open count, remediation time, recurrence).

## What counts as a data-quality issue
- Missing/empty critical values or required records (completeness drift).
- Incorrect or stale values (accuracy/validity) such as invalid enums, timestamps, or financial amounts.
- Duplicate records or broken relationships (uniqueness/integrity/consistency).
- Conflicting definitions (different stakeholders rely on different meanings for the same field).
- Any recurring alert, stakeholder report, or regression flagged by monitoring that blocks delivery expectations.

Refer to `taxonomy.md` for consistent dimension and root cause naming.

## How to add a new issue
1. Copy the header row from `data_quality_issue_log_template.csv` into a new row and populate every required column.
2. Provide clear evidence (`evidence_link_or_query`) such as SQL, dashboards, or stakeholder tickets.
3. Set initial `status` to `New`, assign `severity` (based on impact), and nominate a single `owner`.
4. Fill `reported_date` and pick a realistic `target_fix_date`; leave `resolved_date` blank until closure.

## Triage rules
- Use the severity rubric in `taxonomy.md` to distinguish S1 (system down, data loss) through S4 (low-impact documentation gaps).
- Before moving a row from `New` to `Triaged`, ensure `owner`, `severity`, `business_impact`, and `detection_method` are populated.
- Ownership means the owner either resolves the issue or coordinates the fix with a delivery team; updates on `status` and `root_cause_category` should accompany work.

## Lifecycle
New ‚Üí Triaged ‚Üí In Progress ‚Üí Fixed ‚Üí Verified ‚Üí Closed
- `In Progress`: Engineering fix is underway.
- `Fixed`: Fix applied (code change, backfill, etc.) and recorded in `fix_reference`.
- `Verified`: Re-run the detection query or confirm with the reporting stakeholder; document the steps in `verification_steps`.
- `Closed`: Stakeholders sign off and preventive controls are enacted.

## Root cause + prevention
- Use the root cause categories from `taxonomy.md` (upstream source, transformation bug, definition mismatch, late delivery, manual entry, etc.).
- Every recurring issue must include a `preventive_control` (monitoring alert, constraint, additional test, updated definition, etc.).
- If the issue is traced to an upstream partner, note the escalation path and adjust `preventive_control` accordingly.

## Reporting
- Share a weekly summary with:
  - Open issues by severity and dimension.
  - Time-to-triage (from `reported_date` to `Triaged`).
  - Time-to-close for resolved issues.
  - Top recurring `dq_dimension` and `root_cause_category` pairs.
- Include charts or pivot tables that highlight blockers for the next release.

## Nice-to-have additions
1. Host the same log as a Google Sheet and link to it here once available so teammates can contribute without cloning the repo.
2. Add optional columns such as `recurring_issue_flag` and `last_seen_date` when regression tracking is needed for stakeholder communication.
3. Consider a dashboard tab (pivot tables, filters) on the sheet that surfaces open issues by severity/dimension for quick triage.
</file>

<file path="docs/data-quality/issue-log/taxonomy.md">
# Data Quality Taxonomy

## Data Quality Dimensions
Match `dq_dimension` to the most appropriate descriptor; pick only one primary dimension per issue:
- **Completeness**: Missing rows or columns, nulls in required fields.
- **Accuracy**: Incorrect values that do not match reality.
- **Consistency**: Conflicting values between systems or reports.
- **Validity**: Values outside allowed formats, ranges, or enums.
- **Uniqueness**: Duplicate rows or surrogate keys that are not unique.
- **Integrity**: Broken joins, referential integrity, or relationship issues.

## Severity Rubric (S1‚ÄìS4)
- **S1 (Critical)**: Downstream reporting or systems unusable; financial/regulatory risk.
- **S2 (High)**: Key dashboards or processes misstate data for most users; immediate attention needed.
- **S3 (Medium)**: Impacted personas have a work-around; metric drift is isolated.
- **S4 (Low)**: Cosmetic issues, documentation gaps, or non-blocking anomalies.

Map severity to High/Medium/Low if executive reporting prefers text.

## Root Cause Categories
- **Upstream source**: Partner data missing or malformed before ingestion.
- **Transformation bug**: ETL/SQL logic errors or regression in data pipelines.
- **Definition mismatch**: Different stakeholders use incompatible definitions.
- **Late data**: Expected feeds arrive late, causing incomplete snapshots.
- **Manual entry**: Human error on forms or spreadsheets.
- **Monitoring gap**: Maturity gaps that allow the issue to surface undetected.
- **Schema/constraint change**: System changes without aligning interfaces.

## Optional Flags
- **recurring_issue_flag** (`yes`/`no`): Set to `yes` if the incident represents a regression or repeat occurrence.
- **last_seen_date**: Teaches when the issue was last observed before resolution.
</file>

<file path="docs/openapi/goodreads-mock.yaml">
openapi: 3.0.3
info:
  title: Goodreads Mock
  version: "0.2.0"
servers:
  - url: http://localhost:4010
paths:
  /health:
    get:
      responses:
        "200":
          description: ok
          content:
            application/json:
              example:
                status: ok
  /shelf/rss:
    get:
      parameters:
        - name: user
          in: query
          schema:
            type: string
        - name: shelf
          in: query
          schema:
            type: string
      responses:
        "200":
          description: Sample RSS feed
          content:
            application/rss+xml:
              schema:
                type: string
              examples:
                sample:
                  summary: Sample shelf RSS feed
                  externalValue: mock/goodreads/shelf.xml
  /shelf/export.csv:
    get:
      parameters:
        - name: user
          in: query
          schema:
            type: string
        - name: shelf
          in: query
          schema:
            type: string
      responses:
        "200":
          description: Goodreads export CSV
          content:
            text/csv:
              schema:
                type: string
              examples:
                sample:
                  summary: Sample Goodreads export
                  externalValue: mock/goodreads/export.csv
</file>

<file path="docs/architecture.md">
# Architecture (Phase 0)

## High-level

- `apps/web` is a Next.js UI.
- `services/api` is a FastAPI backend.
- `infra/docker-compose.yml` runs local Postgres + Redis.

## Data flow (today)

Web -> API: health checks only.

## Data flow (planned)

Web -> API -> Goodreads + Library adapters -> normalized database -> jobs/streams -> UI.
</file>

<file path="infra/docker-compose.prod.yml">
name: shelfsync

services:
  postgres:
    image: postgres:16-alpine
    environment:
      POSTGRES_USER: shelfsync
      POSTGRES_PASSWORD: shelfsync_change_me
      POSTGRES_DB: shelfsync
    volumes:
      - postgres_data:/var/lib/postgresql/data
    ports:
      - "127.0.0.1:5432:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U shelfsync -d shelfsync"]
      interval: 5s
      timeout: 5s
      retries: 10

  redis:
    image: redis:8-alpine
    command: ["redis-server", "--save", "", "--appendonly", "no"]
    ports:
      - "127.0.0.1:6379:6379"
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 3s
      retries: 10

  api:
    build:
      context: ./services/api
    command: sh -c "python -m alembic upgrade head && uvicorn app.main:app --host 0.0.0.0 --port 8000"
    environment:
      ENV: production
      DATABASE_URL: postgresql+psycopg2://shelfsync:shelfsync_change_me@postgres:5432/shelfsync
      REDIS_URL: redis://redis:6379/0

      AUTH_SECRET_KEY: "j7xY5vD3oUx4zCnYXgiNM63yGkkSxjvyLyi1aKUqvdbk9We_GiCJdwMiRg1dpocD"
      AUTH_COOKIE_SECURE: "true"
      AUTH_COOKIE_SAMESITE: "lax"

      CATALOG_PROVIDER: fixture
      FIXTURE_CATALOG_PATH: app/fixtures/catalog_fixture.json

      OTEL_ENABLED: "false"
    ports:
      - "127.0.0.1:8000:8000"
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy

  worker:
    build:
      context: ./services/api
    command: sh -c "rq worker -u redis://redis:6379/0"
    environment:
      ENV: production
      DATABASE_URL: postgresql+psycopg2://shelfsync:shelfsync_change_me@postgres:5432/shelfsync
      REDIS_URL: redis://redis:6379/0
      AUTH_SECRET_KEY: "CHANGE_ME_LONG_RANDOM"
    depends_on:
      redis:
        condition: service_healthy
      api:
        condition: service_started

volumes:
  postgres_data:
</file>

<file path="infra/goodreads-mock.yaml">
openapi: 3.0.3
info:
  title: Goodreads Mock
  version: "0.1.0"
paths:
  /health:
    get:
      responses:
        "200":
          description: ok
          content:
            application/json:
              example:
                status: ok
  /shelf/rss:
    get:
      parameters:
        - name: user
          in: query
          schema:
            type: string
        - name: shelf
          in: query
          schema:
            type: string
      responses:
        "200":
          description: Sample RSS feed
          content:
            application/rss+xml:
              example: |
                <?xml version="1.0" encoding="UTF-8"?>
                <rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/">
                  <channel>
                    <title>Goodreads Shelf</title>
                    <item>
                      <title>The Great Gatsby</title>
                      <dc:creator>F. Scott Fitzgerald</dc:creator>
                      <link>https://www.goodreads.com/book/show/4671.The_Great_Gatsby</link>
                      <description><![CDATA[ISBN: 9780743273565]]></description>
                    </item>
                  </channel>
                </rss>
</file>

<file path="mock/goodreads/export.csv">
Title,Author,ISBN13,ISBN,ASIN
The Great Gatsby,F. Scott Fitzgerald,9780743273565,0743273567,B000FC1PWA
Pride and Prejudice,Jane Austen,9780141439518,0141439513,B000QCS8TW
Neuromancer,William Gibson,9780441569595,0441569595,B000FC2L5M
</file>

<file path="mock/goodreads/shelf.xml">
<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/">
  <channel>
    <title>Goodreads Shelf</title>
    <item>
      <title>The Great Gatsby</title>
      <dc:creator>F. Scott Fitzgerald</dc:creator>
      <link>https://www.goodreads.com/book/show/4671.The_Great_Gatsby</link>
      <description><![CDATA[ISBN: 9780743273565 ASIN: B000FC1PWA]]></description>
    </item>
    <item>
      <title>Pride and Prejudice</title>
      <dc:creator>Jane Austen</dc:creator>
      <link>https://www.goodreads.com/book/show/1885.Pride_and_Prejudice</link>
      <description><![CDATA[ISBN13: 9780141439518 ASIN: B000QCS8TW]]></description>
    </item>
  </channel>
</rss>
</file>

<file path="scripts/dev-env.sh">
#!/usr/bin/env bash

script_path=""
if [[ -n "${BASH_SOURCE[0]-}" ]]; then
  script_path="${BASH_SOURCE[0]}"
elif [[ -n "${ZSH_VERSION-}" ]]; then
  script_path="${(%):-%x}"
else
  script_path="$0"
fi

is_sourced=false
if [[ -n "${BASH_SOURCE[0]-}" ]]; then
  [[ "${BASH_SOURCE[0]}" != "${0}" ]] && is_sourced=true
elif [[ -n "${ZSH_VERSION-}" ]]; then
  [[ "${ZSH_EVAL_CONTEXT-}" == *:file ]] && is_sourced=true
fi

saved_shell_opts=""
if [[ "$is_sourced" == true && -n "${BASH_VERSION-}" ]]; then
  saved_shell_opts="$(set +o)"
fi

if [[ -n "${ZSH_VERSION-}" ]]; then
  emulate -L sh
fi

set -euo pipefail

# Source this file to export API env vars into your shell.
ROOT_DIR="$(cd "$(dirname "$script_path")/.." && pwd)"
ENV_FILE="$ROOT_DIR/services/api/.env"

if [[ ! -f "$ENV_FILE" ]]; then
  ENV_FILE="$ROOT_DIR/services/api/.env.example"
fi

env_file_exists=true
if [[ ! -f "$ENV_FILE" ]]; then
  echo "Missing env file at services/api/.env or services/api/.env.example" >&2
  env_file_exists=false
fi

load_env() {
  local mode="${1:-export}"
  local line key value export_line

  while IFS= read -r line || [[ -n "$line" ]]; do
    line="${line%$'\r'}"
    [[ -z "$line" || "$line" == \#* ]] && continue
    [[ "$line" != *"="* ]] && continue

    key="${line%%=*}"
    value="${line#*=}"

    key="${key#export }"
    key="${key#"${key%%[![:space:]]*}"}"
    key="${key%"${key##*[![:space:]]}"}"
    [[ -z "$key" ]] && continue

    value="${value#"${value%%[![:space:]]*}"}"
    value="${value%"${value##*[![:space:]]}"}"

    if [[ "$value" == \"*\" && "$value" == *\" ]]; then
      value="${value:1:-1}"
    elif [[ "$value" == \'*\' && "$value" == *\' ]]; then
      value="${value:1:-1}"
    fi

    if [[ "$mode" == "print" ]]; then
      printf 'export %s=%q\n' "$key" "$value"
    else
      printf -v export_line 'export %s=%q' "$key" "$value"
      eval "$export_line"
    fi
  done < "$ENV_FILE"
}

usage() {
  cat <<'USAGE'
Usage:
  source scripts/dev-env.sh
  eval "$(scripts/dev-env.sh --print)"
  scripts/dev-env.sh -- <command> [args...]
USAGE
}

if [[ "$is_sourced" == false ]]; then
  if [[ "${1-}" == "--print" ]]; then
    if [[ "$env_file_exists" == true ]]; then
      load_env "print"
    fi
    exit 0
  fi

  if [[ "${1-}" == "--" ]]; then
    shift
  fi

  if [[ $# -gt 0 ]]; then
    if [[ "$env_file_exists" == true ]]; then
      load_env "export"
    fi
    exec "$@"
  fi

  usage >&2
  exit 2
fi

if [[ "$env_file_exists" == true ]]; then
  load_env "export"
fi

if [[ "$is_sourced" == true && -n "${BASH_VERSION-}" ]]; then
  eval "$saved_shell_opts"
fi
</file>

<file path="scripts/dev.sh">
#!/usr/bin/env bash
set -euo pipefail

# Phase 0: start local backing services + run web/api in watch mode

ROOT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")/.." && pwd)"

cd "$ROOT_DIR"

docker compose -f infra/docker-compose.yml up -d

echo "Backends are starting..."

echo "\nStarting API (FastAPI + Uvicorn reload)"
(
  cd services/api
  source .venv/bin/activate
  uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
) &

API_PID=$!

echo "\nStarting Web (Next dev server)"
(
  cd apps/web
  npm run dev
) &

WEB_PID=$!

trap 'echo "\nShutting down..."; kill $API_PID $WEB_PID; docker compose -f infra/docker-compose.yml down' INT TERM

wait
</file>

<file path="services/api/alembic/versions/32191e629780_add_sync_runs_table.py">
"""add sync_runs table

Revision ID: 32191e629780
Revises: 156614e6d7d6
Create Date: 2025-12-24 07:08:52.372695

"""

from typing import Sequence, Union

import sqlalchemy as sa
from alembic import op

# revision identifiers, used by Alembic.
revision: str = "32191e629780"
down_revision: Union[str, Sequence[str], None] = "156614e6d7d6"
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    """Upgrade schema."""
    pass


def downgrade() -> None:
    """Downgrade schema."""
    pass
</file>

<file path="services/api/alembic/versions/7b7d94f4f3e1_create_sync_runs_and_notification_events.py">
"""create sync_runs and notification_events tables

Revision ID: 7b7d94f4f3e1
Revises: 06ccc00c800a
Create Date: 2025-12-25 00:00:00.000000

"""

from typing import Sequence, Union

import sqlalchemy as sa
from alembic import op

# revision identifiers, used by Alembic.
revision: str = "7b7d94f4f3e1"
down_revision: Union[str, None] = "06ccc00c800a"
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    # ---- sync_runs ---------------------------------------------------------
    op.create_table(
        "sync_runs",
        sa.Column("id", sa.String(length=36), nullable=False),
        sa.Column("user_id", sa.String(length=36), nullable=False),
        sa.Column("kind", sa.String(length=50), nullable=False),
        sa.Column("status", sa.String(length=20), nullable=False),
        sa.Column("progress_current", sa.Integer(), nullable=False, server_default="0"),
        sa.Column("progress_total", sa.Integer(), nullable=False, server_default="0"),
        sa.Column("error_message", sa.Text(), nullable=True),
        sa.Column("started_at", sa.DateTime(timezone=True), nullable=True),
        sa.Column("finished_at", sa.DateTime(timezone=True), nullable=True),
        sa.Column("created_at", sa.DateTime(timezone=True), nullable=False),
        sa.Column("updated_at", sa.DateTime(timezone=True), nullable=False),
        sa.ForeignKeyConstraint(["user_id"], ["users.id"], ondelete="CASCADE"),
        sa.PrimaryKeyConstraint("id"),
    )
    op.create_index(
        op.f("ix_sync_runs_user_id"), "sync_runs", ["user_id"], unique=False
    )
    op.create_index(op.f("ix_sync_runs_kind"), "sync_runs", ["kind"], unique=False)
    op.create_index(op.f("ix_sync_runs_status"), "sync_runs", ["status"], unique=False)

    # ---- notification_events ----------------------------------------------
    op.create_table(
        "notification_events",
        sa.Column("id", sa.String(length=36), nullable=False),
        sa.Column("user_id", sa.String(length=36), nullable=False),
        sa.Column("shelf_item_id", sa.String(length=36), nullable=False),
        sa.Column("format", sa.String(length=20), nullable=False),
        sa.Column("old_status", sa.String(length=20), nullable=False),
        sa.Column("new_status", sa.String(length=20), nullable=False),
        sa.Column("deep_link", sa.String(length=500), nullable=True),
        sa.Column("created_at", sa.DateTime(timezone=True), nullable=False),
        sa.Column("read_at", sa.DateTime(timezone=True), nullable=True),
        sa.ForeignKeyConstraint(["user_id"], ["users.id"], ondelete="CASCADE"),
        sa.ForeignKeyConstraint(
            ["shelf_item_id"], ["shelf_items.id"], ondelete="CASCADE"
        ),
        sa.PrimaryKeyConstraint("id"),
    )
    op.create_index(
        op.f("ix_notification_events_user_id"),
        "notification_events",
        ["user_id"],
        unique=False,
    )
    op.create_index(
        op.f("ix_notification_events_shelf_item_id"),
        "notification_events",
        ["shelf_item_id"],
        unique=False,
    )
    op.create_index(
        op.f("ix_notification_events_created_at"),
        "notification_events",
        ["created_at"],
        unique=False,
    )


def downgrade() -> None:
    # notification_events
    op.drop_index(
        op.f("ix_notification_events_created_at"), table_name="notification_events"
    )
    op.drop_index(
        op.f("ix_notification_events_shelf_item_id"), table_name="notification_events"
    )
    op.drop_index(
        op.f("ix_notification_events_user_id"), table_name="notification_events"
    )
    op.drop_table("notification_events")

    # sync_runs
    op.drop_index(op.f("ix_sync_runs_status"), table_name="sync_runs")
    op.drop_index(op.f("ix_sync_runs_kind"), table_name="sync_runs")
    op.drop_index(op.f("ix_sync_runs_user_id"), table_name="sync_runs")
    op.drop_table("sync_runs")
</file>

<file path="services/api/alembic/README">
Generic single-database configuration.
</file>

<file path="services/api/alembic/script.py.mako">
"""${message}

Revision ID: ${up_revision}
Revises: ${down_revision | comma,n}
Create Date: ${create_date}

"""
from typing import Sequence, Union

from alembic import op
import sqlalchemy as sa
${imports if imports else ""}

# revision identifiers, used by Alembic.
revision: str = ${repr(up_revision)}
down_revision: Union[str, Sequence[str], None] = ${repr(down_revision)}
branch_labels: Union[str, Sequence[str], None] = ${repr(branch_labels)}
depends_on: Union[str, Sequence[str], None] = ${repr(depends_on)}


def upgrade() -> None:
    """Upgrade schema."""
    ${upgrades if upgrades else "pass"}


def downgrade() -> None:
    """Downgrade schema."""
    ${downgrades if downgrades else "pass"}
</file>

<file path="services/api/app/api/routes/libraries.py">
from __future__ import annotations

from app.api.deps import get_current_user
from app.db.session import get_db
from app.models.library import Library
from app.schemas.library import LibraryOut
from fastapi import APIRouter, Depends
from sqlalchemy.orm import Session

router = APIRouter(prefix="/v1", tags=["libraries"])


@router.get("/libraries", response_model=list[LibraryOut])
def list_libraries(db: Session = Depends(get_db), user=Depends(get_current_user)):
    # user dependency enforces auth
    return db.query(Library).order_by(Library.name.asc()).all()
</file>

<file path="services/api/app/api/__init__.py">

</file>

<file path="services/api/app/core/redis.py">
from __future__ import annotations

from functools import lru_cache

import redis
import redis.asyncio as redis_async


@lru_cache
def get_redis(url: str) -> redis.Redis:
    return redis.Redis.from_url(url, decode_responses=True)


def get_redis_async(url: str) -> redis_async.Redis:
    return redis_async.Redis.from_url(url, decode_responses=False)
</file>

<file path="services/api/app/crud/catalog_matches.py">
from __future__ import annotations

from app.models.catalog_match import CatalogMatch
from sqlalchemy import select
from sqlalchemy.orm import Session


def get_matches_for_shelf_item(
    db: Session,
    *,
    shelf_item_id: str,
    user_id: str | None = None,
    provider: str | None = None,
) -> list[CatalogMatch]:
    """Return all catalog matches for a given shelf item.

    Optional filters:
    - user_id: scope matches to a user (recommended)
    - provider: scope matches to a provider (e.g. "overdrive")
    """
    stmt = select(CatalogMatch).where(CatalogMatch.shelf_item_id == shelf_item_id)

    if user_id is not None:
        stmt = stmt.where(CatalogMatch.user_id == user_id)

    if provider is not None:
        stmt = stmt.where(CatalogMatch.provider == provider)

    # Prefer high-confidence matches first.
    stmt = stmt.order_by(CatalogMatch.confidence.desc(), CatalogMatch.updated_at.desc())

    return list(db.execute(stmt).scalars().all())
</file>

<file path="services/api/app/db/session.py">
from __future__ import annotations

from typing import Generator

from app.core.config import settings
from sqlalchemy import create_engine
from sqlalchemy.orm import Session, sessionmaker

engine = create_engine(
    settings.database_url,
    pool_pre_ping=True,
)

SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)


def get_db() -> Generator[Session, None, None]:
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()
</file>

<file path="services/api/app/fixtures/catalog_fixture.json">
{
  "provider": "fixture",
  "items": [
    {
      "provider_item_id": "fx_001",
      "title": "Project Hail Mary",
      "author": "Andy Weir",
      "isbn13": "9780593135204",
      "formats": {
        "ebook": {
          "status": "available",
          "copies_available": 3,
          "copies_total": 8,
          "holds": 12,
          "deep_link": "https://example.invalid/libby/ebook/fx_001"
        },
        "audiobook": {
          "status": "hold",
          "copies_available": 0,
          "copies_total": 4,
          "holds": 44,
          "deep_link": "https://example.invalid/libby/audiobook/fx_001"
        }
      }
    },
    {
      "provider_item_id": "fx_002",
      "title": "The Hobbit",
      "author": "J.R.R. Tolkien",
      "isbn13": "9780547928227",
      "formats": {
        "ebook": {
          "status": "hold",
          "copies_available": 0,
          "copies_total": 2,
          "holds": 120,
          "deep_link": "https://example.invalid/libby/ebook/fx_002"
        },
        "audiobook": {
          "status": "available",
          "copies_available": 1,
          "copies_total": 2,
          "holds": 5,
          "deep_link": "https://example.invalid/libby/audiobook/fx_002"
        }
      }
    },
    {
      "provider_item_id": "fx_003",
      "title": "The Pragmatic Programmer",
      "author": "Andrew Hunt; David Thomas",
      "isbn13": "9780135957059",
      "formats": {
        "ebook": {
          "status": "not_owned",
          "deep_link": "https://example.invalid/libby/ebook/fx_003"
        },
        "audiobook": {
          "status": "not_owned",
          "deep_link": "https://example.invalid/libby/audiobook/fx_003"
        }
      }
    },
    {
      "provider_item_id": "fx_004",
      "title": "Dune",
      "author": "Frank Herbert",
      "isbn13": "9780441013593",
      "formats": {
        "ebook": {
          "status": "available",
          "copies_available": 2,
          "copies_total": 6,
          "holds": 18,
          "deep_link": "https://example.invalid/libby/ebook/fx_004"
        },
        "audiobook": {
          "status": "hold",
          "copies_available": 0,
          "copies_total": 3,
          "holds": 66,
          "deep_link": "https://example.invalid/libby/audiobook/fx_004"
        }
      }
    }
  ]
}
</file>

<file path="services/api/app/middleware/request_id.py">
import uuid

from starlette.middleware.base import BaseHTTPMiddleware


class RequestIdMiddleware(BaseHTTPMiddleware):
    async def dispatch(self, request, call_next):
        req_id = request.headers.get("X-Request-Id") or str(uuid.uuid4())
        request.state.request_id = req_id
        response = await call_next(request)
        response.headers["X-Request-Id"] = req_id
        return response
</file>

<file path="services/api/app/models/base.py">
from __future__ import annotations

from sqlalchemy.orm import DeclarativeBase


class Base(DeclarativeBase):
    pass
</file>

<file path="services/api/app/models/library.py">
from __future__ import annotations

from app.models.base import Base
from sqlalchemy import String
from sqlalchemy.orm import Mapped, mapped_column


class Library(Base):
    __tablename__ = "libraries"

    # A stable identifier you choose (later phases can replace with provider-driven IDs)
    id: Mapped[str] = mapped_column(String(80), primary_key=True)
    name: Mapped[str] = mapped_column(String(200), nullable=False)
</file>

<file path="services/api/app/providers/types.py">
from __future__ import annotations

from dataclasses import dataclass

from app.services.catalog.types import ProviderAvailability


@dataclass(frozen=True)
class AvailabilityResult:
    """Holds availability data keyed by the catalog item that owns it."""

    catalog_item_id: str
    availability: ProviderAvailability
</file>

<file path="services/api/app/schemas/books.py">
from __future__ import annotations

from datetime import datetime

from app.schemas.dashboard import AvailabilityOut, ReadNextOut
from pydantic import BaseModel


class BookDetailShelfItemOut(BaseModel):
    id: str
    title: str
    author: str | None
    isbn10: str | None
    isbn13: str | None
    asin: str | None
    shelf: str | None
    needs_fuzzy_match: bool


class BookDetailMatchOut(BaseModel):
    catalog_item_id: str
    provider: str
    provider_item_id: str
    method: str
    confidence: float


class BookDetailSourceOut(BaseModel):
    id: str
    source_type: str
    provider: str
    source_ref: str
    last_synced_at: datetime | None
    last_sync_status: str | None
    last_sync_error: str | None


class BookDetailSettingsOut(BaseModel):
    library_system: str | None
    preferred_formats: list[str]


class BookDetailOut(BaseModel):
    shelf_item: BookDetailShelfItemOut
    match: BookDetailMatchOut | None
    availability: list[AvailabilityOut]
    source: BookDetailSourceOut | None
    settings: BookDetailSettingsOut
    read_next: ReadNextOut
    generated_at: datetime
</file>

<file path="services/api/app/schemas/library.py">
from __future__ import annotations

from pydantic import BaseModel


class LibraryOut(BaseModel):
    id: str
    name: str

    class Config:
        from_attributes = True
</file>

<file path="services/api/app/schemas/matching.py">
from __future__ import annotations

from datetime import datetime

from pydantic import BaseModel


class CatalogItemOut(BaseModel):
    id: str
    provider: str
    provider_item_id: str
    title: str
    author: str | None
    isbn10: str | None
    isbn13: str | None
    asin: str | None


class AvailabilityOut(BaseModel):
    format: str
    status: str
    copies_available: int | None
    copies_total: int | None
    holds: int | None
    deep_link: str | None
    last_checked_at: datetime


class MatchOut(BaseModel):
    shelf_item_id: str
    catalog_item: CatalogItemOut
    method: str
    confidence: float
    evidence: dict
    availability: list[AvailabilityOut]


class RefreshEnqueuedOut(BaseModel):
    job_id: str


class JobStatusOut(BaseModel):
    id: str
    status: str
    created_at: datetime | None
    started_at: datetime | None
    ended_at: datetime | None
    result: dict | None
    exc_info: str | None
</file>

<file path="services/api/app/services/catalog/factory.py">
from __future__ import annotations

from functools import lru_cache

from app.core.config import settings
from app.services.catalog.fixture_provider import FixtureProvider
from app.services.catalog.overdrive_provider import OverDriveProvider
from app.services.catalog.provider import CatalogProvider


@lru_cache
def get_provider() -> CatalogProvider:
    if settings.catalog_provider == "fixture":
        return FixtureProvider(fixture_path=settings.fixture_catalog_path)
    if settings.catalog_provider == "overdrive":
        return OverDriveProvider()
    raise ValueError(f"Unknown catalog provider: {settings.catalog_provider}")
</file>

<file path="services/api/app/services/catalog/fixture_provider.py">
from __future__ import annotations

import json
import re
from pathlib import Path

from app.services.catalog.types import (
    AvailabilityStatus,
    Format,
    ProviderAvailability,
    ProviderBook,
)


def _norm(s: str) -> str:
    s = s.lower().strip()
    s = re.sub(r"[^a-z0-9\s]", " ", s)
    s = re.sub(r"\s+", " ", s)
    return s


def _norm_isbn(s: str | None) -> str | None:
    if not s:
        return None
    digits = re.sub(r"[^0-9xX]", "", s)
    return digits.upper() if digits else None


class FixtureProvider:
    name = "fixture"

    def __init__(self, fixture_path: str):
        self.fixture_path = fixture_path
        self._data = self._load()

    def _load(self) -> dict:
        p = Path(self.fixture_path)
        if not p.exists():
            raise FileNotFoundError(f"Fixture file not found: {p}")
        raw = p.read_text(encoding="utf-8")
        return json.loads(raw)

    async def search(
        self,
        *,
        title: str | None,
        author: str | None,
        isbn10: str | None,
        isbn13: str | None,
        limit: int = 10,
    ) -> list[ProviderBook]:
        items: list[dict] = self._data.get("items", [])
        q_title = _norm(title or "")
        q_author = _norm(author or "")
        q_isbn10 = _norm_isbn(isbn10)
        q_isbn13 = _norm_isbn(isbn13)

        out: list[ProviderBook] = []
        for it in items:
            it_isbn13 = _norm_isbn(it.get("isbn13"))
            it_isbn10 = _norm_isbn(it.get("isbn10"))

            # ISBN: strongest filter
            if q_isbn13 and it_isbn13 == q_isbn13:
                out.append(self._to_book(it))
                continue
            if q_isbn10 and it_isbn10 == q_isbn10:
                out.append(self._to_book(it))
                continue

            # Otherwise: basic substring match on normalized fields
            t = _norm(it.get("title", ""))
            a = _norm(it.get("author", ""))

            if q_title and q_title not in t:
                continue
            if q_author and q_author not in a:
                # allow title-only searches
                if q_title:
                    pass
                else:
                    continue

            out.append(self._to_book(it))

        return out[:limit]

    async def availability_bulk(
        self, *, provider_item_ids: list[str]
    ) -> list[ProviderAvailability]:
        wanted = set(provider_item_ids)
        out: list[ProviderAvailability] = []

        for it in self._data.get("items", []):
            if it.get("provider_item_id") not in wanted:
                continue
            formats: dict = it.get("formats", {})
            for fmt_key, payload in formats.items():
                out.append(
                    ProviderAvailability(
                        provider=self.name,
                        provider_item_id=it["provider_item_id"],
                        format=Format(fmt_key),
                        status=AvailabilityStatus(payload["status"]),
                        copies_available=payload.get("copies_available"),
                        copies_total=payload.get("copies_total"),
                        holds=payload.get("holds"),
                        deep_link=payload.get("deep_link"),
                    )
                )

        return out

    def _to_book(self, it: dict) -> ProviderBook:
        return ProviderBook(
            provider=self.name,
            provider_item_id=it["provider_item_id"],
            title=it.get("title") or "",
            author=it.get("author"),
            isbn10=it.get("isbn10"),
            isbn13=it.get("isbn13"),
            asin=it.get("asin"),
            raw=it,
        )
</file>

<file path="services/api/app/services/catalog/provider.py">
from __future__ import annotations

from typing import Protocol

from app.services.catalog.types import ProviderAvailability, ProviderBook


class CatalogProvider(Protocol):
    name: str

    async def search(
        self,
        *,
        title: str | None,
        author: str | None,
        isbn10: str | None,
        isbn13: str | None,
        limit: int = 10,
    ) -> list[ProviderBook]: ...

    async def availability_bulk(
        self, *, provider_item_ids: list[str]
    ) -> list[ProviderAvailability]: ...
</file>

<file path="services/api/app/services/catalog/types.py">
from __future__ import annotations

from enum import Enum
from typing import Any

from pydantic import BaseModel, Field


class Format(str, Enum):
    ebook = "ebook"
    audiobook = "audiobook"


class AvailabilityStatus(str, Enum):
    available = "available"
    hold = "hold"
    not_owned = "not_owned"


class ProviderBook(BaseModel):
    provider: str
    provider_item_id: str

    title: str
    author: str | None = None

    isbn10: str | None = None
    isbn13: str | None = None
    asin: str | None = None

    # arbitrary provider metadata (deep links, formats, etc.)
    raw: dict[str, Any] = Field(default_factory=dict)


class ProviderAvailability(BaseModel):
    provider: str
    provider_item_id: str

    format: Format
    status: AvailabilityStatus

    copies_available: int | None = None
    copies_total: int | None = None
    holds: int | None = None

    deep_link: str | None = None
</file>

<file path="services/api/app/services/matching/matcher.py">
from __future__ import annotations

import re
from dataclasses import dataclass
from difflib import SequenceMatcher

from app.models.shelf_item import ShelfItem
from app.services.catalog.provider import CatalogProvider
from app.services.catalog.types import ProviderBook


def _norm_text(s: str) -> str:
    s = s.lower().strip()
    s = re.sub(r"[^a-z0-9\s]", " ", s)
    s = re.sub(r"\s+", " ", s)
    return s


def _ratio(a: str, b: str) -> float:
    if not a or not b:
        return 0.0
    return SequenceMatcher(None, a, b).ratio()


@dataclass(frozen=True)
class MatchResult:
    book: ProviderBook
    method: str  # isbn | fuzzy
    confidence: float
    evidence: dict


async def match_shelf_item(
    provider: CatalogProvider, item: ShelfItem, *, limit: int = 10
) -> MatchResult | None:
    # 1) ISBN exact
    candidates = await provider.search(
        title=item.title,
        author=item.author,
        isbn10=item.isbn10,
        isbn13=item.isbn13,
        limit=limit,
    )

    if item.isbn13:
        for c in candidates:
            if c.isbn13 and c.isbn13.replace("-", "") == item.isbn13.replace("-", ""):
                return MatchResult(
                    book=c,
                    method="isbn",
                    confidence=1.0,
                    evidence={"reason": "isbn13 exact", "candidates": [c.model_dump()]},
                )

    if item.isbn10:
        for c in candidates:
            if c.isbn10 and c.isbn10.replace("-", "") == item.isbn10.replace("-", ""):
                return MatchResult(
                    book=c,
                    method="isbn",
                    confidence=1.0,
                    evidence={"reason": "isbn10 exact", "candidates": [c.model_dump()]},
                )

    # 2) Fuzzy (title + author)
    t = _norm_text(item.title or "")
    a = _norm_text(item.author or "")

    scored: list[tuple[float, float, ProviderBook]] = []
    for c in candidates:
        ct = _norm_text(c.title or "")
        ca = _norm_text(c.author or "")
        title_score = _ratio(t, ct)
        author_score = _ratio(a, ca) if a and ca else 0.5
        combined = 0.75 * title_score + 0.25 * author_score
        scored.append((combined, title_score, c))

    if not scored:
        return None

    scored.sort(key=lambda x: (x[0], x[1]), reverse=True)
    best_combined, best_title, best = scored[0]

    # Threshold: tune later; keep conservative to avoid false positives
    if best_combined < 0.72:
        return None

    evidence = {
        "threshold": 0.72,
        "title_norm": t,
        "author_norm": a,
        "best": {
            "combined": best_combined,
            "title_score": best_title,
            "book": best.model_dump(),
        },
        "top_candidates": [
            {
                "combined": s[0],
                "title_score": s[1],
                "book": s[2].model_dump(),
            }
            for s in scored[:5]
        ],
    }

    return MatchResult(
        book=best, method="fuzzy", confidence=float(best_combined), evidence=evidence
    )
</file>

<file path="services/api/app/services/matching/persist.py">
from __future__ import annotations

from datetime import datetime, timezone

from app.models.availability_snapshot import AvailabilitySnapshot
from app.models.catalog_item import CatalogItem
from app.models.catalog_match import CatalogMatch
from app.services.catalog.types import ProviderAvailability, ProviderBook
from sqlalchemy import select
from sqlalchemy.orm import Session


def upsert_catalog_item(db: Session, book: ProviderBook) -> CatalogItem:
    existing = db.execute(
        select(CatalogItem)
        .where(CatalogItem.provider == book.provider)
        .where(CatalogItem.provider_item_id == book.provider_item_id)
    ).scalar_one_or_none()

    if existing:
        existing.title = book.title
        existing.author = book.author
        existing.isbn10 = book.isbn10
        existing.isbn13 = book.isbn13
        existing.asin = book.asin
        existing.raw = book.raw
        return existing

    item = CatalogItem(
        provider=book.provider,
        provider_item_id=book.provider_item_id,
        title=book.title,
        author=book.author,
        isbn10=book.isbn10,
        isbn13=book.isbn13,
        asin=book.asin,
        raw=book.raw,
    )
    db.add(item)
    return item


def upsert_match(
    db: Session,
    *,
    user_id: str,
    shelf_item_id: str,
    catalog_item_id: str,
    provider: str,
    method: str,
    confidence: float,
    evidence: dict,
) -> CatalogMatch:
    existing = db.execute(
        select(CatalogMatch)
        .where(CatalogMatch.user_id == user_id)
        .where(CatalogMatch.shelf_item_id == shelf_item_id)
    ).scalar_one_or_none()

    if existing:
        existing.catalog_item_id = catalog_item_id
        existing.provider = provider
        existing.method = method
        existing.confidence = confidence
        existing.evidence = evidence
        return existing

    m = CatalogMatch(
        user_id=user_id,
        shelf_item_id=shelf_item_id,
        catalog_item_id=catalog_item_id,
        provider=provider,
        method=method,
        confidence=confidence,
        evidence=evidence,
    )
    db.add(m)
    return m


def upsert_availability_snapshot(
    db: Session,
    *,
    user_id: str,
    catalog_item_id: str,
    a: ProviderAvailability,
) -> AvailabilitySnapshot:
    existing = db.execute(
        select(AvailabilitySnapshot)
        .where(AvailabilitySnapshot.user_id == user_id)
        .where(AvailabilitySnapshot.catalog_item_id == catalog_item_id)
        .where(AvailabilitySnapshot.format == a.format.value)
    ).scalar_one_or_none()

    now = datetime.now(timezone.utc)

    if existing:
        existing.status = a.status.value
        existing.copies_available = a.copies_available
        existing.copies_total = a.copies_total
        existing.holds = a.holds
        existing.deep_link = a.deep_link
        existing.last_checked_at = now
        return existing

    row = AvailabilitySnapshot(
        user_id=user_id,
        catalog_item_id=catalog_item_id,
        format=a.format.value,
        status=a.status.value,
        copies_available=a.copies_available,
        copies_total=a.copies_total,
        holds=a.holds,
        deep_link=a.deep_link,
        last_checked_at=now,
    )
    db.add(row)
    return row
</file>

<file path="services/api/app/services/read_next/scoring.py">
from __future__ import annotations

from dataclasses import dataclass
from typing import Any


def _ci(s: str | None) -> str:
    return (s or "").strip().casefold()


def _fmt(fmt: str | None) -> str:
    return _ci(fmt)


def estimate_hold_pressure(holds: int | None, copies_total: int | None) -> float | None:
    if holds is None or copies_total is None or copies_total <= 0:
        return None
    return holds / copies_total


@dataclass(frozen=True)
class ReadNextScore:
    score: float
    tier: str
    best_format: str | None
    hold_ratio: float | None
    reasons: list[str]


def score_candidates(
    availability: list[dict[str, Any]],
    preferred_formats: list[str],
) -> ReadNextScore:
    preferred = [_fmt(f) for f in preferred_formats]

    best_score = -1.0
    best_tier = "not_owned"
    best_format: str | None = None
    best_hold_ratio: float | None = None
    reasons: list[str] = []

    for a in availability:
        fmt = _fmt(a.get("format"))
        status = _ci(a.get("status"))

        copies_available = a.get("copies_available")
        copies_total = a.get("copies_total")
        holds = a.get("holds")

        tier = "not_owned"
        base = 0.0
        if status == "available" and (copies_available or 0) > 0:
            tier = "available"
            base = 100.0
        elif status in {"hold", "holds"}:
            tier = "hold"
            base = 50.0
        elif status == "not_owned":
            tier = "not_owned"
            base = 0.0
        else:
            tier = "unknown"
            base = 10.0

        fmt_bonus = 0.0
        if fmt in preferred:
            fmt_bonus = 10.0 * (len(preferred) - preferred.index(fmt))

        hold_ratio = estimate_hold_pressure(holds, copies_total)
        pressure_penalty = 0.0
        if tier == "hold" and hold_ratio is not None:
            pressure_penalty = min(25.0, hold_ratio * 5.0)

        score = base + fmt_bonus - pressure_penalty

        if score > best_score:
            best_score = score
            best_tier = tier
            best_format = fmt or None
            best_hold_ratio = hold_ratio

    reasons.append(f"tier={best_tier}")
    if best_format:
        reasons.append(f"format={best_format}")

    return ReadNextScore(
        score=float(best_score),
        tier=best_tier,
        best_format=best_format,
        hold_ratio=best_hold_ratio,
        reasons=reasons,
    )
</file>

<file path="services/api/app/services/availability_cache.py">
from __future__ import annotations

import json
from dataclasses import dataclass
from datetime import datetime, timezone

from app.core.config import settings
from app.core.redis_client import get_redis
from app.services.catalog.factory import get_provider
from app.services.catalog.types import AvailabilityStatus, Format, ProviderAvailability
from app.workers.async_utils import run_async


@dataclass(frozen=True)
class CachedAvailability:
    availability: ProviderAvailability
    last_checked_at: datetime


def _availability_key(
    *, provider: str, library_system: str, provider_item_id: str, fmt: str
) -> str:
    return f"availability:{provider}:{library_system}:{provider_item_id}:{fmt}"


def _now_utc() -> datetime:
    return datetime.now(timezone.utc)


def get_availability_cached(
    *,
    library_system: str,
    provider_item_ids: list[str],
    formats: list[str],
) -> dict[tuple[str, str], CachedAvailability]:
    """Return availability per (provider_item_id, format).

    Behavior:
    - Reads from Redis first.
    - Only calls provider for cache misses.
    - Writes misses back to Redis with TTL.

    Returns a map keyed by (provider_item_id, format).
    """

    provider = get_provider()
    provider_name = getattr(provider, "name", "unknown")

    r = get_redis()
    ttl = int(settings.availability_cache_ttl_secs)
    wanted = [(pid, fmt) for pid in provider_item_ids for fmt in formats]

    out: dict[tuple[str, str], CachedAvailability] = {}
    missing_provider_ids: set[str] = set()

    # 1) Try cache
    if r is not None:
        pipe = r.pipeline()
        keys = [
            _availability_key(
                provider=provider_name,
                library_system=library_system,
                provider_item_id=pid,
                fmt=fmt,
            )
            for (pid, fmt) in wanted
        ]
        for k in keys:
            pipe.get(k)
        values = pipe.execute()

        for (pid, fmt), raw in zip(wanted, values):
            if not raw:
                missing_provider_ids.add(pid)
                continue
            try:
                payload = json.loads(raw)
                avail = ProviderAvailability.model_validate(payload["availability"])
                last_checked = datetime.fromisoformat(payload["last_checked_at"])
                out[(pid, fmt)] = CachedAvailability(
                    availability=avail, last_checked_at=last_checked
                )
            except Exception:
                missing_provider_ids.add(pid)

    else:
        missing_provider_ids = set(provider_item_ids)

    # 2) Fetch misses
    if missing_provider_ids:
        fetched = run_async(
            provider.availability_bulk(provider_item_ids=sorted(missing_provider_ids))
        )

        checked_at = _now_utc()

        # Store provider results
        for item in fetched:
            fmt = str(item.format)
            key = (item.provider_item_id, fmt)
            out[key] = CachedAvailability(availability=item, last_checked_at=checked_at)

        # Fill any requested (pid, fmt) that provider didn't return
        for pid in missing_provider_ids:
            for fmt in formats:
                if (pid, fmt) in out:
                    continue
                out[(pid, fmt)] = CachedAvailability(
                    availability=ProviderAvailability(
                        provider=provider_name,
                        provider_item_id=pid,
                        format=Format(fmt),
                        status=AvailabilityStatus.not_owned,
                        copies_available=None,
                        copies_total=None,
                        holds=None,
                        deep_link=None,
                    ),
                    last_checked_at=checked_at,
                )

        # Write to Redis
        if r is not None:
            pipe = r.pipeline()
            for (pid, fmt), entry in out.items():
                if pid not in missing_provider_ids:
                    continue
                k = _availability_key(
                    provider=provider_name,
                    library_system=library_system,
                    provider_item_id=pid,
                    fmt=fmt,
                )
                payload = {
                    "availability": entry.availability.model_dump(mode="json"),
                    "last_checked_at": entry.last_checked_at.isoformat(),
                }
                pipe.setex(k, ttl, json.dumps(payload))
            pipe.execute()

    return out
</file>

<file path="services/api/app/services/read_next_scoring.py">
from __future__ import annotations

from dataclasses import dataclass
from typing import Any, Sequence


@dataclass(frozen=True)
class ReadNextScore:
    score: float
    tier: str  # available | hold | not_owned
    best_format: str | None
    hold_ratio: float | None
    reasons: list[str]


# Large tier weights ensure ‚Äúavailable now‚Äù dominates ‚Äúhold‚Äù, etc.
_STATUS_WEIGHT: dict[str, float] = {
    "available": 1000.0,
    "hold": 500.0,
    "not_owned": 0.0,
}


def _get(obj: Any, key: str) -> Any:
    if isinstance(obj, dict):
        return obj.get(key)
    return getattr(obj, key, None)


def compute_read_next(
    availability: Sequence[Any] | None,
    preferred_formats: Sequence[str],
) -> ReadNextScore:
    """Compute a deterministic, explainable score.

    `availability` can be a list of dicts (book detail endpoint) or a list of
    Pydantic models (dashboard endpoint).

    The algorithm:
    - Filter to preferred formats
    - Score each format candidate
    - Choose best candidate
    - Return score + tier + explanation

    NOTE: This is intentionally a pure function.
    """
    preferred = list(preferred_formats or [])
    avail = list(availability or [])

    if not avail:
        return ReadNextScore(
            score=_STATUS_WEIGHT["not_owned"],
            tier="not_owned",
            best_format=None,
            hold_ratio=None,
            reasons=[
                "No availability data (not owned or not checked yet)",
                "Tier: not_owned",
            ],
        )

    candidates: list[dict[str, Any]] = []

    for a in avail:
        fmt = _get(a, "format")
        if not fmt:
            continue
        if preferred and fmt not in preferred:
            continue

        status = _get(a, "status") or "not_owned"
        copies_available = _get(a, "copies_available")
        copies_total = _get(a, "copies_total")
        holds = _get(a, "holds")

        fmt_index = preferred.index(fmt) if (fmt in preferred) else 999
        # Earlier preferred formats get a larger, but bounded bonus.
        fmt_bonus = 20.0 / (fmt_index + 1) if fmt_index != 999 else 0.0

        base = _STATUS_WEIGHT.get(status, _STATUS_WEIGHT["not_owned"])

        hold_ratio: float | None = None
        hold_penalty = 0.0

        if status == "available":
            # Prefer more available copies, capped to avoid overpowering tier.
            ca = int(copies_available or 0)
            copies_bonus = float(min(max(ca, 0), 10))
        else:
            copies_bonus = 0.0

        if status == "hold":
            h = holds if holds is not None else None
            ct = copies_total if copies_total is not None else None

            if h is not None and ct is not None and int(ct) > 0:
                hold_ratio = float(h) / float(max(int(ct), 1))
                # penalty grows with queue ratio but is capped
                hold_penalty = min(hold_ratio * 25.0, 400.0)
            elif h is not None:
                # fallback: direct holds as penalty; also capped
                hold_penalty = min(float(h) * 2.0, 400.0)

        score = base + fmt_bonus + copies_bonus - hold_penalty

        candidates.append(
            {
                "fmt": fmt,
                "status": status,
                "score": score,
                "hold_ratio": hold_ratio,
                "copies_available": copies_available,
                "copies_total": copies_total,
                "holds": holds,
                "fmt_index": fmt_index,
            }
        )

    if not candidates:
        return ReadNextScore(
            score=_STATUS_WEIGHT["not_owned"],
            tier="not_owned",
            best_format=None,
            hold_ratio=None,
            reasons=["No preferred-format availability data", "Tier: not_owned"],
        )

    # Pick the highest numeric score. The tie-break here prefers better fmt index only
    # when scores are equal; overall stability is handled by API/UI sorting tie-breaks.
    best = max(candidates, key=lambda x: (x["score"], -x["fmt_index"]))

    tier = best["status"]
    best_format = best["fmt"]
    hold_ratio = best["hold_ratio"]

    reasons: list[str] = []

    if tier == "available":
        reasons.append(
            f"Available now in {best_format} (preferred #{best['fmt_index'] + 1})"
            if best_format in preferred
            else f"Available now in {best_format}"
        )
        if (
            best.get("copies_available") is not None
            or best.get("copies_total") is not None
        ):
            reasons.append(
                f"Copies available: {best.get('copies_available') or 0} / {best.get('copies_total') or 0}"
            )

    elif tier == "hold":
        reasons.append(
            f"On hold in {best_format} (preferred #{best['fmt_index'] + 1})"
            if best_format in preferred
            else f"On hold in {best_format}"
        )
        if (
            hold_ratio is not None
            and best.get("holds") is not None
            and best.get("copies_total") is not None
        ):
            reasons.append(
                f"Hold queue: {best.get('holds')} holds / {best.get('copies_total')} copies (ratio {hold_ratio:.2f})"
            )
        elif best.get("holds") is not None:
            reasons.append(f"Hold queue: {best.get('holds')} holds")
        else:
            reasons.append("Hold queue length unavailable")

    else:
        reasons.append("Not owned in your selected library/catalog")

    reasons.append(f"Tier: {tier}")

    return ReadNextScore(
        score=float(best["score"]),
        tier=tier,
        best_format=best_format,
        hold_ratio=hold_ratio,
        reasons=reasons,
    )
</file>

<file path="services/api/app/workers/redis_conn.py">
from __future__ import annotations

import redis
from app.core.config import settings
from redis import Redis


def get_redis_connection() -> Redis:
    """Return a Redis connection suitable for RQ (jobs + metadata)."""
    return redis.from_url(settings.redis_url)
</file>

<file path="services/api/app/__init__.py">

</file>

<file path="services/api/bin/alembic">
#!/usr/bin/env bash
set -euo pipefail

ROOT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")/.." && pwd)"
VENV_DIR="$ROOT_DIR/.venv"
PYTHON_BIN="$VENV_DIR/bin/python"

if [[ ! -x "$PYTHON_BIN" ]]; then
  echo "Expected a virtualenv at $VENV_DIR. Create it and install requirements first." >&2
  exit 1
fi

cd "$ROOT_DIR"
exec "$PYTHON_BIN" -m alembic "$@"
</file>

<file path="services/api/tests/test_health.py">
def test_health_check(client):
    response = client.get("/health")

    assert response.status_code == 200
    assert response.json() == {"ok": True}
</file>

<file path="services/api/tests/test_read_next_scoring.py">
from app.services.read_next_scoring import compute_read_next


def test_available_beats_hold_beats_not_owned():
    preferred = ["ebook", "audiobook"]

    available = [
        {
            "format": "ebook",
            "status": "available",
            "copies_available": 1,
            "copies_total": 1,
            "holds": 0,
        }
    ]
    hold = [
        {
            "format": "ebook",
            "status": "hold",
            "copies_available": 0,
            "copies_total": 2,
            "holds": 10,
        }
    ]
    not_owned = [
        {
            "format": "ebook",
            "status": "not_owned",
            "copies_available": 0,
            "copies_total": 0,
            "holds": 0,
        }
    ]

    s1 = compute_read_next(available, preferred)
    s2 = compute_read_next(hold, preferred)
    s3 = compute_read_next(not_owned, preferred)

    assert s1.tier == "available"
    assert s2.tier == "hold"
    assert s3.tier == "not_owned"


def test_preferred_format_wins_when_same_status():
    preferred = ["ebook", "audiobook"]

    ebook_avail = [{"format": "ebook", "status": "available", "copies_available": 1}]
    audio_avail = [
        {"format": "audiobook", "status": "available", "copies_available": 1}
    ]

    s_ebook = compute_read_next(ebook_avail, preferred)
    s_audio = compute_read_next(audio_avail, preferred)

    assert s_ebook.score > s_audio.score


def test_graceful_on_missing_fields():
    preferred = ["ebook"]
    # Missing copies/holds fields should not throw.
    weird = [{"format": "ebook", "status": "hold"}]
    s = compute_read_next(weird, preferred)
    assert s.tier == "hold"
    assert isinstance(s.reasons, list)
</file>

<file path="services/api/.dockerignore">
.venv/
__pycache__/
*.py[cod]
.pytest_cache/
.mypy_cache/
.coverage
.env
dist/
build/
</file>

<file path="services/api/alembic.ini">
# A generic, single database configuration.

[alembic]
# path to migration scripts.
# this is typically a path given in POSIX (e.g. forward slashes)
# format, relative to the token %(here)s which refers to the location of this
# ini file
script_location = %(here)s/alembic

# template used to generate migration file names; The default value is %%(rev)s_%%(slug)s
# Uncomment the line below if you want the files to be prepended with date and time
# see https://alembic.sqlalchemy.org/en/latest/tutorial.html#editing-the-ini-file
# for all available tokens
# file_template = %%(year)d_%%(month).2d_%%(day).2d_%%(hour).2d%%(minute).2d-%%(rev)s_%%(slug)s

# sys.path path, will be prepended to sys.path if present.
# defaults to the current working directory.  for multiple paths, the path separator
# is defined by "path_separator" below.
prepend_sys_path = .


# timezone to use when rendering the date within the migration file
# as well as the filename.
# If specified, requires the tzdata library which can be installed by adding
# `alembic[tz]` to the pip requirements.
# string value is passed to ZoneInfo()
# leave blank for localtime
# timezone =

# max length of characters to apply to the "slug" field
# truncate_slug_length = 40

# set to 'true' to run the environment during
# the 'revision' command, regardless of autogenerate
# revision_environment = false

# set to 'true' to allow .pyc and .pyo files without
# a source .py file to be detected as revisions in the
# versions/ directory
# sourceless = false

# version location specification; This defaults
# to <script_location>/versions.  When using multiple version
# directories, initial revisions must be specified with --version-path.
# The path separator used here should be the separator specified by "path_separator"
# below.
# version_locations = %(here)s/bar:%(here)s/bat:%(here)s/alembic/versions

# path_separator; This indicates what character is used to split lists of file
# paths, including version_locations and prepend_sys_path within configparser
# files such as alembic.ini.
# The default rendered in new alembic.ini files is "os", which uses os.pathsep
# to provide os-dependent path splitting.
#
# Note that in order to support legacy alembic.ini files, this default does NOT
# take place if path_separator is not present in alembic.ini.  If this
# option is omitted entirely, fallback logic is as follows:
#
# 1. Parsing of the version_locations option falls back to using the legacy
#    "version_path_separator" key, which if absent then falls back to the legacy
#    behavior of splitting on spaces and/or commas.
# 2. Parsing of the prepend_sys_path option falls back to the legacy
#    behavior of splitting on spaces, commas, or colons.
#
# Valid values for path_separator are:
#
# path_separator = :
# path_separator = ;
# path_separator = space
# path_separator = newline
#
# Use os.pathsep. Default configuration used for new projects.
path_separator = os

# set to 'true' to search source files recursively
# in each "version_locations" directory
# new in Alembic version 1.10
# recursive_version_locations = false

# the output encoding used when revision files
# are written from script.py.mako
# output_encoding = utf-8

# database URL.  This is consumed by the user-maintained env.py script only.
# other means of configuring database URLs may be customized within the env.py
# file.
sqlalchemy.url = driver://user:pass@localhost/dbname


[post_write_hooks]
# post_write_hooks defines scripts or Python functions that are run
# on newly generated revision scripts.  See the documentation for further
# detail and examples

# format using "black" - use the console_scripts runner, against the "black" entrypoint
# hooks = black
# black.type = console_scripts
# black.entrypoint = black
# black.options = -l 79 REVISION_SCRIPT_FILENAME

# lint with attempts to fix using "ruff" - use the module runner, against the "ruff" module
# hooks = ruff
# ruff.type = module
# ruff.module = ruff
# ruff.options = check --fix REVISION_SCRIPT_FILENAME

# Alternatively, use the exec runner to execute a binary found on your PATH
# hooks = ruff
# ruff.type = exec
# ruff.executable = ruff
# ruff.options = check --fix REVISION_SCRIPT_FILENAME

# Logging configuration.  This is also consumed by the user-maintained
# env.py script only.
[loggers]
keys = root,sqlalchemy,alembic

[handlers]
keys = console

[formatters]
keys = generic

[logger_root]
level = WARNING
handlers = console
qualname =

[logger_sqlalchemy]
level = WARNING
handlers =
qualname = sqlalchemy.engine

[logger_alembic]
level = INFO
handlers =
qualname = alembic

[handler_console]
class = StreamHandler
args = (sys.stderr,)
level = NOTSET
formatter = generic

[formatter_generic]
format = %(levelname)-5.5s [%(name)s] %(message)s
datefmt = %H:%M:%S
</file>

<file path="services/api/Dockerfile">
FROM python:3.14-slim

ENV PYTHONDONTWRITEBYTECODE=1 \
    PYTHONUNBUFFERED=1

WORKDIR /app

COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000"]
</file>

<file path=".editorconfig">
root = true

[*]
charset = utf-8
end_of_line = lf
insert_final_newline = true
indent_style = space
indent_size = 2
trim_trailing_whitespace = true

[*.py]
indent_size = 4
</file>

<file path=".nvmrc">
24
</file>

<file path=".python-version">
3.14
</file>

<file path="CODE_OF_CONDUCT.md">

</file>

<file path="cookies.txt">
# Netscape HTTP Cookie File
# https://curl.se/docs/http-cookies.html
# This file was generated by libcurl! Edit at your own risk.

#HttpOnly_localhost	FALSE	/	FALSE	1767150950	access_token	eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiI2ZmE5ZTkwMi05MDc2LTQzNmMtODI5Yi0yZjA0MDkxZmU4ZGUiLCJleHAiOjE3NjcxNTA5NTB9.JEhcZx6I0SfZaY3ibfHB6UrlnUzS39_ptL_7XcistGw
</file>

<file path="LICENSE">

</file>

<file path="SECURITY.md">

</file>

<file path="apps/web/src/app/api/proxy/sse/route.ts">
import { NextRequest } from "next/server";

export const runtime = "nodejs";

export async function GET(req: NextRequest) {
  const path = req.nextUrl.searchParams.get("path");
  if (!path) {
    return new Response("Missing path", { status: 400 });
  }

  const apiBase = process.env.NEXT_PUBLIC_API_BASE_URL || "http://localhost:8000";
  const targetUrl = new URL(path, apiBase);

  const res = await fetch(targetUrl, {
    method: "GET",
    headers: {
      // forward cookies for auth
      cookie: req.headers.get("cookie") || "",
      accept: "text/event-stream",
    },
    cache: "no-store",
  });

  return new Response(res.body, {
    status: res.status,
    headers: {
      "content-type": "text/event-stream",
      "cache-control": "no-cache, no-transform",
      connection: "keep-alive",
    },
  });
}
</file>

<file path="apps/web/src/app/globals.css">
@tailwind base;
@tailwind components;
@tailwind utilities;

:root {
  color-scheme: light;
  --ink: #1c1b1a;
  --paper: #fbf4ea;
  --paper-strong: #f5e8d7;
  --accent: #d95d39;
  --accent-2: #2a9d8f;
  --accent-3: #f4a261;
  --card: #fffaf2;
  --shadow: rgba(38, 31, 20, 0.12);
}

body {
  min-height: 100%;
  background-color: var(--paper);
  font-family: var(--font-body, ui-sans-serif, system-ui);
}

.font-display {
  font-family: var(--font-display, ui-serif, serif);
  letter-spacing: -0.02em;
}
</file>

<file path="apps/web/src/app/layout.tsx">
import type { Metadata } from "next";
import { Newsreader, Space_Grotesk } from "next/font/google";

import "./globals.css";

const displayFont = Newsreader({
  subsets: ["latin"],
  variable: "--font-display",
});

const bodyFont = Space_Grotesk({
  subsets: ["latin"],
  variable: "--font-body",
});

export const metadata: Metadata = {
  title: "ShelfSync",
  description: "Find library availability for your Goodreads shelves."
};

export default function RootLayout({
  children
}: {
  children: React.ReactNode;
}) {
  return (
    <html lang="en">
      <body
        className={`${displayFont.variable} ${bodyFont.variable} bg-[var(--paper)] text-[var(--ink)] antialiased`}
      >
        {children}
      </body>
    </html>
  );
}
</file>

<file path="apps/web/src/app/page.tsx">
import Link from "next/link";

import { TryDemoButton } from "@/components/TryDemoButton";

export default function Home() {
  return (
    <main className="min-h-screen">
      <div className="relative overflow-hidden">
        <div className="pointer-events-none absolute inset-0 -z-10">
          <div className="absolute -left-32 top-10 h-72 w-72 rounded-full bg-[radial-gradient(circle_at_center,_rgba(217,93,57,0.35),_rgba(217,93,57,0))] blur-2xl" />
          <div className="absolute right-10 top-24 h-80 w-80 rounded-full bg-[radial-gradient(circle_at_center,_rgba(42,157,143,0.35),_rgba(42,157,143,0))] blur-2xl" />
          <div className="absolute bottom-0 left-1/3 h-96 w-96 rounded-full bg-[radial-gradient(circle_at_center,_rgba(244,162,97,0.25),_rgba(244,162,97,0))] blur-3xl" />
        </div>

        <div className="mx-auto flex max-w-6xl flex-col gap-16 px-6 py-12 lg:py-20">
          <nav className="flex items-center justify-between text-sm">
            <div className="font-display text-xl tracking-tight">ShelfSync</div>
            <div className="flex items-center gap-4">
              <Link className="rounded-full border border-black/10 px-4 py-2 text-xs uppercase tracking-[0.2em]" href="/signin">
                Sign in
              </Link>
              <Link className="rounded-full bg-[var(--ink)] px-4 py-2 text-xs uppercase tracking-[0.2em] text-[var(--paper)]" href="/signup">
                Create account
              </Link>
            </div>
          </nav>

          <section className="grid items-center gap-12 lg:grid-cols-[1.1fr_0.9fr]">
            <div className="space-y-6">
              <p className="text-xs uppercase tracking-[0.5em] text-black/50">Goodreads ‚Üí Library</p>
              <h1 className="font-display text-4xl leading-tight sm:text-5xl">
                Sync your shelves, then see what your library can actually loan today.
              </h1>
              <p className="max-w-xl text-lg text-black/70">
                Import RSS or CSV exports from Goodreads, normalize the titles, and get a clean
                availability snapshot you can act on.
              </p>
              <div className="flex flex-wrap items-center gap-4">
                <TryDemoButton className="rounded-full bg-[var(--accent)] px-6 py-3 text-sm font-semibold text-white shadow-[0_18px_40px_-28px_var(--accent)]" />
                <Link className="rounded-full border border-black/20 px-6 py-3 text-sm font-semibold" href="/signup">
                  Start a new shelf
                </Link>
              </div>
            </div>

            <div className="rounded-3xl border border-black/10 bg-[var(--card)] p-6 shadow-[0_25px_60px_-40px_var(--shadow)]">
              <div className="space-y-4">
                <div className="flex items-center justify-between text-xs uppercase tracking-[0.3em] text-black/40">
                  <span>Connect</span>
                  <span>Import</span>
                  <span>Check</span>
                </div>
                <div className="rounded-2xl border border-black/10 bg-white/70 p-5">
                  <p className="text-sm font-semibold">Demo pipeline</p>
                  <div className="mt-4 space-y-3 text-sm text-black/60">
                    <div className="flex items-center justify-between">
                      <span>Goodreads RSS</span>
                      <span className="rounded-full bg-black/5 px-3 py-1 text-xs">connected</span>
                    </div>
                    <div className="flex items-center justify-between">
                      <span>CSV export</span>
                      <span className="rounded-full bg-black/5 px-3 py-1 text-xs">optional</span>
                    </div>
                    <div className="flex items-center justify-between">
                      <span>Availability scan</span>
                      <span className="rounded-full bg-black/5 px-3 py-1 text-xs">next</span>
                    </div>
                  </div>
                </div>
                <div className="grid gap-4 sm:grid-cols-2">
                  <div className="rounded-2xl border border-black/10 bg-white/70 p-4 text-sm">
                    <p className="text-xs uppercase tracking-[0.3em] text-black/40">Snapshot</p>
                    <p className="mt-3 text-2xl font-semibold text-[var(--accent)]">24</p>
                    <p className="text-xs text-black/50">titles ready to check</p>
                  </div>
                  <div className="rounded-2xl border border-black/10 bg-white/70 p-4 text-sm">
                    <p className="text-xs uppercase tracking-[0.3em] text-black/40">Coverage</p>
                    <p className="mt-3 text-2xl font-semibold text-[var(--accent-2)]">82%</p>
                    <p className="text-xs text-black/50">with ISBN metadata</p>
                  </div>
                </div>
              </div>
            </div>
          </section>

          <section className="grid gap-6 lg:grid-cols-3">
            {[
              {
                title: "Connect once",
                description: "Paste your Goodreads RSS or upload CSV exports. We store the source and keep it fresh."
              },
              {
                title: "Normalize titles",
                description: "We extract ISBNs, ASINs, and Goodreads IDs so every item is ready for matching."
              },
              {
                title: "Track availability",
                description: "See what your library can deliver today and what needs a manual check."
              }
            ].map((item) => (
              <div
                key={item.title}
                className="rounded-2xl border border-black/10 bg-white/70 p-6 shadow-[0_25px_60px_-48px_var(--shadow)]"
              >
                <p className="font-display text-xl">{item.title}</p>
                <p className="mt-3 text-sm text-black/60">{item.description}</p>
              </div>
            ))}
          </section>
        </div>
      </div>
    </main>
  );
}
</file>

<file path="apps/web/src/lib/api.ts">
export const API_BASE = process.env.NEXT_PUBLIC_API_BASE_URL;

if (!API_BASE) {
  throw new Error("Missing NEXT_PUBLIC_API_BASE_URL");
}

export async function apiFetch<T>(path: string, init?: RequestInit): Promise<T> {
  const isFormData = typeof FormData !== "undefined" && init?.body instanceof FormData;

  const headers: HeadersInit = {
    ...(isFormData ? {} : { "Content-Type": "application/json" }),
    ...(init?.headers || {}),
  };

  const res = await fetch(`${API_BASE}${path}`, {
    ...init,
    headers,
    credentials: "include",
  });

  if (!res.ok) {
    const text = await res.text();
    throw new Error(text || `Request failed: ${res.status}`);
  }

  // 204
  if (res.status === 204) return undefined as unknown as T;

  return (await res.json()) as T;
}
</file>

<file path="infra/docker-compose.full.yml">
name: shelfsync

services:
  postgres:
    image: postgres:18-alpine
    environment:
      POSTGRES_USER: shelfsync
      POSTGRES_PASSWORD: shelfsync
      POSTGRES_DB: shelfsync
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U shelfsync -d shelfsync"]
      interval: 5s
      timeout: 5s
      retries: 10

  redis:
    image: redis:8-alpine
    ports:
      - "6379:6379"
    command: ["redis-server", "--save", "", "--appendonly", "no"]
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 3s
      retries: 10

  goodreads-mock:
    image: stoplight/prism:5
    command: ["mock", "-h", "0.0.0.0", "/spec/goodreads-mock.yaml"]
    ports:
      - "4010:4010"
    volumes:
      - ../docs/openapi/goodreads-mock.yaml:/spec/goodreads-mock.yaml:ro
      - ../mock:/spec/mock:ro

  api:
    build:
      context: ../services/api
    command: sh -c "python -m alembic upgrade head && uvicorn app.main:app --host 0.0.0.0 --port 8000"
    environment:
      DATABASE_URL: postgresql+psycopg2://shelfsync:shelfsync@postgres:5432/shelfsync
      REDIS_URL: redis://redis:6379/0
      GOODREADS_BASE_URL: http://goodreads-mock:4010
    ports:
      - "8000:8000"
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      goodreads-mock:
        condition: service_started

volumes:
  postgres_data:
</file>

<file path="infra/docker-compose.yml">
name: shelfsync

services:
  postgres:
    image: postgres:18-alpine
    environment:
      POSTGRES_USER: shelfsync
      POSTGRES_PASSWORD: shelfsync
      POSTGRES_DB: shelfsync
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U shelfsync -d shelfsync"]
      interval: 5s
      timeout: 5s
      retries: 10

  redis:
    image: redis:8-alpine
    ports:
      - "6379:6379"
    command: ["redis-server", "--save", "", "--appendonly", "no"]
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 3s
      retries: 10

  goodreads-mock:
    image: stoplight/prism:5
    command: ["mock", "-h", "0.0.0.0", "/spec/goodreads-mock.yaml"]
    ports:
      - "4010:4010"
    volumes:
      - ../docs/openapi/goodreads-mock.yaml:/spec/goodreads-mock.yaml:ro
      - ../mock:/spec/mock:ro

volumes:
  postgres_data:
</file>

<file path="services/api/alembic/versions/06ccc00c800a_phase7_notifications.py">
"""phase7 notifications

Revision ID: 06ccc00c800a
Revises: 32191e629780
Create Date: 2025-12-25 16:59:39.258811

"""

from typing import Sequence, Union

import sqlalchemy as sa
from alembic import op

# revision identifiers, used by Alembic.
revision: str = "06ccc00c800a"
down_revision: Union[str, None] = "32191e629780"
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    # Add per-user notifications toggle (default ON).
    op.add_column(
        "user_settings",
        sa.Column(
            "notifications_enabled",
            sa.Boolean(),
            nullable=False,
            server_default=sa.true(),
        ),
    )


def downgrade() -> None:
    op.drop_column("user_settings", "notifications_enabled")
</file>

<file path="services/api/alembic/versions/156614e6d7d6_phase4_seed_libraries.py">
"""phase4 seed libraries

Revision ID: 156614e6d7d6
Revises: bedca43b8c03
Create Date: 2025-12-24 06:17:16.861999

"""

from typing import Sequence, Union

# revision identifiers, used by Alembic.
revision: str = "156614e6d7d6"
down_revision: Union[str, Sequence[str], None] = "bedca43b8c03"
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    """Upgrade schema."""
    pass


def downgrade() -> None:
    """Downgrade schema."""
    pass
</file>

<file path="services/api/alembic/versions/8399dceac96e_phase1_init_schema.py">
"""phase1 init schema

Revision ID: 8399dceac96e
Revises:
Create Date: 2025-12-22 20:48:21.235631

"""

from typing import Sequence, Union

import sqlalchemy as sa
from alembic import op

# revision identifiers, used by Alembic.
revision: str = "8399dceac96e"
down_revision: Union[str, Sequence[str], None] = None
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    """Upgrade schema."""
    # ### commands auto generated by Alembic - please adjust! ###
    op.create_table(
        "libraries",
        sa.Column("id", sa.String(length=80), nullable=False),
        sa.Column("name", sa.String(length=200), nullable=False),
        sa.PrimaryKeyConstraint("id"),
    )
    op.create_table(
        "users",
        sa.Column("id", sa.String(length=36), nullable=False),
        sa.Column("email", sa.String(length=320), nullable=False),
        sa.Column("password_hash", sa.String(length=255), nullable=False),
        sa.Column("is_active", sa.Boolean(), nullable=False),
        sa.Column("created_at", sa.DateTime(timezone=True), nullable=False),
        sa.PrimaryKeyConstraint("id"),
    )
    op.create_index(op.f("ix_users_email"), "users", ["email"], unique=True)
    op.create_table(
        "shelf_sources",
        sa.Column("id", sa.String(length=36), nullable=False),
        sa.Column("user_id", sa.String(length=36), nullable=False),
        sa.Column("source_type", sa.String(length=20), nullable=False),
        sa.Column("source_ref", sa.String(length=2000), nullable=False),
        sa.Column("created_at", sa.DateTime(timezone=True), nullable=False),
        sa.ForeignKeyConstraint(["user_id"], ["users.id"], ondelete="CASCADE"),
        sa.PrimaryKeyConstraint("id"),
    )
    op.create_index(
        op.f("ix_shelf_sources_user_id"), "shelf_sources", ["user_id"], unique=False
    )
    op.create_table(
        "user_settings",
        sa.Column("user_id", sa.String(length=36), nullable=False),
        sa.Column("library_system", sa.String(length=200), nullable=True),
        sa.Column("preferred_formats", sa.JSON(), nullable=False),
        sa.Column("updated_at", sa.DateTime(timezone=True), nullable=False),
        sa.ForeignKeyConstraint(["user_id"], ["users.id"], ondelete="CASCADE"),
        sa.PrimaryKeyConstraint("user_id"),
    )
    op.create_table(
        "shelf_items",
        sa.Column("id", sa.String(length=36), nullable=False),
        sa.Column("user_id", sa.String(length=36), nullable=False),
        sa.Column("shelf_source_id", sa.String(length=36), nullable=True),
        sa.Column("title", sa.String(length=600), nullable=False),
        sa.Column("author", sa.String(length=400), nullable=False),
        sa.Column("created_at", sa.DateTime(timezone=True), nullable=False),
        sa.ForeignKeyConstraint(
            ["shelf_source_id"], ["shelf_sources.id"], ondelete="SET NULL"
        ),
        sa.ForeignKeyConstraint(["user_id"], ["users.id"], ondelete="CASCADE"),
        sa.PrimaryKeyConstraint("id"),
    )
    op.create_index(
        op.f("ix_shelf_items_shelf_source_id"),
        "shelf_items",
        ["shelf_source_id"],
        unique=False,
    )
    op.create_index(
        op.f("ix_shelf_items_user_id"), "shelf_items", ["user_id"], unique=False
    )
    op.create_index(
        "ix_shelf_items_user_title_author",
        "shelf_items",
        ["user_id", "title", "author"],
        unique=False,
    )
    # ### end Alembic commands ###


def downgrade() -> None:
    """Downgrade schema."""
    # ### commands auto generated by Alembic - please adjust! ###
    op.drop_index("ix_shelf_items_user_title_author", table_name="shelf_items")
    op.drop_index(op.f("ix_shelf_items_user_id"), table_name="shelf_items")
    op.drop_index(op.f("ix_shelf_items_shelf_source_id"), table_name="shelf_items")
    op.drop_table("shelf_items")
    op.drop_table("user_settings")
    op.drop_index(op.f("ix_shelf_sources_user_id"), table_name="shelf_sources")
    op.drop_table("shelf_sources")
    op.drop_index(op.f("ix_users_email"), table_name="users")
    op.drop_table("users")
    op.drop_table("libraries")
    # ### end Alembic commands ###
</file>

<file path="services/api/alembic/versions/bedca43b8c03_phase3_catalog_matching.py">
"""phase3 catalog matching

Revision ID: bedca43b8c03
Revises: b2c2190162bb
Create Date: 2025-12-23 19:50:47.846892

"""

from typing import Sequence, Union

# revision identifiers, used by Alembic.
revision: str = "bedca43b8c03"
down_revision: Union[str, Sequence[str], None] = "b2c2190162bb"
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    """Upgrade schema."""
    # ### commands auto generated by Alembic - please adjust! ###
    pass
    # ### end Alembic commands ###


def downgrade() -> None:
    """Downgrade schema."""
    # ### commands auto generated by Alembic - please adjust! ###
    pass
    # ### end Alembic commands ###
</file>

<file path="services/api/app/api/routes/dashboard.py">
from __future__ import annotations

from dataclasses import asdict
from datetime import datetime, timezone
from typing import Literal

from app.api.deps import get_current_user
from app.api.rate_limit import rate_limiter
from app.db.session import get_db
from app.models.availability_snapshot import AvailabilitySnapshot
from app.models.catalog_item import CatalogItem
from app.models.catalog_match import CatalogMatch
from app.models.shelf_item import ShelfItem
from app.models.shelf_source import ShelfSource
from app.models.user_settings import UserSettings
from app.schemas.dashboard import (
    AvailabilityOut,
    DashboardOut,
    DashboardRowOut,
    LastSyncOut,
    MatchMiniOut,
    PageOut,
    ReadNextOut,
)
from app.services.read_next_scoring import compute_read_next
from fastapi import APIRouter, Depends, Query
from sqlalchemy import select
from sqlalchemy.orm import Session

router = APIRouter(prefix="/v1", tags=["dashboard"])


@router.get(
    "/dashboard",
    response_model=DashboardOut,
    dependencies=[Depends(rate_limiter("dashboard", limit=120, window_seconds=60))],
)
def get_dashboard(
    *,
    db: Session = Depends(get_db),
    user=Depends(get_current_user),
    limit: int = Query(default=50, ge=1, le=200),
    offset: int = Query(default=0, ge=0),
    sort: Literal["read_next", "title", "updated"] = Query(default="read_next"),
) -> DashboardOut:
    # Settings
    user_settings = db.execute(
        select(UserSettings).where(UserSettings.user_id == user.id)
    ).scalar_one_or_none()
    if user_settings is None:
        user_settings = UserSettings(user_id=user.id)
        db.add(user_settings)
        db.commit()
        db.refresh(user_settings)

    preferred_formats = list(user_settings.preferred_formats or [])

    # Sources (used for last_sync and to keep dashboard scoped to user sources)
    sources = (
        db.execute(select(ShelfSource).where(ShelfSource.user_id == user.id))
        .scalars()
        .all()
    )
    source_ids = [s.id for s in sources]

    # Last sync (best effort)
    min_dt = datetime.min.replace(tzinfo=timezone.utc)
    latest_src = (
        max(sources, key=lambda s: s.last_synced_at or min_dt) if sources else None
    )
    last_sync = LastSyncOut(
        source_type=latest_src.source_type if latest_src else None,
        source_id=latest_src.id if latest_src else None,
        last_synced_at=latest_src.last_synced_at if latest_src else None,
        last_sync_status=latest_src.last_sync_status if latest_src else None,
        last_sync_error=latest_src.last_sync_error if latest_src else None,
    )

    # Load shelf items
    items_stmt = select(ShelfItem).where(ShelfItem.user_id == user.id)
    if source_ids:
        items_stmt = items_stmt.where(ShelfItem.shelf_source_id.in_(source_ids))

    all_items = db.execute(items_stmt).scalars().all()

    if not all_items:
        return DashboardOut(
            settings={
                "library_system": user_settings.library_system,
                "preferred_formats": preferred_formats,
                "updated_at": user_settings.updated_at,
            },
            last_sync=last_sync,
            page=PageOut(limit=limit, offset=offset, total=0),
            items=[],
        )

    # Load catalog matches (at most one per shelf item by constraint)
    shelf_item_ids = [i.id for i in all_items]
    matches = (
        db.execute(
            select(CatalogMatch).where(
                CatalogMatch.user_id == user.id,
                CatalogMatch.shelf_item_id.in_(shelf_item_ids),
            )
        )
        .scalars()
        .all()
    )
    match_by_shelf_id: dict[str, CatalogMatch] = {m.shelf_item_id: m for m in matches}

    # Load catalog items referenced by matches
    catalog_item_ids = {m.catalog_item_id for m in matches}
    catalog_items = (
        db.execute(
            select(CatalogItem).where(CatalogItem.id.in_(list(catalog_item_ids)))
        )
        .scalars()
        .all()
        if catalog_item_ids
        else []
    )
    catalog_by_id: dict[str, CatalogItem] = {c.id: c for c in catalog_items}

    # Load availability snapshots for those catalog items (grouped by catalog_item_id)
    snapshots = (
        db.execute(
            select(AvailabilitySnapshot).where(
                AvailabilitySnapshot.user_id == user.id,
                AvailabilitySnapshot.catalog_item_id.in_(list(catalog_item_ids)),
            )
        )
        .scalars()
        .all()
        if catalog_item_ids
        else []
    )
    snaps_by_catalog_id: dict[str, list[AvailabilitySnapshot]] = {}
    for s in snapshots:
        snaps_by_catalog_id.setdefault(s.catalog_item_id, []).append(s)

    rows: list[DashboardRowOut] = []
    for si in all_items:
        m = match_by_shelf_id.get(si.id)

        match_out: MatchMiniOut | None = None
        availability: list[AvailabilityOut] = []

        if m is not None:
            c = catalog_by_id.get(m.catalog_item_id)
            if c is not None:
                match_out = MatchMiniOut(
                    catalog_item_id=c.id,
                    provider=m.provider,
                    provider_item_id=c.provider_item_id,
                    method=m.method,
                    confidence=m.confidence,
                )

                for a in snaps_by_catalog_id.get(c.id, []):
                    availability.append(
                        AvailabilityOut(
                            format=a.format,
                            status=a.status,
                            copies_available=a.copies_available,
                            copies_total=a.copies_total,
                            holds=a.holds,
                            deep_link=a.deep_link,
                            last_checked_at=a.last_checked_at,
                        )
                    )

        rn = compute_read_next(availability, preferred_formats)

        rows.append(
            DashboardRowOut(
                shelf_item_id=si.id,
                title=si.title,
                author=si.author,
                shelf=si.shelf,
                needs_fuzzy_match=si.needs_fuzzy_match,
                match=match_out,
                availability=availability,
                read_next=ReadNextOut(**asdict(rn)),
            )
        )

    # Sorting
    if sort == "read_next":
        rows.sort(key=lambda r: r.read_next.score, reverse=True)
    elif sort == "title":
        rows.sort(key=lambda r: (r.title or "").lower())
    elif sort == "updated":
        id_to_updated = {si.id: si.updated_at for si in all_items}
        rows.sort(
            key=lambda r: id_to_updated.get(r.shelf_item_id)
            or datetime.min.replace(tzinfo=timezone.utc),
            reverse=True,
        )

    total = len(rows)
    page_items = rows[offset : offset + limit]

    return DashboardOut(
        settings={
            "library_system": user_settings.library_system,
            "preferred_formats": preferred_formats,
            "updated_at": user_settings.updated_at,
        },
        last_sync=last_sync,
        page=PageOut(limit=limit, offset=offset, total=total),
        items=page_items,
    )
</file>

<file path="services/api/app/api/routes/health.py">
from fastapi import APIRouter

router = APIRouter(tags=["health"])


@router.get("/health")
def health_check():
    return {"ok": True}
</file>

<file path="services/api/app/api/routes/notifications.py">
from __future__ import annotations

import json
from datetime import datetime, timezone
from typing import AsyncGenerator

from app.api.deps import get_current_user
from app.api.rate_limit import rate_limiter
from app.core.config import settings
from app.core.redis import get_redis_async
from app.crud.notifications import (
    list_notifications,
    mark_all_read,
    mark_read,
    unread_count,
)
from app.db.session import get_db
from app.schemas.notifications import (
    NotificationListOut,
    NotificationOut,
    PageOut,
    UnreadCountOut,
)
from fastapi import APIRouter, Depends, HTTPException, Query
from fastapi.responses import StreamingResponse
from sqlalchemy.orm import Session

router = APIRouter(prefix="/v1", tags=["notifications"])


@router.get(
    "/notifications",
    response_model=NotificationListOut,
    dependencies=[Depends(rate_limiter("notifications", limit=120, window_seconds=60))],
)
def get_notifications(
    *,
    limit: int = Query(50, ge=1, le=200),
    offset: int = Query(0, ge=0),
    unread_only: bool = Query(False),
    db: Session = Depends(get_db),
    user=Depends(get_current_user),
):
    total, rows = list_notifications(
        db,
        user_id=user.id,
        unread_only=unread_only,
        limit=limit,
        offset=offset,
    )

    items = [
        NotificationOut(
            id=r.event.id,
            created_at=r.event.created_at,
            read_at=r.event.read_at,
            shelf_item_id=r.event.shelf_item_id,
            title=r.title,
            author=r.author,
            format=r.event.format,
            old_status=r.event.old_status,
            new_status=r.event.new_status,
            deep_link=r.event.deep_link,
        )
        for r in rows
    ]

    return NotificationListOut(
        page=PageOut(limit=limit, offset=offset, total=total), items=items
    )


@router.get(
    "/notifications/unread-count",
    response_model=UnreadCountOut,
    dependencies=[
        Depends(rate_limiter("notifications_unread", limit=240, window_seconds=60))
    ],
)
def get_unread_count(
    db: Session = Depends(get_db),
    user=Depends(get_current_user),
):
    return UnreadCountOut(unread=unread_count(db, user_id=user.id))


@router.post(
    "/notifications/{notification_id}/read",
    status_code=204,
    dependencies=[
        Depends(rate_limiter("notifications_mark_read", limit=240, window_seconds=60))
    ],
)
def mark_notification_read(
    notification_id: str,
    db: Session = Depends(get_db),
    user=Depends(get_current_user),
):
    ok = mark_read(db, user_id=user.id, notification_id=notification_id)
    if not ok:
        raise HTTPException(status_code=404, detail="Notification not found")
    return None


@router.post(
    "/notifications/mark-all-read",
    dependencies=[
        Depends(rate_limiter("notifications_mark_all", limit=60, window_seconds=60))
    ],
)
def mark_all_notifications_read(
    db: Session = Depends(get_db),
    user=Depends(get_current_user),
):
    n = mark_all_read(db, user_id=user.id)
    return {"updated": n}


@router.get("/notifications/events")
async def stream_notifications(
    user=Depends(get_current_user),
) -> StreamingResponse:
    redis_client = get_redis_async(settings.redis_url)
    channel = f"notify:{user.id}"

    async def event_generator() -> AsyncGenerator[str, None]:
        pubsub = redis_client.pubsub()
        await pubsub.subscribe(channel)

        # Basic keepalive to help proxies (optional)
        last_keepalive = datetime.now(timezone.utc)

        try:
            async for message in pubsub.listen():
                now = datetime.now(timezone.utc)

                # Keepalive comment every ~25s
                if (now - last_keepalive).total_seconds() > 25:
                    yield ": keepalive\n\n"
                    last_keepalive = now

                if message is None:
                    continue
                if message.get("type") != "message":
                    continue

                data_raw = message.get("data")
                if isinstance(data_raw, (bytes, bytearray)):
                    data_str = data_raw.decode("utf-8")
                else:
                    data_str = str(data_raw)

                payload = json.loads(data_str)
                payload.setdefault("ts", now.isoformat())

                yield f"data: {json.dumps(payload)}\n\n"
        finally:
            await pubsub.unsubscribe(channel)
            await pubsub.close()

    return StreamingResponse(event_generator(), media_type="text/event-stream")
</file>

<file path="services/api/app/api/routes/shelf_items.py">
from __future__ import annotations

from app.api.deps import get_current_user
from app.db.session import get_db
from app.models.shelf_item import ShelfItem
from app.schemas.shelf import ShelfItemOut
from fastapi import APIRouter, Depends
from sqlalchemy import select
from sqlalchemy.orm import Session

router = APIRouter(prefix="/shelf-items", tags=["shelf-items"])


@router.get("", response_model=list[ShelfItemOut])
def list_shelf_items(
    source_id: str | None = None,
    shelf: str | None = None,
    limit: int = 50,
    offset: int = 0,
    db: Session = Depends(get_db),
    user=Depends(get_current_user),
):
    q = select(ShelfItem).where(ShelfItem.user_id == user.id)
    if source_id:
        q = q.where(ShelfItem.shelf_source_id == source_id)
    if shelf:
        q = q.where(ShelfItem.shelf == shelf)
    q = q.order_by(ShelfItem.updated_at.desc()).limit(limit).offset(offset)

    rows = db.execute(q).scalars().all()
    return rows
</file>

<file path="services/api/app/api/routes/sync_runs.py">
from __future__ import annotations

import json
from datetime import datetime, timezone
from typing import AsyncGenerator

from app.api.deps import get_current_user
from app.core.config import settings
from app.core.redis import get_redis_async
from app.core.security import hash_password
from app.crud.sync_runs import create_sync_run, get_sync_run
from app.db.session import get_db
from app.models.user import User
from app.models.user_settings import UserSettings
from app.schemas.sync_run import StartSyncRunIn, SyncRunOut
from app.workers.queue import enqueue_availability_refresh
from fastapi import APIRouter, Depends, HTTPException, Request, Response
from fastapi.responses import StreamingResponse
from sqlalchemy import select
from sqlalchemy.orm import Session

router = APIRouter(prefix="/v1", tags=["sync-runs"])


def _get_or_create_demo_user(db: Session) -> User:
    demo_email = "demo@example.com"
    u = db.execute(select(User).where(User.email == demo_email)).scalar_one_or_none()
    if u is not None:
        return u

    u = User(email=demo_email, password_hash=hash_password("password"))
    db.add(u)
    db.flush()
    db.add(UserSettings(user_id=u.id))
    db.commit()
    db.refresh(u)
    return u


def _optional_user(request: Request, db: Session) -> User | None:
    try:
        return get_current_user(request=request, db=db)
    except HTTPException:
        return None


@router.post("/sync-runs", response_model=SyncRunOut)
def start_sync_run(
    payload: StartSyncRunIn,
    request: Request,
    response: Response,
    db: Session = Depends(get_db),
):
    # Optional auth (test hits this endpoint without cookies)
    user = _optional_user(request, db)

    if user is None:
        if not (
            settings.demo_login_enabled
            and settings.env in {"local", "development", "dev"}
        ):
            raise HTTPException(status_code=401, detail="Not authenticated")
        user = _get_or_create_demo_user(db)

    run = create_sync_run(db, user_id=user.id, kind=payload.kind)
    enqueue_availability_refresh(sync_run_id=run.id)
    return run


@router.get("/sync-runs/{run_id}", response_model=SyncRunOut)
def get_sync_run_detail(
    run_id: str,
    db: Session = Depends(get_db),
    user=Depends(get_current_user),
):
    run = get_sync_run(db, run_id=run_id)
    if run is None or run.user_id != user.id:
        raise HTTPException(status_code=404, detail="Sync run not found")
    return run


@router.get("/sync-runs/{run_id}/events")
async def stream_sync_events(
    run_id: str,
    user=Depends(get_current_user),
) -> StreamingResponse:
    redis_client = get_redis_async(settings.redis_url)

    channel = f"sync:{user.id}:{run_id}"

    async def event_generator() -> AsyncGenerator[str, None]:
        pubsub = redis_client.pubsub()
        await pubsub.subscribe(channel)

        try:
            async for message in pubsub.listen():
                if message is None:
                    continue
                if message.get("type") != "message":
                    continue

                data_raw = message.get("data")
                if data_raw is None:
                    continue

                if isinstance(data_raw, (bytes, bytearray)):
                    data_str = data_raw.decode("utf-8")
                else:
                    data_str = str(data_raw)

                payload = json.loads(data_str)
                payload.setdefault("ts", datetime.now(timezone.utc).isoformat())
                payload["run_id"] = run_id

                yield f"data: {json.dumps(payload)}\n\n"
        finally:
            await pubsub.unsubscribe(channel)
            await pubsub.close()

    return StreamingResponse(event_generator(), media_type="text/event-stream")
</file>

<file path="services/api/app/api/deps.py">
from __future__ import annotations

from typing import Optional

from app.core.config import settings
from app.core.security import decode_access_token
from app.db.session import get_db
from app.models.user import User
from fastapi import Depends, HTTPException, Request, status
from jose import JWTError  # type: ignore[import-untyped]
from sqlalchemy.orm import Session


def _extract_bearer_token(request: Request) -> Optional[str]:
    auth = request.headers.get("Authorization")
    if not auth:
        return None
    parts = auth.split(" ", 1)
    if len(parts) != 2:
        return None
    scheme, token = parts
    if scheme.lower() != "bearer":
        return None
    return token


def get_current_user(request: Request, db: Session = Depends(get_db)) -> User:
    # Allow either Authorization: Bearer <token> OR cookie-based auth
    token = _extract_bearer_token(request) or request.cookies.get(
        settings.auth_cookie_name
    )
    if not token:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED, detail="Not authenticated"
        )

    try:
        payload = decode_access_token(token)
        user_id = payload.get("sub")
        if not user_id:
            raise HTTPException(
                status_code=status.HTTP_401_UNAUTHORIZED, detail="Invalid token"
            )
    except (JWTError, HTTPException):
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED, detail="Invalid token"
        )

    user = db.get(User, user_id)
    if not user or not user.is_active:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED, detail="User not found"
        )

    return user
</file>

<file path="services/api/app/api/router.py">
from __future__ import annotations

import importlib

from fastapi import APIRouter

api_router = APIRouter()


def _include(module_path: str) -> None:
    try:
        mod = importlib.import_module(module_path)
    except Exception:
        # Optional route module; ignore if not present.
        return

    router = getattr(mod, "router", None)
    if router is not None:
        api_router.include_router(router)


# Keep this list in the order you want routes registered.
for _mod in (
    "app.api.routes.health",
    "app.api.routes.auth",
    "app.api.routes.settings",
    "app.api.routes.libraries",
    "app.api.routes.shelf_sources",
    "app.api.routes.shelf_items",
    "app.api.routes.matching",
    "app.api.routes.dashboard",
    "app.api.routes.books",
    "app.api.routes.sync_runs",
):
    _include(_mod)
</file>

<file path="services/api/app/core/redis_client.py">
from __future__ import annotations

import logging
from functools import lru_cache

import redis
from app.core.config import settings
from redis import Redis

logger = logging.getLogger(__name__)


@lru_cache
def get_redis() -> Redis | None:
    try:
        client: Redis = redis.from_url(settings.redis_url, decode_responses=True)
        client.ping()
        return client
    except Exception as exc:
        logger.warning("Redis unavailable; caching/rate limiting disabled: %s", exc)
        return None
</file>

<file path="services/api/app/crud/notifications.py">
from __future__ import annotations

from dataclasses import dataclass
from datetime import datetime, timezone
from typing import Any, cast

from app.models.notification_event import NotificationEvent
from app.models.shelf_item import ShelfItem
from sqlalchemy import func, select, update
from sqlalchemy.engine import CursorResult
from sqlalchemy.orm import Session


def utcnow() -> datetime:
    return datetime.now(timezone.utc)


@dataclass(frozen=True)
class NotificationRow:
    event: NotificationEvent
    title: str
    author: str | None


def unread_count(db: Session, *, user_id: str) -> int:
    """Count unread notifications for a user.

    Kept for compatibility with app.api.routes.notifications imports.
    """
    return int(
        db.execute(
            select(func.count())
            .select_from(NotificationEvent)
            .where(
                NotificationEvent.user_id == user_id,
                NotificationEvent.read_at.is_(None),
            )
        ).scalar_one()
    )


# Backwards/forwards compatibility if other code used a different name.
count_unread = unread_count


def list_notifications(
    db: Session,
    *,
    user_id: str,
    unread_only: bool,
    limit: int,
    offset: int,
) -> tuple[int, list[NotificationRow]]:
    base = (
        select(NotificationEvent, ShelfItem.title, ShelfItem.author)
        .join(ShelfItem, ShelfItem.id == NotificationEvent.shelf_item_id)
        .where(NotificationEvent.user_id == user_id)
    )
    if unread_only:
        base = base.where(NotificationEvent.read_at.is_(None))

    total = int(
        db.execute(select(func.count()).select_from(base.subquery())).scalar_one()
    )

    stmt = (
        base.order_by(NotificationEvent.created_at.desc()).limit(limit).offset(offset)
    )

    # .tuples() gives mypy a predictable tuple type for unpacking
    rows_raw = db.execute(stmt).tuples().all()

    rows: list[NotificationRow] = []
    for ev, title, author in rows_raw:
        rows.append(NotificationRow(event=ev, title=title, author=author))

    return total, rows


def mark_read(db: Session, *, user_id: str, notification_id: str) -> bool:
    """Mark a single notification as read (idempotent)."""
    ev = db.get(NotificationEvent, notification_id)
    if ev is None or ev.user_id != user_id:
        return False

    if ev.read_at is None:
        ev.read_at = utcnow()
        db.add(ev)
        db.commit()

    return True


def mark_all_read(db: Session, *, user_id: str) -> int:
    now = utcnow()
    stmt = (
        update(NotificationEvent)
        .where(
            NotificationEvent.user_id == user_id, NotificationEvent.read_at.is_(None)
        )
        .values(read_at=now)
    )
    res = cast(CursorResult[Any], db.execute(stmt))
    db.commit()
    return int(res.rowcount or 0)
</file>

<file path="services/api/app/crud/shelf_items.py">
from __future__ import annotations

from app.models.shelf_item import ShelfItem
from sqlalchemy import select
from sqlalchemy.orm import Session


def list_shelf_items_for_user(db: Session, *, user_id: str) -> list[ShelfItem]:
    """Return the user's shelf items ordered from newest to oldest."""
    stmt = (
        select(ShelfItem)
        .where(ShelfItem.user_id == user_id)
        .order_by(ShelfItem.updated_at.desc())
    )
    return list(db.execute(stmt).scalars().all())
</file>

<file path="services/api/app/crud/sync_runs.py">
from __future__ import annotations

from datetime import datetime, timezone
from typing import Optional

from app.models.sync_run import SyncRun
from sqlalchemy.orm import Session


def _utcnow() -> datetime:
    return datetime.now(timezone.utc)


def create_sync_run(db: Session, *, user_id: str, kind: str) -> SyncRun:
    run = SyncRun(
        user_id=user_id,
        kind=kind,
        status="queued",
        progress_current=0,
        progress_total=0,
    )
    db.add(run)
    db.commit()
    db.refresh(run)
    return run


def set_sync_run_running(db: Session, *, run: SyncRun, total: int) -> SyncRun:
    run.status = "running"
    run.started_at = _utcnow()
    run.progress_total = total
    run.progress_current = 0
    db.commit()
    db.refresh(run)
    return run


def update_progress(
    db: Session, *, run: SyncRun, current: int, total: Optional[int] = None
) -> SyncRun:
    run.progress_current = current
    if total is not None:
        run.progress_total = total
    db.commit()
    db.refresh(run)
    return run


def set_sync_run_failed(db: Session, *, run: SyncRun, message: str) -> SyncRun:
    run.status = "failed"
    run.error_message = message
    run.finished_at = _utcnow()
    db.commit()
    db.refresh(run)
    return run


def set_sync_run_succeeded(db: Session, *, run: SyncRun) -> SyncRun:
    run.status = "succeeded"
    run.finished_at = _utcnow()
    db.commit()
    db.refresh(run)
    return run


def get_sync_run(db: Session, *, run_id: str) -> Optional[SyncRun]:
    return db.query(SyncRun).filter(SyncRun.id == run_id).first()


def latest_sync_run_for_user(
    db: Session, *, user_id: str, kind: str = "availability_refresh"
) -> Optional[SyncRun]:
    return (
        db.query(SyncRun)
        .filter(SyncRun.user_id == user_id, SyncRun.kind == kind)
        .order_by(SyncRun.created_at.desc())
        .first()
    )
</file>

<file path="services/api/app/domain/normalize.py">
from __future__ import annotations

import re
from dataclasses import dataclass

_non_alnum = re.compile(r"[^a-z0-9]+")
_digits_or_x = re.compile(r"[^0-9Xx]")


def normalize_text(s: str) -> str:
    s = s.strip().lower()
    s = _non_alnum.sub(" ", s)
    s = " ".join(s.split())
    return s


def normalize_isbn(raw: str) -> str:
    """Return a best-effort normalized ISBN string.

    - Removes hyphens/spaces.
    - Keeps digits (and X for ISBN-10 check digit).
    - Does not validate checksum in Phase 2 (keep it simple and resilient).

    If you want checksum validation later, add it as a non-fatal warning and keep ingestion permissive.
    """
    cleaned = _digits_or_x.sub("", raw or "").upper()
    return cleaned


@dataclass(frozen=True)
class NormalizedIdentifiers:
    isbn13: str | None
    isbn10: str | None
    asin: str | None

    normalized_title: str
    normalized_author: str
    normalized_key: str
    needs_fuzzy_match: bool


def build_normalized(
    *,
    title: str,
    author: str,
    isbn13: str | None = None,
    isbn10: str | None = None,
    asin: str | None = None,
) -> NormalizedIdentifiers:
    n_title = normalize_text(title)
    n_author = normalize_text(author)

    i13 = normalize_isbn(isbn13) if isbn13 else None
    i10 = normalize_isbn(isbn10) if isbn10 else None

    # Prefer ISBN-13, then ISBN-10, then ASIN, else title+author.
    if i13 and len(i13) == 13 and i13.isdigit():
        key = f"isbn13:{i13}"
        return NormalizedIdentifiers(i13, i10, asin, n_title, n_author, key, False)

    if i10 and len(i10) == 10:
        key = f"isbn10:{i10}"
        return NormalizedIdentifiers(i13, i10, asin, n_title, n_author, key, False)

    if asin:
        a = asin.strip()
        key = f"asin:{a}"
        return NormalizedIdentifiers(i13, i10, a, n_title, n_author, key, False)

    # Fallback to fuzzy matching later
    key = f"title_author:{n_title}|{n_author}"
    return NormalizedIdentifiers(i13, i10, asin, n_title, n_author, key, True)
</file>

<file path="services/api/app/models/catalog_item.py">
from __future__ import annotations

from datetime import datetime, timezone
from uuid import uuid4

from app.models.base import Base
from sqlalchemy import JSON, DateTime, String, UniqueConstraint
from sqlalchemy.orm import Mapped, mapped_column


class CatalogItem(Base):
    __tablename__ = "catalog_items"

    id: Mapped[str] = mapped_column(
        String(36), primary_key=True, default=lambda: str(uuid4())
    )

    provider: Mapped[str] = mapped_column(String(40), nullable=False)
    provider_item_id: Mapped[str] = mapped_column(String(160), nullable=False)

    title: Mapped[str] = mapped_column(String(400), nullable=False)
    author: Mapped[str | None] = mapped_column(String(240), nullable=True)

    isbn10: Mapped[str | None] = mapped_column(String(10), nullable=True)
    isbn13: Mapped[str | None] = mapped_column(String(13), nullable=True)
    asin: Mapped[str | None] = mapped_column(String(20), nullable=True)

    raw: Mapped[dict] = mapped_column(JSON, nullable=False, default=dict)

    created_at: Mapped[datetime] = mapped_column(
        DateTime(timezone=True),
        default=lambda: datetime.now(timezone.utc),
        nullable=False,
    )
    updated_at: Mapped[datetime] = mapped_column(
        DateTime(timezone=True),
        default=lambda: datetime.now(timezone.utc),
        onupdate=lambda: datetime.now(timezone.utc),
        nullable=False,
    )

    __table_args__ = (
        UniqueConstraint(
            "provider", "provider_item_id", name="uq_catalog_items_provider_item"
        ),
    )
</file>

<file path="services/api/app/models/catalog_match.py">
from __future__ import annotations

from datetime import datetime, timezone
from uuid import uuid4

from app.models.base import Base
from sqlalchemy import JSON, DateTime, Float, ForeignKey, String, UniqueConstraint
from sqlalchemy.orm import Mapped, mapped_column


class CatalogMatch(Base):
    __tablename__ = "catalog_matches"

    id: Mapped[str] = mapped_column(
        String(36), primary_key=True, default=lambda: str(uuid4())
    )

    user_id: Mapped[str] = mapped_column(
        String(36),
        ForeignKey("users.id", ondelete="CASCADE"),
        nullable=False,
        index=True,
    )
    shelf_item_id: Mapped[str] = mapped_column(
        String(36),
        ForeignKey("shelf_items.id", ondelete="CASCADE"),
        nullable=False,
        index=True,
    )
    catalog_item_id: Mapped[str] = mapped_column(
        String(36),
        ForeignKey("catalog_items.id", ondelete="CASCADE"),
        nullable=False,
        index=True,
    )

    provider: Mapped[str] = mapped_column(String(40), nullable=False)
    method: Mapped[str] = mapped_column(String(40), nullable=False)  # isbn | fuzzy
    confidence: Mapped[float] = mapped_column(Float, nullable=False)

    # store explainability payload; keep it relatively small
    evidence: Mapped[dict] = mapped_column(JSON, nullable=False, default=dict)

    created_at: Mapped[datetime] = mapped_column(
        DateTime(timezone=True),
        default=lambda: datetime.now(timezone.utc),
        nullable=False,
    )
    updated_at: Mapped[datetime] = mapped_column(
        DateTime(timezone=True),
        default=lambda: datetime.now(timezone.utc),
        onupdate=lambda: datetime.now(timezone.utc),
        nullable=False,
    )

    __table_args__ = (
        UniqueConstraint(
            "user_id", "shelf_item_id", name="uq_catalog_match_user_shelf_item"
        ),
    )
</file>

<file path="services/api/app/models/notification_event.py">
from __future__ import annotations

from datetime import datetime, timezone
from uuid import uuid4

from app.models.base import Base
from sqlalchemy import DateTime, ForeignKey, String
from sqlalchemy.orm import Mapped, mapped_column


def utcnow() -> datetime:
    return datetime.now(timezone.utc)


class NotificationEvent(Base):
    __tablename__ = "notification_events"

    id: Mapped[str] = mapped_column(
        String(36), primary_key=True, default=lambda: str(uuid4())
    )

    user_id: Mapped[str] = mapped_column(
        String(36),
        ForeignKey("users.id", ondelete="CASCADE"),
        nullable=False,
        index=True,
    )

    shelf_item_id: Mapped[str] = mapped_column(
        String(36),
        ForeignKey("shelf_items.id", ondelete="CASCADE"),
        nullable=False,
        index=True,
    )

    format: Mapped[str] = mapped_column(String(20), nullable=False)
    old_status: Mapped[str] = mapped_column(String(20), nullable=False)
    new_status: Mapped[str] = mapped_column(String(20), nullable=False)

    deep_link: Mapped[str | None] = mapped_column(String(500), nullable=True)

    created_at: Mapped[datetime] = mapped_column(
        DateTime(timezone=True), default=utcnow, nullable=False, index=True
    )
    read_at: Mapped[datetime | None] = mapped_column(
        DateTime(timezone=True), nullable=True
    )
</file>

<file path="services/api/app/models/sync_run.py">
from __future__ import annotations

from datetime import datetime, timezone
from uuid import uuid4

from app.models.base import Base
from sqlalchemy import DateTime, ForeignKey, Integer, String, Text
from sqlalchemy.orm import Mapped, mapped_column, relationship


def utcnow() -> datetime:
    return datetime.now(timezone.utc)


class SyncRun(Base):
    __tablename__ = "sync_runs"

    id: Mapped[str] = mapped_column(
        String(36), primary_key=True, default=lambda: str(uuid4())
    )

    user_id: Mapped[str] = mapped_column(
        String(36),
        ForeignKey("users.id", ondelete="CASCADE"),
        index=True,
        nullable=False,
    )

    # e.g. "availability_refresh"
    kind: Mapped[str] = mapped_column(String(50), index=True, nullable=False)

    # queued | running | succeeded | failed
    status: Mapped[str] = mapped_column(String(20), index=True, nullable=False)

    progress_current: Mapped[int] = mapped_column(Integer, nullable=False, default=0)
    progress_total: Mapped[int] = mapped_column(Integer, nullable=False, default=0)

    error_message: Mapped[str | None] = mapped_column(Text, nullable=True)

    started_at: Mapped[datetime | None] = mapped_column(
        DateTime(timezone=True), nullable=True
    )
    finished_at: Mapped[datetime | None] = mapped_column(
        DateTime(timezone=True), nullable=True
    )

    created_at: Mapped[datetime] = mapped_column(
        DateTime(timezone=True), default=utcnow, nullable=False
    )
    updated_at: Mapped[datetime] = mapped_column(
        DateTime(timezone=True), default=utcnow, onupdate=utcnow, nullable=False
    )

    user = relationship("User", back_populates="sync_runs")
</file>

<file path="services/api/app/providers/factory.py">
from __future__ import annotations

from typing import Iterable

from app.models.catalog_item import CatalogItem
from app.models.catalog_match import CatalogMatch
from app.models.shelf_item import ShelfItem
from app.providers.types import AvailabilityResult
from app.services.catalog.factory import get_provider as get_catalog_provider
from app.workers.async_utils import run_async
from sqlalchemy import select
from sqlalchemy.orm import Session


class AvailabilityProvider:
    """Adapter that fetches provider availability for matched catalog items."""

    def __init__(self, db: Session, user_id: str):
        self._db = db
        self._user_id = user_id
        self._catalog_provider = get_catalog_provider()

    @property
    def name(self) -> str:
        return self._catalog_provider.name

    def availability_bulk(self, items: Iterable[ShelfItem]) -> list[AvailabilityResult]:
        shelf_ids = [s.id for s in items]
        if not shelf_ids:
            return []

        stmt = (
            select(CatalogMatch, CatalogItem)
            .where(CatalogMatch.user_id == self._user_id)
            .where(CatalogMatch.shelf_item_id.in_(shelf_ids))
            .join(CatalogItem, CatalogItem.id == CatalogMatch.catalog_item_id)
            .where(CatalogItem.provider == self._catalog_provider.name)
        )
        rows = self._db.execute(stmt).all()
        if not rows:
            return []

        provider_to_catalog: dict[str, str] = {}
        provider_item_ids: list[str] = []
        for _, catalog_item in rows:
            pid = catalog_item.provider_item_id
            if pid in provider_to_catalog:
                continue
            provider_to_catalog[pid] = catalog_item.id
            provider_item_ids.append(pid)

        # Ask the catalog provider for availability on its own IDs.
        availability_list = run_async(
            self._catalog_provider.availability_bulk(
                provider_item_ids=provider_item_ids
            )
        )

        out: list[AvailabilityResult] = []
        for availability in availability_list:
            catalog_id = provider_to_catalog.get(availability.provider_item_id)
            if not catalog_id:
                continue
            out.append(
                AvailabilityResult(
                    catalog_item_id=catalog_id, availability=availability
                )
            )

        return out


def get_provider(db: Session, user_id: str) -> AvailabilityProvider:
    """
    Return a provider configured for the given user.

    The provider looks up existing catalog matches and delegates availability
    lookups to the configured catalog provider.
    """
    return AvailabilityProvider(db=db, user_id=user_id)
</file>

<file path="services/api/app/providers/fixture_provider.py">
from __future__ import annotations

import os
from typing import Sequence

from app.models.shelf_item import ShelfItem
from app.providers.types import AvailabilityResult
from app.services.catalog.types import AvailabilityStatus, Format, ProviderAvailability


class FixtureProvider:
    """A tiny availability provider used for demos/tests.

    This is intentionally simple: it returns a deterministic "available" result per item.
    The SYNC_INJECT_FAILURE_ONCE env var can be used to force a single failure.
    """

    name = "fixture"

    def availability_bulk(self, items: Sequence[ShelfItem]) -> list[AvailabilityResult]:
        if os.getenv("SYNC_INJECT_FAILURE_ONCE") == "true":
            os.environ["SYNC_INJECT_FAILURE_ONCE"] = "false"
            raise RuntimeError("Injected provider failure (demo)")

        out: list[AvailabilityResult] = []
        for it in items:
            out.append(
                AvailabilityResult(
                    catalog_item_id=it.id,
                    availability=ProviderAvailability(
                        provider="fixture",
                        provider_item_id=it.id,
                        format=Format.ebook,
                        status=AvailabilityStatus.available,
                        copies_available=1,
                        copies_total=1,
                        holds=0,
                    ),
                )
            )
        return out
</file>

<file path="services/api/app/schemas/auth.py">
from __future__ import annotations

from pydantic import BaseModel, EmailStr, Field, field_validator


class SignUpIn(BaseModel):
    email: EmailStr
    password: str = Field(min_length=8, max_length=72)

    @field_validator("password")
    @classmethod
    def password_must_fit_bcrypt_limit(cls, v: str) -> str:
        if len(v.encode("utf-8")) > 72:
            raise ValueError("Password must be 72 bytes or fewer when UTF-8 encoded.")
        return v


class LoginIn(BaseModel):
    email: EmailStr
    password: str


class UserOut(BaseModel):
    id: str
    email: EmailStr

    class Config:
        from_attributes = True
</file>

<file path="services/api/app/schemas/dashboard.py">
from __future__ import annotations

from datetime import datetime

from pydantic import BaseModel


class MatchMiniOut(BaseModel):
    catalog_item_id: str
    provider: str
    provider_item_id: str
    method: str
    confidence: float


class AvailabilityOut(BaseModel):
    format: str
    status: str
    copies_available: int | None
    copies_total: int | None
    holds: int | None
    deep_link: str | None
    last_checked_at: datetime


class ReadNextOut(BaseModel):
    score: float
    tier: str
    best_format: str | None
    hold_ratio: float | None
    reasons: list[str]


class DashboardRowOut(BaseModel):
    shelf_item_id: str
    title: str
    author: str | None
    shelf: str | None
    needs_fuzzy_match: bool

    match: MatchMiniOut | None
    availability: list[AvailabilityOut]
    read_next: ReadNextOut


class LastSyncOut(BaseModel):
    source_type: str | None
    source_id: str | None
    last_synced_at: datetime | None
    last_sync_status: str | None
    last_sync_error: str | None


class PageOut(BaseModel):
    limit: int
    offset: int
    total: int


class DashboardOut(BaseModel):
    settings: dict
    last_sync: LastSyncOut
    page: PageOut
    items: list[DashboardRowOut]
</file>

<file path="services/api/app/schemas/notifications.py">
from __future__ import annotations

from datetime import datetime

from pydantic import BaseModel


class NotificationOut(BaseModel):
    id: str
    created_at: datetime
    read_at: datetime | None

    shelf_item_id: str
    title: str
    author: str | None

    format: str
    old_status: str
    new_status: str
    deep_link: str | None


class PageOut(BaseModel):
    limit: int
    offset: int
    total: int


class NotificationListOut(BaseModel):
    page: PageOut
    items: list[NotificationOut]


class UnreadCountOut(BaseModel):
    unread: int
</file>

<file path="services/api/app/schemas/sync_run.py">
from __future__ import annotations

from datetime import datetime

from pydantic import BaseModel


class StartSyncRunIn(BaseModel):
    kind: str


class SyncRunOut(BaseModel):
    id: str
    kind: str
    status: str
    progress_current: int
    progress_total: int
    error_message: str | None = None
    started_at: datetime | None = None
    finished_at: datetime | None = None
    created_at: datetime
    updated_at: datetime

    class Config:
        from_attributes = True


class SyncRunEvent(BaseModel):
    run_id: str
    type: str
    payload: dict
    ts: datetime
</file>

<file path="services/api/app/services/catalog/overdrive_provider.py">
from __future__ import annotations

from app.services.catalog.types import ProviderAvailability, ProviderBook


class OverDriveProvider:
    """Phase 3: interface placeholder.

    Phase 5+: implement real OverDrive/Libby calls (and/or scraping) behind env creds.
    """

    name = "overdrive"

    async def search(
        self,
        *,
        title: str | None,
        author: str | None,
        isbn10: str | None,
        isbn13: str | None,
        limit: int = 10,
    ) -> list[ProviderBook]:
        raise NotImplementedError("OverDriveProvider is implemented in Phase 5+")

    async def availability_bulk(
        self, *, provider_item_ids: list[str]
    ) -> list[ProviderAvailability]:
        raise NotImplementedError("OverDriveProvider is implemented in Phase 5+")
</file>

<file path="services/api/app/services/normalization.py">
import re

_ws = re.compile(r"\s+")
_non_alnum = re.compile(r"[^a-zA-Z0-9]+")


def normalize_text(s: str) -> str:
    # Remove leading and trailing whitespace
    s = (s or "").strip().lower()
    s = _non_alnum.sub(" ", s)
    s = _ws.sub(" ", s).strip()
    return s


def normalize_isbn(s: str | None) -> str | None:
    if not s:
        return None
    digits = re.sub(r"[^0-9xX]", "", s)
    return digits.upper() if digits else None
</file>

<file path="services/api/app/services/shelf_import.py">
from __future__ import annotations

from dataclasses import dataclass, field

from app.models.shelf_item import ShelfItem
from app.models.shelf_source import ShelfSource
from app.services.normalization import normalize_text
from sqlalchemy import select
from sqlalchemy.orm import Session


@dataclass
class ImportErrorItem:
    key: str
    error: str


@dataclass
class ImportSummary:
    created: int = 0
    updated: int = 0
    skipped: int = 0
    errors: list[ImportErrorItem] = field(default_factory=list)


def upsert_shelf_items(
    db: Session, *, user_id: str, source: ShelfSource, items: list[dict]
) -> ImportSummary:
    summary = ImportSummary()

    # Preload existing items by external_id (best effort)
    ext_ids = [it.get("external_id") for it in items if it.get("external_id")]
    existing_by_ext: dict[str, ShelfItem] = {}

    if ext_ids:
        rows = (
            db.execute(
                select(ShelfItem)
                .where(ShelfItem.shelf_source_id == source.id)
                .where(ShelfItem.external_id.in_(ext_ids))
            )
            .scalars()
            .all()
        )
        existing_by_ext = {r.external_id: r for r in rows if r.external_id}

    for it in items:
        try:
            title = (it.get("title") or "").strip()
            author = (it.get("author") or "").strip()
            if not title or not author:
                summary.skipped += 1
                continue

            isbn10 = it.get("isbn10")
            isbn13 = it.get("isbn13")
            asin = it.get("asin")
            ext = it.get("external_id")

            norm_title = normalize_text(title)
            norm_author = normalize_text(author)
            needs_fuzzy = not (isbn10 or isbn13 or asin)

            row = existing_by_ext.get(ext) if ext else None
            if row:
                row.title = title
                row.author = author
                row.isbn10 = isbn10
                row.isbn13 = isbn13
                row.asin = asin
                row.normalized_title = norm_title
                row.normalized_author = norm_author
                row.shelf = it.get("shelf")
                row.needs_fuzzy_match = needs_fuzzy
                summary.updated += 1
            else:
                db.add(
                    ShelfItem(
                        user_id=user_id,
                        shelf_source_id=source.id,
                        external_id=ext,
                        title=title,
                        author=author,
                        isbn10=isbn10,
                        isbn13=isbn13,
                        asin=asin,
                        normalized_title=norm_title,
                        normalized_author=norm_author,
                        shelf=it.get("shelf"),
                        needs_fuzzy_match=needs_fuzzy,
                    )
                )
                summary.created += 1

        except Exception as e:
            summary.errors.append(
                ImportErrorItem(key=str(it.get("external_id") or title), error=str(e))
            )

    db.commit()
    return summary
</file>

<file path="services/api/app/workers/async_utils.py">
from __future__ import annotations

import asyncio
from typing import Awaitable, TypeVar

T = TypeVar("T")


def run_async(coro: Awaitable[T]) -> T:
    """Run an async coroutine from a sync context.

    Uses a fresh event loop to avoid "asyncio.run() cannot be called" edge cases.
    """
    loop = asyncio.new_event_loop()
    try:
        asyncio.set_event_loop(loop)
        return loop.run_until_complete(coro)
    finally:
        try:
            loop.close()
        finally:
            asyncio.set_event_loop(None)
</file>

<file path="services/api/tests/test_goodreads_csv_parser.py">
import pytest
from app.services.goodreads_csv import CsvImportError, parse_goodreads_csv


def test_parse_goodreads_csv_minimal():
    csv_bytes = (
        "Title,Author,Book Id,ISBN,ISBN13,Exclusive Shelf\n"
        "Dune,Frank Herbert,42,0441013597,9780441013593,to-read\n"
    ).encode("utf-8")

    rows, errors = parse_goodreads_csv(csv_bytes)
    assert errors == []
    assert len(rows) == 1
    assert rows[0]["title"] == "Dune"
    assert rows[0]["author"] == "Frank Herbert"
    assert rows[0]["external_id"] == "42"


def test_parse_goodreads_csv_missing_columns():
    bad = "A,B\n1,2\n".encode("utf-8")
    with pytest.raises(CsvImportError):
        parse_goodreads_csv(bad)
</file>

<file path="services/api/tests/test_goodreads_rss_parser.py">
import pytest
from app.services.goodreads_rss import parse_goodreads_rss


def test_parse_goodreads_rss_minimal():
    xml = """<?xml version="1.0"?>
<rss><channel>
  <item>
    <guid>123</guid>
    <book_title>The Hobbit</book_title>
    <author_name>J.R.R. Tolkien</author_name>
    <isbn13>9780547928227</isbn13>
  </item>
</channel></rss>
"""

    items = parse_goodreads_rss(xml, default_shelf="to-read")
    assert len(items) == 1
    assert items[0].external_id == "123"
    assert items[0].title == "The Hobbit"
    assert items[0].author == "J.R.R. Tolkien"
    assert items[0].isbn13 == "9780547928227"
    assert items[0].shelf == "to-read"


def test_parse_bad_xml_raises():
    with pytest.raises(ValueError):
        parse_goodreads_rss("<rss>")
</file>

<file path="services/api/tests/test_matching.py">
import pytest
from app.services.catalog.fixture_provider import FixtureProvider
from app.services.matching.matcher import match_shelf_item


class DummyShelfItem:
    def __init__(self, title, author=None, isbn10=None, isbn13=None):
        self.title = title
        self.author = author
        self.isbn10 = isbn10
        self.isbn13 = isbn13


@pytest.mark.asyncio
async def test_isbn_match(tmp_path):
    fixture = tmp_path / "f.json"
    fixture.write_text(
        '{"provider":"fixture","items":[{"provider_item_id":"x","title":"T","author":"A","isbn13":"9780593135204","formats":{}}]}',
        encoding="utf-8",
    )
    provider = FixtureProvider(str(fixture))
    item = DummyShelfItem(
        title="Project Hail Mary", author="Andy Weir", isbn13="9780593135204"
    )

    res = await match_shelf_item(provider, item)  # type: ignore[arg-type]
    assert res is not None
    assert res.method == "isbn"
    assert res.confidence == 1.0


@pytest.mark.asyncio
async def test_fuzzy_match_threshold(tmp_path):
    fixture = tmp_path / "f.json"
    fixture.write_text(
        '{"provider":"fixture","items":[{"provider_item_id":"x","title":"The Hobbit","author":"J.R.R. Tolkien","formats":{}}]}',
        encoding="utf-8",
    )
    provider = FixtureProvider(str(fixture))

    good = DummyShelfItem(title="Hobbit", author="Tolkien")
    res = await match_shelf_item(provider, good)  # type: ignore[arg-type]
    assert res is not None
    assert res.method == "fuzzy"

    bad = DummyShelfItem(title="Completely Different Book", author="Nobody")
    res2 = await match_shelf_item(provider, bad)  # type: ignore[arg-type]
    assert res2 is None
</file>

<file path="services/api/tests/test_notifications.py">
from __future__ import annotations

from datetime import datetime, timezone

from app.crud.availability import upsert_snapshots
from app.models import (
    AvailabilitySnapshot,
    CatalogItem,
    CatalogMatch,
    ShelfItem,
    User,
    UserSettings,
)
from app.models.notification_event import NotificationEvent
from app.providers.types import AvailabilityResult
from app.services.catalog.types import AvailabilityStatus, Format, ProviderAvailability
from sqlalchemy import select


def utcnow() -> datetime:
    return datetime.now(timezone.utc)


def _seed_user_item_match_and_hold_snapshot(
    db_session,
    *,
    email: str,
    notifications_enabled: bool,
) -> tuple[User, ShelfItem, CatalogItem]:
    user = User(email=email, password_hash="x")
    db_session.add(user)
    db_session.flush()

    settings = UserSettings(
        user_id=user.id, notifications_enabled=notifications_enabled
    )
    db_session.add(settings)

    shelf_item = ShelfItem(
        user_id=user.id,
        title="Example Title",
        author="Example Author",
        normalized_title="example title",
        normalized_author="example author",
    )
    db_session.add(shelf_item)
    db_session.flush()

    catalog_item = CatalogItem(
        id="c1",
        provider="fixture",
        provider_item_id="p1",
        title="Example Title",
        author="Example Author",
        raw={},
    )
    db_session.add(catalog_item)
    db_session.flush()

    match = CatalogMatch(
        user_id=user.id,
        shelf_item_id=shelf_item.id,
        catalog_item_id=catalog_item.id,
        provider="fixture",
        method="fixture",
        confidence=1.0,
        evidence={},
    )
    db_session.add(match)

    hold_snapshot = AvailabilitySnapshot(
        user_id=user.id,
        catalog_item_id=catalog_item.id,
        format=Format.ebook.value,
        status=AvailabilityStatus.hold.value,
        copies_available=None,
        copies_total=None,
        holds=1,
        deep_link=None,
        last_checked_at=utcnow(),
    )
    db_session.add(hold_snapshot)

    db_session.commit()
    return user, shelf_item, catalog_item


def test_notification_created_on_hold_to_available(db_session):
    user, shelf_item, catalog_item = _seed_user_item_match_and_hold_snapshot(
        db_session,
        email="a@example.com",
        notifications_enabled=True,
    )

    availability = ProviderAvailability(
        provider="fixture",
        provider_item_id="p1",
        format=Format.ebook,
        status=AvailabilityStatus.available,
        copies_available=1,
        copies_total=1,
        holds=0,
        deep_link=None,
    )

    created = upsert_snapshots(
        db_session,
        user_id=user.id,
        results=[
            AvailabilityResult(
                catalog_item_id=catalog_item.id, availability=availability
            )
        ],
    )
    db_session.commit()

    assert len(created) == 1
    assert created[0].shelf_item_id == shelf_item.id
    assert created[0].format == Format.ebook.value

    events = db_session.execute(select(NotificationEvent)).scalars().all()
    assert len(events) == 1
    ev = events[0]
    assert ev.user_id == user.id
    assert ev.shelf_item_id == shelf_item.id
    assert ev.format == Format.ebook.value
    assert ev.old_status == AvailabilityStatus.hold.value
    assert ev.new_status == AvailabilityStatus.available.value


def test_notifications_respect_user_setting(db_session):
    user, shelf_item, catalog_item = _seed_user_item_match_and_hold_snapshot(
        db_session,
        email="b@example.com",
        notifications_enabled=False,
    )

    availability = ProviderAvailability(
        provider="fixture",
        provider_item_id="p1",
        format=Format.ebook,
        status=AvailabilityStatus.available,
        copies_available=1,
        copies_total=1,
        holds=0,
        deep_link=None,
    )

    created = upsert_snapshots(
        db_session,
        user_id=user.id,
        results=[
            AvailabilityResult(
                catalog_item_id=catalog_item.id, availability=availability
            )
        ],
    )
    db_session.commit()

    assert created == []
    events = db_session.execute(select(NotificationEvent)).scalars().all()
    assert events == []
</file>

<file path="services/api/tests/test_shelf_import_idempotent.py">
from app.models.base import Base
from app.models.shelf_source import ShelfSource
from app.services.shelf_import import upsert_shelf_items
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker


def test_upsert_idempotent():
    eng = create_engine("sqlite+pysqlite:///:memory:")
    Base.metadata.create_all(bind=eng)
    Session = sessionmaker(bind=eng)

    db = Session()
    source = ShelfSource(
        user_id="u1",
        source_type="rss",
        provider="goodreads",
        source_ref="x",
        meta={},
        is_active=True,
    )
    db.add(source)
    db.commit()
    db.refresh(source)

    items = [
        {
            "external_id": "1",
            "title": "A",
            "author": "B",
            "isbn10": None,
            "isbn13": None,
            "asin": None,
            "shelf": "to-read",
        }
    ]

    s1 = upsert_shelf_items(db, user_id="u1", source=source, items=items)
    assert s1.created == 1

    s2 = upsert_shelf_items(db, user_id="u1", source=source, items=items)
    assert s2.created == 0
    assert s2.updated == 1
</file>

<file path="services/api/tests/test_sync_runs.py">
def test_start_sync_run_creates_run_and_enqueues_job(client, monkeypatch):
    # Arrange: sign in user (reuse existing helpers)
    # monkeypatch enqueue_availability_refresh to avoid hitting redis

    called = {}

    def fake_enqueue_availability_refresh(*, sync_run_id):
        called["id"] = str(sync_run_id)

    monkeypatch.setattr(
        "app.api.routes.sync_runs.enqueue_availability_refresh",
        fake_enqueue_availability_refresh,
    )

    # Act
    resp = client.post("/v1/sync-runs", json={"kind": "availability_refresh"})

    # Assert
    assert resp.status_code == 200
    body = resp.json()
    assert body["status"] == "queued"
    assert called["id"] == body["id"]
</file>

<file path=".gitignore">
# Python
__pycache__/
*.py[cod]
.venv/
.pytest_cache/
.mypy_cache/

# Node
node_modules/
.next/
out/

# Env
.env
.env.*
!.env.example
!.env.*.example

# OS
.DS_Store
</file>

<file path=".github/workflows/ci.yml">
name: CI

on:
  pull_request:
  push:
    branches: [master]

permissions:
  contents: read

concurrency:
  group: ci-${{ github.ref }}
  cancel-in-progress: true

jobs:
  api:
    runs-on: ubuntu-latest
    defaults:
      run:
        working-directory: services/api
    steps:
      - uses: actions/checkout@v5
      - uses: actions/setup-python@v6
        with:
          python-version: "3.14"
          cache: "pip"
          cache-dependency-path: services/api/requirements-dev.txt
      - run: python -m pip install --upgrade pip
      - run: pip install -r requirements-dev.txt
      - run: black --check .
      - run: isort --check-only .
      - run: mypy app || true
      - run: pytest

  web:
    runs-on: ubuntu-latest
    env:
      NEXT_PUBLIC_API_BASE_URL: http://localhost:8000
    defaults:
      run:
        working-directory: apps/web
    steps:
      - uses: actions/checkout@v5
      - uses: actions/setup-node@v6
        with:
          node-version: "24"
          cache: "npm"
          cache-dependency-path: apps/web/package-lock.json
      - run: npm ci
      - run: npm run lint
      - run: npm run build
</file>

<file path="apps/web/src/app/books/[id]/page.tsx">
"use client";

import Link from "next/link";
import { useEffect, useMemo, useState } from "react";

import { AuthGuard } from "@/components/AuthGuard";
import { apiFetch } from "@/lib/api";
import { compareReadNext, readNextTooltip, type ReadNext } from "@/lib/readNext";

type Availability = {
  format: string;
  status: "available" | "hold" | "not_owned";
  copies_available: number | null;
  copies_total: number | null;
  holds: number | null;
  deep_link: string | null;
  last_checked_at: string;
};

type CatalogItem = {
  id: string;
  provider: string;
  provider_item_id: string;
  title: string;
  author: string | null;
  isbn10: string | null;
  isbn13: string | null;
  asin: string | null;
};

type Match = {
  method: string;
  confidence: number;
  evidence: Record<string, unknown>;
  catalog_item: CatalogItem;
} | null;

type BookDetail = {
  shelf_item: {
    id: string;
    title: string;
    author: string;
    isbn10: string | null;
    isbn13: string | null;
    asin: string | null;
    shelf: string | null;
    needs_fuzzy_match: boolean;
    created_at: string;
  };
  source: {
    source_type: string | null;
    source_ref: string | null;
    last_synced_at: string | null;
    last_sync_status: string | null;
    last_sync_error: string | null;
  };
  match: Match;
  availability: Availability[];
  settings: {
    library_system: string | null;
    preferred_formats: string[];
    updated_at: string;
  };
  read_next: ReadNext;
};

type DashboardRow = {
  shelf_item_id: string;
  title: string;
  author: string | null;
  read_next: ReadNext;
};

type DashboardResponse = {
  items: DashboardRow[];
  page: { total: number; limit: number; offset: number };
};

export default function BookDetailPage({ params }: { params: { id: string } }) {
  return (
    <AuthGuard>
      <BookDetailInner id={params.id} />
    </AuthGuard>
  );
}

function BookDetailInner({ id }: { id: string }) {
  const [data, setData] = useState<BookDetail | null>(null);
  const [rank, setRank] = useState<number | null>(null);
  const [rankLoading, setRankLoading] = useState(false);
  const [error, setError] = useState<string | null>(null);

  useEffect(() => {
    let alive = true;

    setData(null);
    setRank(null);
    setError(null);

    (async () => {
      try {
        const book = await apiFetch<BookDetail>(`/v1/books/${id}`);
        if (!alive) return;
        setData(book);
      } catch (e) {
        if (!alive) return;
        setError(e instanceof Error ? e.message : String(e));
        return;
      }

      setRankLoading(true);
      try {
        const dash = await apiFetch<DashboardResponse>(`/v1/dashboard?limit=500&offset=0&sort=read_next`);
        if (!alive) return;
        const ranked = [...dash.items].sort(compareReadNext);
        const idx = ranked.findIndex((r) => r.shelf_item_id === id);
        setRank(idx >= 0 ? idx + 1 : null);
      } catch {
        if (!alive) return;
        setRank(null);
      } finally {
        if (alive) setRankLoading(false);
      }
    })();

    return () => {
      alive = false;
    };
  }, [id]);

  const rnTooltip = useMemo(() => (data ? readNextTooltip(data.read_next) : ""), [data]);

  if (error) {
    return (
      <main className="p-6 max-w-4xl mx-auto">
        <Link href="/dashboard" className="text-sm text-blue-600 hover:underline">
          ‚Üê Back to dashboard
        </Link>

        <div className="mt-4 rounded border border-red-200 bg-red-50 p-3 text-sm">
          <div className="font-medium">Couldn‚Äôt load this book.</div>
          <div className="mt-1 text-gray-700">{error}</div>
        </div>
      </main>
    );
  }

  if (!data) {
    return (
      <main className="p-6 max-w-4xl mx-auto">
        <Link href="/dashboard" className="text-sm text-blue-600 hover:underline">
          ‚Üê Back to dashboard
        </Link>

        <div className="mt-4 rounded border p-4 text-sm text-gray-600">Loading‚Ä¶</div>
      </main>
    );
  }

  return (
    <main className="p-6 max-w-4xl mx-auto">
      <div className="flex items-start justify-between gap-4">
        <div>
          <Link href="/dashboard" className="text-sm text-blue-600 hover:underline">
            ‚Üê Back to dashboard
          </Link>
          <h1 className="text-2xl font-semibold mt-2">{data.shelf_item.title}</h1>
          <p className="text-gray-600">{data.shelf_item.author}</p>
          {data.shelf_item.shelf ? <p className="text-xs text-gray-500 mt-1">Shelf: {data.shelf_item.shelf}</p> : null}
        </div>
      </div>

      <section className="mt-6 rounded border p-4">
        <h2 className="font-semibold">Read Next</h2>
        <p className="text-sm text-gray-600 mt-1">
          {rankLoading ? (
            <span>Calculating rank‚Ä¶</span>
          ) : rank ? (
            <span>
              When sorting by Read Next, this item is ranked <span className="font-medium">#{rank}</span>.
            </span>
          ) : (
            <span>Rank unavailable (not found in the dashboard list).</span>
          )}
        </p>

        <div className="mt-3 text-sm">
          <div>
            <span className="font-medium">Tier:</span> {data.read_next.tier.replace("_", " ")}
          </div>
          <div className="mt-1">
            <span className="font-medium">Score:</span> {data.read_next.score.toFixed(1)}
          </div>
          {data.read_next.best_format ? (
            <div className="mt-1">
              <span className="font-medium">Best format:</span> {data.read_next.best_format}
            </div>
          ) : null}

          <div className="mt-3">
            <div className="font-medium">Why it ranks here</div>
            <ul className="list-disc pl-5 mt-2 text-gray-700">
              {data.read_next.reasons.map((r, i) => (
                <li key={i}>{r}</li>
              ))}
            </ul>
            <div className="text-xs text-gray-500 mt-3 whitespace-pre-line" title={rnTooltip}>
              Tip: hover for the full scoring summary.
            </div>
          </div>
        </div>
      </section>

      <section className="mt-6 rounded border p-4">
        <h2 className="font-semibold">Availability</h2>
        {data.availability.length ? (
          <ul className="mt-2 space-y-2">
            {data.availability.map((a) => (
              <li key={a.format} className="text-sm">
                <div className="flex items-center justify-between">
                  <div className="capitalize">
                    {a.format}: {a.status.replace("_", " ")}
                  </div>
                  {a.deep_link ? (
                    <a href={a.deep_link} className="text-blue-600 hover:underline" target="_blank" rel="noreferrer">
                      Open
                    </a>
                  ) : null}
                </div>
                <div className="text-xs text-gray-600 mt-1">
                  {a.status === "available" && a.copies_available != null ? <span>{a.copies_available} available</span> : null}
                  {a.status === "hold" && a.holds != null ? <span>{a.holds} holds</span> : null}
                  {a.last_checked_at ? <span className="ml-2">Checked: {new Date(a.last_checked_at).toLocaleString()}</span> : null}
                </div>
              </li>
            ))}
          </ul>
        ) : (
          <p className="text-sm text-gray-600 mt-2">No availability data yet.</p>
        )}
      </section>

      <section className="mt-6 rounded border p-4">
        <h2 className="font-semibold">Catalog match</h2>
        {data.match ? (
          <div className="mt-2 text-sm">
            <div>
              <span className="font-medium">Provider:</span> {data.match.catalog_item.provider}
            </div>
            <div className="mt-1">
              <span className="font-medium">Confidence:</span> {(data.match.confidence * 100).toFixed(0)}%
            </div>
            <div className="mt-2">
              <span className="font-medium">Catalog title:</span> {data.match.catalog_item.title}
            </div>
            <div className="text-xs text-gray-600 mt-2">
              Evidence:
              <pre className="mt-1 p-2 bg-gray-50 rounded overflow-auto">{JSON.stringify(data.match.evidence, null, 2)}</pre>
            </div>
          </div>
        ) : (
          <p className="text-sm text-gray-600 mt-2">No match found.</p>
        )}
      </section>
    </main>
  );
}
</file>

<file path="apps/web/package.json">
{
  "name": "web",
  "version": "0.1.0",
  "private": true,
  "scripts": {
    "dev": "next dev",
    "lint": "next lint",
    "build": "next build",
    "start": "next start",
    "test": "vitest run",
    "test:watch": "vitest"
  },
  "dependencies": {
    "next": "^14.2.5",
    "react": "^18.2.0",
    "react-dom": "^18.2.0"
  },
  "devDependencies": {
    "@playwright/test": "1.57.0",
    "@testing-library/jest-dom": "^6.8.0",
    "@testing-library/react": "^16.3.0",
    "@types/node": "^20.14.10",
    "@types/react": "^18.3.3",
    "@types/react-dom": "^18.3.0",
    "@vitejs/plugin-react": "^5.0.2",
    "autoprefixer": "^10.4.20",
    "eslint": "^8.57.0",
    "eslint-config-next": "^14.2.35",
    "jsdom": "^25.0.1",
    "msw": "^2.10.0",
    "postcss": "^8.4.49",
    "tailwindcss": "^3.4.17",
    "typescript": "^5.5.4",
    "vitest": "^3.2.4"
  }
}
</file>

<file path="apps/web/tsconfig.json">
{
  "compilerOptions": {
    "target": "ES2020",
    "lib": [
      "DOM",
      "DOM.Iterable",
      "ES2020"
    ],
    "allowJs": true,
    "skipLibCheck": true,
    "strict": true,
    "noEmit": true,
    "esModuleInterop": true,
    "module": "ESNext",
    "moduleResolution": "Bundler",
    "resolveJsonModule": true,
    "isolatedModules": true,
    "jsx": "preserve",
    "baseUrl": ".",
    "paths": {
      "@/*": [
        "./src/*"
      ]
    },
    "incremental": true,
    "plugins": [
      {
        "name": "next"
      }
    ],
    "types": ["vitest/globals"]
  },
  "include": [
    "next-env.d.ts",
    "**/*.ts",
    "**/*.tsx",
    ".next/types/**/*.ts"
  ],
  "exclude": [
    "node_modules"
  ]
}
</file>

<file path="services/api/alembic/versions/c7a5b2b4af1e_add_missing_shelf_columns.py">
"""add missing shelf columns

Revision ID: c7a5b2b4af1e
Revises: 8f008b42145c
Create Date: 2025-12-23 23:59:00.000000

"""

from typing import Sequence, Union

import sqlalchemy as sa
from alembic import op

# revision identifiers, used by Alembic.
revision: str = "c7a5b2b4af1e"
down_revision: Union[str, Sequence[str], None] = "8f008b42145c"
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    """Upgrade schema."""
    op.add_column(
        "shelf_sources",
        sa.Column(
            "provider", sa.String(length=40), nullable=False, server_default="goodreads"
        ),
    )
    op.add_column(
        "shelf_sources",
        sa.Column(
            "meta",
            sa.JSON(),
            nullable=False,
            server_default=sa.text("'{}'"),
        ),
    )
    op.add_column(
        "shelf_sources",
        sa.Column("last_synced_at", sa.DateTime(timezone=True), nullable=True),
    )
    op.add_column(
        "shelf_sources",
        sa.Column("last_sync_status", sa.String(length=30), nullable=True),
    )
    op.add_column(
        "shelf_sources",
        sa.Column("last_sync_error", sa.String(length=2000), nullable=True),
    )
    op.add_column(
        "shelf_sources",
        sa.Column(
            "updated_at",
            sa.DateTime(timezone=True),
            nullable=False,
            server_default=sa.text("CURRENT_TIMESTAMP"),
        ),
    )

    op.add_column(
        "shelf_items",
        sa.Column("external_id", sa.String(length=120), nullable=True),
    )
    op.add_column(
        "shelf_items",
        sa.Column("shelf", sa.String(length=80), nullable=True),
    )


def downgrade() -> None:
    """Downgrade schema."""
    op.drop_column("shelf_items", "shelf")
    op.drop_column("shelf_items", "external_id")
    op.drop_column("shelf_sources", "updated_at")
    op.drop_column("shelf_sources", "last_sync_error")
    op.drop_column("shelf_sources", "last_sync_status")
    op.drop_column("shelf_sources", "last_synced_at")
    op.drop_column("shelf_sources", "meta")
    op.drop_column("shelf_sources", "provider")
</file>

<file path="services/api/alembic/env.py">
from __future__ import annotations

import os
from logging.config import fileConfig

from alembic import context
from app.models import Base  # noqa: F401
from sqlalchemy import engine_from_config, pool

config = context.config

if config.config_file_name is not None:
    fileConfig(config.config_file_name)

# Point Alembic at your metadata
target_metadata = Base.metadata

DEFAULT_DATABASE_URL = (
    "postgresql+psycopg2://shelfsync:shelfsync@localhost:5432/shelfsync"
)


def get_database_url() -> str:
    env_url = os.getenv("DATABASE_URL")
    if env_url:
        return env_url

    ini_url = config.get_main_option("sqlalchemy.url")
    if ini_url and ini_url != "driver://user:pass@localhost/dbname":
        return ini_url

    try:
        from app.core.config import settings
    except Exception:
        return DEFAULT_DATABASE_URL

    return settings.database_url


def run_migrations_offline() -> None:
    url = get_database_url()
    context.configure(
        url=url,
        target_metadata=target_metadata,
        literal_binds=True,
        dialect_opts={"paramstyle": "named"},
    )

    with context.begin_transaction():
        context.run_migrations()


def run_migrations_online() -> None:
    configuration = config.get_section(config.config_ini_section) or {}
    configuration["sqlalchemy.url"] = get_database_url()

    connection = config.attributes.get("connection")
    if connection is None:
        connectable = engine_from_config(
            configuration,
            prefix="sqlalchemy.",
            poolclass=pool.NullPool,
        )

        with connectable.connect() as connection:
            context.configure(connection=connection, target_metadata=target_metadata)
            with context.begin_transaction():
                context.run_migrations()
    else:
        context.configure(connection=connection, target_metadata=target_metadata)
        with context.begin_transaction():
            context.run_migrations()


if context.is_offline_mode():
    run_migrations_offline()
else:
    run_migrations_online()
</file>

<file path="services/api/app/api/routes/auth.py">
from __future__ import annotations

from datetime import timedelta

from app.api.deps import get_current_user
from app.core.config import settings
from app.core.security import (
    create_access_token,
    hash_password,
    password_needs_rehash,
    verify_password,
)
from app.db.session import get_db
from app.models.user import User
from app.models.user_settings import UserSettings
from app.schemas.auth import LoginIn, SignUpIn, UserOut
from fastapi import APIRouter, Depends, HTTPException, Response
from sqlalchemy import select
from sqlalchemy.orm import Session

router = APIRouter(prefix="/v1/auth", tags=["auth"])

DEMO_EMAIL = "demo@example.com"


def _get_user_by_email(db: Session, email: str) -> User | None:
    return db.execute(select(User).where(User.email == email)).scalar_one_or_none()


def _ensure_user_settings(db: Session, user_id: str) -> None:
    existing = db.execute(
        select(UserSettings).where(UserSettings.user_id == user_id)
    ).scalar_one_or_none()
    if existing is None:
        db.add(UserSettings(user_id=user_id))
        db.flush()


def _create_user(db: Session, *, email: str, password: str) -> User:
    u = User(email=email, password_hash=hash_password(password))
    db.add(u)
    db.flush()
    _ensure_user_settings(db, u.id)
    db.commit()
    db.refresh(u)
    return u


def _update_user_password(db: Session, u: User, password: str) -> None:
    u.password_hash = hash_password(password)
    db.add(u)
    db.commit()
    db.refresh(u)


def _set_auth_cookie(response: Response, *, subject: str) -> None:
    token = create_access_token(
        subject=subject,
        expires_delta=timedelta(minutes=settings.auth_access_token_ttl_minutes),
    )
    response.set_cookie(
        key=settings.auth_cookie_name,
        value=token,
        httponly=True,
        secure=settings.auth_cookie_secure,
        samesite=settings.auth_cookie_samesite,
        path="/",
    )


@router.post("/signup", response_model=UserOut)
def signup(payload: SignUpIn, response: Response, db: Session = Depends(get_db)):
    existing = _get_user_by_email(db, payload.email)
    if existing is not None:
        raise HTTPException(status_code=400, detail="Email already registered")

    u = _create_user(db, email=payload.email, password=payload.password)
    _set_auth_cookie(response, subject=u.id)
    return UserOut(id=u.id, email=u.email)


@router.post("/login", response_model=UserOut)
def login(payload: LoginIn, response: Response, db: Session = Depends(get_db)):
    u = _get_user_by_email(db, payload.email)

    # Demo auto-create for local/dev if enabled
    if (
        u is None
        and settings.demo_login_enabled
        and settings.env in {"local", "development", "dev"}
    ):
        if payload.email == DEMO_EMAIL:
            u = _create_user(db, email=payload.email, password=payload.password)

    if u is None:
        raise HTTPException(status_code=401, detail="Invalid email or password")

    if not verify_password(payload.password, u.password_hash):
        raise HTTPException(status_code=401, detail="Invalid email or password")

    # If this was a legacy hash, upgrade it automatically.
    if password_needs_rehash(u.password_hash):
        _update_user_password(db, u, payload.password)

    _set_auth_cookie(response, subject=u.id)
    return UserOut(id=u.id, email=u.email)


@router.get("/me", response_model=UserOut)
def me(user: User = Depends(get_current_user)) -> UserOut:
    return UserOut(id=user.id, email=user.email)


@router.post("/logout")
def logout(response: Response):
    response.delete_cookie(settings.auth_cookie_name, path="/")
    return {"ok": True}
</file>

<file path="services/api/app/api/routes/books.py">
from __future__ import annotations

from dataclasses import asdict
from datetime import datetime, timezone

from app.api.deps import get_current_user
from app.api.rate_limit import rate_limiter
from app.core.config import settings
from app.db.session import get_db
from app.models.availability_snapshot import AvailabilitySnapshot
from app.models.catalog_item import CatalogItem
from app.models.catalog_match import CatalogMatch
from app.models.shelf_item import ShelfItem
from app.models.shelf_source import ShelfSource
from app.models.user_settings import UserSettings
from app.schemas.books import (
    BookDetailMatchOut,
    BookDetailOut,
    BookDetailSettingsOut,
    BookDetailShelfItemOut,
    BookDetailSourceOut,
)
from app.schemas.dashboard import AvailabilityOut, ReadNextOut
from app.services.read_next_scoring import compute_read_next
from fastapi import APIRouter, Depends, HTTPException
from sqlalchemy import select
from sqlalchemy.orm import Session

router = APIRouter(prefix="/v1", tags=["books"])


@router.get(
    "/books/{shelf_item_id}",
    response_model=BookDetailOut,
    dependencies=[
        Depends(
            rate_limiter(
                "book_detail",
                limit=settings.rate_limit_books_per_window,
                window_seconds=settings.rate_limit_window_seconds,
            )
        )
    ],
)
def get_book_detail(
    shelf_item_id: str,
    db: Session = Depends(get_db),
    user=Depends(get_current_user),
):
    si = db.get(ShelfItem, shelf_item_id)
    if si is None or si.user_id != user.id:
        raise HTTPException(status_code=404, detail="Book not found")

    user_settings = db.execute(
        select(UserSettings).where(UserSettings.user_id == user.id)
    ).scalar_one_or_none()
    if user_settings is None:
        user_settings = UserSettings(user_id=user.id)
        db.add(user_settings)
        db.commit()
        db.refresh(user_settings)

    preferred_formats = list(user_settings.preferred_formats or [])

    m = (
        db.execute(
            select(CatalogMatch)
            .where(CatalogMatch.user_id == user.id)
            .where(CatalogMatch.shelf_item_id == si.id)
            .order_by(CatalogMatch.confidence.desc())
        )
        .scalars()
        .first()
    )

    availability: list[AvailabilityOut] = []
    match_out: BookDetailMatchOut | None = None

    if m is not None:
        ci = db.get(CatalogItem, m.catalog_item_id)

        if ci is not None:
            match_out = BookDetailMatchOut(
                catalog_item_id=m.catalog_item_id,
                provider=m.provider,
                provider_item_id=ci.provider_item_id,
                method=m.method,
                confidence=float(m.confidence or 0.0),
            )

        snaps = (
            db.execute(
                select(AvailabilitySnapshot)
                .where(AvailabilitySnapshot.user_id == user.id)
                .where(AvailabilitySnapshot.catalog_item_id == m.catalog_item_id)
            )
            .scalars()
            .all()
        )

        def _sort_key(a: AvailabilitySnapshot) -> tuple[int, str]:
            if a.format in preferred_formats:
                return (preferred_formats.index(a.format), a.format)
            return (999, a.format)

        for a in sorted(snaps, key=_sort_key):
            availability.append(
                AvailabilityOut(
                    format=a.format,
                    status=a.status,
                    copies_available=a.copies_available,
                    copies_total=a.copies_total,
                    holds=a.holds,
                    deep_link=a.deep_link,
                    last_checked_at=a.last_checked_at,
                )
            )

    rn = compute_read_next(availability, preferred_formats)

    source_out: BookDetailSourceOut | None = None
    if si.shelf_source_id:
        src = db.get(ShelfSource, si.shelf_source_id)
        if src is not None:
            source_out = BookDetailSourceOut(
                id=src.id,
                source_type=src.source_type,
                provider=src.provider,
                source_ref=src.source_ref,
                last_synced_at=src.last_synced_at,
                last_sync_status=src.last_sync_status,
                last_sync_error=src.last_sync_error,
            )

    return BookDetailOut(
        shelf_item=BookDetailShelfItemOut(
            id=si.id,
            title=si.title,
            author=si.author,
            isbn10=si.isbn10,
            isbn13=si.isbn13,
            asin=si.asin,
            shelf=si.shelf,
            needs_fuzzy_match=si.needs_fuzzy_match,
        ),
        match=match_out,
        availability=availability,
        source=source_out,
        settings=BookDetailSettingsOut(
            library_system=user_settings.library_system,
            preferred_formats=preferred_formats,
        ),
        read_next=ReadNextOut(**asdict(rn)),
        generated_at=datetime.now(timezone.utc),
    )
</file>

<file path="services/api/app/api/routes/matching.py">
from __future__ import annotations

from app.api.deps import get_current_user
from app.db.session import get_db
from app.models.availability_snapshot import AvailabilitySnapshot
from app.models.catalog_item import CatalogItem
from app.models.catalog_match import CatalogMatch
from app.models.shelf_item import ShelfItem
from app.schemas.matching import (
    AvailabilityOut,
    CatalogItemOut,
    JobStatusOut,
    MatchOut,
    RefreshEnqueuedOut,
)
from app.workers.jobs import refresh_matching_for_user
from app.workers.queue import get_queue
from app.workers.redis_conn import get_redis_connection
from fastapi import APIRouter, Depends, HTTPException
from rq import Retry
from rq.job import Job
from sqlalchemy import select
from sqlalchemy.orm import Session

router = APIRouter(prefix="/v1", tags=["matching"])


@router.post("/matching/refresh", response_model=RefreshEnqueuedOut)
def refresh_matching(user=Depends(get_current_user)):
    q = get_queue()
    job = q.enqueue(
        refresh_matching_for_user, user.id, retry=Retry(max=2, interval=[5, 15])
    )
    return {"job_id": job.id}


@router.get("/matching/refresh/{job_id}", response_model=JobStatusOut)
def refresh_status(job_id: str, user=Depends(get_current_user)):
    # user param enforces auth; job is keyed only by id
    try:
        conn = get_redis_connection()
        job = Job.fetch(job_id, connection=conn)
    except Exception as e:
        raise HTTPException(status_code=404, detail=f"Job not found: {e}")

    return JobStatusOut(
        id=job.id,
        status=job.get_status(),
        created_at=job.created_at,
        started_at=job.started_at,
        ended_at=job.ended_at,
        result=job.result if isinstance(job.result, dict) else None,
        exc_info=job.exc_info,
    )


@router.get("/matches", response_model=list[MatchOut])
def list_matches(
    db: Session = Depends(get_db),
    user=Depends(get_current_user),
    limit: int = 50,
    offset: int = 0,
):
    # Join: shelf_items ‚Üí catalog_matches ‚Üí catalog_items, then attach availability snapshots
    rows = db.execute(
        select(ShelfItem, CatalogMatch, CatalogItem)
        .join(CatalogMatch, CatalogMatch.shelf_item_id == ShelfItem.id)
        .join(CatalogItem, CatalogItem.id == CatalogMatch.catalog_item_id)
        .where(ShelfItem.user_id == user.id)
        .order_by(ShelfItem.created_at.desc())
        .limit(limit)
        .offset(offset)
    ).all()

    catalog_ids = [ci.id for _, _, ci in rows]
    av_rows = (
        db.execute(
            select(AvailabilitySnapshot)
            .where(AvailabilitySnapshot.user_id == user.id)
            .where(AvailabilitySnapshot.catalog_item_id.in_(catalog_ids))
        )
        .scalars()
        .all()
    )
    avail_by_catalog: dict[str, list[AvailabilitySnapshot]] = {}
    for a in av_rows:
        avail_by_catalog.setdefault(a.catalog_item_id, []).append(a)

    out: list[MatchOut] = []
    for si, m, ci in rows:
        av_out = [
            AvailabilityOut(
                format=a.format,
                status=a.status,
                copies_available=a.copies_available,
                copies_total=a.copies_total,
                holds=a.holds,
                deep_link=a.deep_link,
                last_checked_at=a.last_checked_at,
            )
            for a in sorted(avail_by_catalog.get(ci.id, []), key=lambda x: x.format)
        ]

        out.append(
            MatchOut(
                shelf_item_id=si.id,
                catalog_item=CatalogItemOut(
                    id=ci.id,
                    provider=ci.provider,
                    provider_item_id=ci.provider_item_id,
                    title=ci.title,
                    author=ci.author,
                    isbn10=ci.isbn10,
                    isbn13=ci.isbn13,
                    asin=ci.asin,
                ),
                method=m.method,
                confidence=m.confidence,
                evidence=m.evidence,
                availability=av_out,
            )
        )

    return out


@router.get("/matches/{shelf_item_id}", response_model=MatchOut)
def get_match(
    shelf_item_id: str,
    db: Session = Depends(get_db),
    user=Depends(get_current_user),
):
    row = db.execute(
        select(ShelfItem, CatalogMatch, CatalogItem)
        .join(CatalogMatch, CatalogMatch.shelf_item_id == ShelfItem.id)
        .join(CatalogItem, CatalogItem.id == CatalogMatch.catalog_item_id)
        .where(ShelfItem.user_id == user.id)
        .where(ShelfItem.id == shelf_item_id)
    ).first()

    if not row:
        raise HTTPException(status_code=404, detail="Match not found")

    si, m, ci = row
    av = (
        db.execute(
            select(AvailabilitySnapshot)
            .where(AvailabilitySnapshot.user_id == user.id)
            .where(AvailabilitySnapshot.catalog_item_id == ci.id)
        )
        .scalars()
        .all()
    )

    return MatchOut(
        shelf_item_id=si.id,
        catalog_item=CatalogItemOut(
            id=ci.id,
            provider=ci.provider,
            provider_item_id=ci.provider_item_id,
            title=ci.title,
            author=ci.author,
            isbn10=ci.isbn10,
            isbn13=ci.isbn13,
            asin=ci.asin,
        ),
        method=m.method,
        confidence=m.confidence,
        evidence=m.evidence,
        availability=[
            AvailabilityOut(
                format=a.format,
                status=a.status,
                copies_available=a.copies_available,
                copies_total=a.copies_total,
                holds=a.holds,
                deep_link=a.deep_link,
                last_checked_at=a.last_checked_at,
            )
            for a in sorted(av, key=lambda x: x.format)
        ],
    )
</file>

<file path="services/api/app/api/routes/settings.py">
from __future__ import annotations

from app.api.deps import get_current_user
from app.db.session import get_db
from app.models.user_settings import UserSettings
from app.schemas.settings import SettingsPatchIn, UserSettingsOut
from fastapi import APIRouter, Depends, HTTPException
from sqlalchemy.orm import Session

router = APIRouter(prefix="/v1", tags=["settings"])


@router.get("/settings", response_model=UserSettingsOut)
def get_settings(db: Session = Depends(get_db), user=Depends(get_current_user)):
    s = db.get(UserSettings, user.id)
    if not s:
        raise HTTPException(status_code=404, detail="Settings not found")
    return s


@router.patch("/settings", response_model=UserSettingsOut)
def patch_settings(
    payload: SettingsPatchIn,
    db: Session = Depends(get_db),
    user=Depends(get_current_user),
):
    s = db.get(UserSettings, user.id)
    if not s:
        raise HTTPException(status_code=404, detail="Settings not found")

    if payload.library_system is not None:
        s.library_system = payload.library_system

    if payload.preferred_formats is not None:
        # Normalize + de-dupe, keep stable order ebook->audiobook
        want = []
        for fmt in ["ebook", "audiobook"]:
            if fmt in payload.preferred_formats:
                want.append(fmt)
        s.preferred_formats = want

    if payload.notifications_enabled is not None:
        s.notifications_enabled = payload.notifications_enabled

    db.add(s)
    db.commit()
    db.refresh(s)
    return s
</file>

<file path="services/api/app/api/routes/shelf_sources.py">
from __future__ import annotations

from app.api.deps import get_current_user
from app.db.session import get_db
from app.models.shelf_source import ShelfSource
from app.schemas.shelf import (
    ImportErrorOut,
    ImportSummaryOut,
    RssConnectIn,
    ShelfSourceOut,
    SyncEnqueuedOut,
)
from app.services.goodreads_csv import parse_goodreads_csv
from app.services.shelf_import import upsert_shelf_items
from app.workers.jobs import sync_goodreads_rss
from app.workers.queue import get_queue
from fastapi import APIRouter, Depends, File, HTTPException, UploadFile
from rq import Retry
from sqlalchemy import select
from sqlalchemy.orm import Session

router = APIRouter(prefix="/v1/shelf-sources", tags=["shelf-sources"])


@router.get("", response_model=list[ShelfSourceOut])
def list_sources(db: Session = Depends(get_db), user=Depends(get_current_user)):
    return (
        db.execute(select(ShelfSource).where(ShelfSource.user_id == user.id))
        .scalars()
        .all()
    )


@router.post("/rss", response_model=ShelfSourceOut)
def connect_rss(
    payload: RssConnectIn, db: Session = Depends(get_db), user=Depends(get_current_user)
):
    existing = db.execute(
        select(ShelfSource)
        .where(ShelfSource.user_id == user.id)
        .where(ShelfSource.source_type == "rss")
        .where(ShelfSource.provider == "goodreads")
        .where(ShelfSource.source_ref == payload.rss_url)
    ).scalar_one_or_none()

    if existing is None:
        source = ShelfSource(
            user_id=user.id,
            source_type="rss",
            provider="goodreads",
            source_ref=payload.rss_url,
            meta={"shelf": payload.shelf} if payload.shelf else {},
            is_active=True,
        )
        db.add(source)
        db.commit()
        db.refresh(source)
    else:
        existing.meta = {
            **(existing.meta or {}),
            **({"shelf": payload.shelf} if payload.shelf else {}),
        }
        existing.is_active = True
        db.add(existing)
        db.commit()
        db.refresh(existing)
        source = existing

    if payload.sync_now:
        q = get_queue()
        q.enqueue(sync_goodreads_rss, source.id, retry=Retry(max=2, interval=[5, 15]))

    return source


@router.post("/csv", response_model=ImportSummaryOut)
def import_csv(
    file: UploadFile = File(...),
    db: Session = Depends(get_db),
    user=Depends(get_current_user),
):
    raw = file.file.read()

    rows, parse_errors = parse_goodreads_csv(raw)

    source = db.execute(
        select(ShelfSource)
        .where(ShelfSource.user_id == user.id)
        .where(ShelfSource.source_type == "csv")
        .where(ShelfSource.provider == "goodreads")
    ).scalar_one_or_none()
    if source is None:
        source = ShelfSource(
            user_id=user.id,
            source_type="csv",
            provider="goodreads",
            source_ref=file.filename or "upload",
            meta={},
            is_active=True,
        )
        db.add(source)
        db.commit()
        db.refresh(source)

    summary = upsert_shelf_items(db, user_id=user.id, source=source, items=rows)

    errors_out: list[ImportErrorOut] = [
        ImportErrorOut(key=e.key, error=e.error) for e in summary.errors
    ] + [
        ImportErrorOut(key=f"line:{e['line']}", error=e["error"]) for e in parse_errors
    ]

    return ImportSummaryOut(
        created=summary.created,
        updated=summary.updated,
        skipped=summary.skipped + len(parse_errors),
        errors=errors_out,
    )


@router.post("/{source_id}/sync", response_model=SyncEnqueuedOut)
def sync_source(
    source_id: str, db: Session = Depends(get_db), user=Depends(get_current_user)
):
    source = db.get(ShelfSource, source_id)
    if source is None or source.user_id != user.id:
        raise HTTPException(status_code=404, detail="Source not found")

    if source.source_type != "rss":
        raise HTTPException(status_code=400, detail="Only RSS sources can be synced")

    q = get_queue()
    job = q.enqueue(sync_goodreads_rss, source.id, retry=Retry(max=2, interval=[5, 15]))
    return SyncEnqueuedOut(job_id=job.id)


@router.delete("/{source_id}", status_code=204)
def delete_source(
    source_id: str, db: Session = Depends(get_db), user=Depends(get_current_user)
):
    source = db.get(ShelfSource, source_id)
    if source is None or source.user_id != user.id:
        raise HTTPException(status_code=404, detail="Source not found")
    db.delete(source)
    db.commit()
    return None
</file>

<file path="services/api/app/api/rate_limit.py">
from __future__ import annotations

import time
from typing import Callable, cast

from app.api.deps import get_current_user
from app.core.redis_client import get_redis
from fastapi import Depends, HTTPException
from redis import Redis


def rate_limiter(
    scope: str,
    *,
    limit: int,
    window_seconds: int,
) -> Callable[[], None]:
    """Simple fixed-window rate limiter using Redis INCR + EXPIRE.

    If Redis is unavailable, the limiter becomes a no-op (fail open).
    """

    def _dep(user=Depends(get_current_user)) -> None:
        r = get_redis()
        if r is None:
            return

        # Fixed-window bucket
        now = int(time.time())
        bucket = now // window_seconds
        key = f"rl:{scope}:{user.id}:{bucket}"

        try:
            # redis-py typing can be `Awaitable[Any] | Any` in stubs; cast to satisfy mypy.
            count = cast(int, cast(Redis, r).incr(key))
            if count == 1:
                cast(Redis, r).expire(key, window_seconds)
        except Exception:
            # If Redis errors, don't block requests.
            return

        if count > limit:
            retry_after = max(1, window_seconds - (now % window_seconds))
            raise HTTPException(
                status_code=429,
                detail="Rate limit exceeded",
                headers={"Retry-After": str(retry_after)},
            )

    return _dep
</file>

<file path="services/api/app/core/otel.py">
from __future__ import annotations

from app.core.config import settings
from opentelemetry import trace
from opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter
from opentelemetry.instrumentation.fastapi import FastAPIInstrumentor
from opentelemetry.instrumentation.httpx import HTTPXClientInstrumentor
from opentelemetry.sdk.resources import Resource
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor


def init_otel(app) -> None:
    if not getattr(settings, "otel_enabled", False):
        return

    resource = Resource.create(
        {"service.name": getattr(settings, "api_name", "shelfsync-api")}
    )
    provider = TracerProvider(resource=resource)

    # Prefer env vars (OTEL_EXPORTER_OTLP_ENDPOINT, etc.); allow an optional settings override.
    endpoint = getattr(settings, "otel_otlp_endpoint", None)
    exporter = OTLPSpanExporter(endpoint=endpoint) if endpoint else OTLPSpanExporter()

    provider.add_span_processor(BatchSpanProcessor(exporter))
    trace.set_tracer_provider(provider)

    FastAPIInstrumentor.instrument_app(app)
    HTTPXClientInstrumentor().instrument()
</file>

<file path="services/api/app/core/security.py">
from __future__ import annotations

from datetime import datetime, timedelta, timezone

from app.core.config import settings
from jose import jwt
from passlib.context import CryptContext  # type: ignore[import-untyped]

# Support legacy PBKDF2 hashes for login migration, but always *create* bcrypt hashes.
pwd_context = CryptContext(
    schemes=["bcrypt", "pbkdf2_sha256"],
    deprecated="auto",
)


def verify_password(plain_password: str, hashed_password: str) -> bool:
    return pwd_context.verify(plain_password, hashed_password)


def hash_password(password: str) -> str:
    return pwd_context.hash(password)


def password_needs_rehash(hashed_password: str) -> bool:
    # True when the stored hash uses a deprecated scheme/params (e.g., pbkdf2_sha256).
    return pwd_context.needs_update(hashed_password)


def create_access_token(subject: str, expires_delta: timedelta | None = None) -> str:
    if expires_delta is None:
        expires_delta = timedelta(minutes=settings.auth_access_token_ttl_minutes)

    expire = datetime.now(timezone.utc) + expires_delta
    to_encode = {"sub": subject, "exp": expire}
    return jwt.encode(
        to_encode, settings.auth_secret_key, algorithm=settings.auth_algorithm
    )


def decode_access_token(token: str) -> dict:
    return jwt.decode(
        token, settings.auth_secret_key, algorithms=[settings.auth_algorithm]
    )
</file>

<file path="services/api/app/ingestion/csv_import.py">
from __future__ import annotations

import csv
import io

REQUIRED = {"title", "author"}


def _normalize_header(h: str) -> str:
    return " ".join((h or "").strip().lower().split())


def parse_goodreads_csv(csv_bytes: bytes) -> tuple[list[dict], list[str]]:
    """Parse Goodreads export CSV.

    Returns: (items, errors)
    Each item contains minimal fields:
    - title
    - author
    - isbn13
    - isbn10
    - asin

    Parsing is forgiving about extra columns.
    It collects row-level errors and continues, unless the file is structurally invalid.
    """

    errors: list[str] = []

    try:
        text = csv_bytes.decode("utf-8-sig")
    except UnicodeDecodeError:
        text = csv_bytes.decode("utf-8", errors="replace")

    buf = io.StringIO(text)
    reader = csv.DictReader(buf)

    if not reader.fieldnames:
        return [], ["CSV appears to be missing a header row."]

    # Build a header map so we can handle variants like "ISBN13" vs "ISBN 13".
    header_map = {_normalize_header(h): h for h in reader.fieldnames}

    missing = [h for h in REQUIRED if h not in header_map]
    if missing:
        return [], [f"CSV is missing required column(s): {', '.join(sorted(missing))}"]

    title_col = header_map["title"]
    author_col = header_map["author"]

    isbn13_col = header_map.get("isbn13") or header_map.get("isbn 13")
    isbn10_col = (
        header_map.get("isbn") or header_map.get("isbn10") or header_map.get("isbn 10")
    )
    asin_col = header_map.get("asin")

    items: list[dict] = []

    for idx, row in enumerate(reader, start=2):
        title = (row.get(title_col) or "").strip()
        author = (row.get(author_col) or "").strip()

        if not title or not author:
            errors.append(f"Row {idx}: missing title or author")
            continue

        items.append(
            {
                "title": title,
                "author": author,
                "isbn13": (row.get(isbn13_col) or "").strip() if isbn13_col else None,
                "isbn10": (row.get(isbn10_col) or "").strip() if isbn10_col else None,
                "asin": (row.get(asin_col) or "").strip() if asin_col else None,
            }
        )

    return items, errors
</file>

<file path="services/api/app/ingestion/fetch.py">
from __future__ import annotations

from urllib.parse import urlparse, urlunparse

import httpx
from app.core.config import settings


def _rewrite_goodreads_url(url: str, base_url: str | None) -> str:
    if not base_url:
        return url

    parsed = urlparse(url)
    hostname = (parsed.hostname or "").lower()
    if not hostname or not (
        hostname == "goodreads.com" or hostname.endswith(".goodreads.com")
    ):
        return url

    base = urlparse(base_url)
    if not base.scheme or not base.netloc:
        return url

    base_path = base.path.rstrip("/")
    incoming_path = parsed.path or "/"
    merged_path = f"{base_path}{incoming_path}" if base_path else incoming_path

    rewritten = parsed._replace(
        scheme=base.scheme, netloc=base.netloc, path=merged_path
    )
    return urlunparse(rewritten)


async def fetch_text(url: str) -> str:
    request_url = _rewrite_goodreads_url(url, settings.goodreads_base_url)
    async with httpx.AsyncClient(timeout=15.0, follow_redirects=True) as client:
        resp = await client.get(request_url, headers={"User-Agent": "ShelfSync/0.1"})
        resp.raise_for_status()
        return resp.text
</file>

<file path="services/api/app/ingestion/rss.py">
from __future__ import annotations

import re
import xml.etree.ElementTree as ET

from bs4 import BeautifulSoup

ISBN_RE = re.compile(
    r"\b(?:ISBN(?:-13)?|ISBN13)\s*[:#]?\s*([0-9\-]{10,17})\b", re.IGNORECASE
)
ASIN_RE = re.compile(r"\bASIN\s*[:#]?\s*([A-Z0-9]{8,20})\b", re.IGNORECASE)


def _local(tag: str) -> str:
    # Handles namespaced tags like {http://purl.org/dc/elements/1.1/}creator
    return tag.split("}")[-1]


def _find_first_text(elem: ET.Element, wanted: set[str]) -> str | None:
    for child in elem.iter():
        if _local(child.tag) in wanted and (child.text or "").strip():
            return (child.text or "").strip()
    return None


def _extract_identifiers_from_description(
    description_html: str,
) -> tuple[str | None, str | None]:
    if not description_html:
        return None, None

    # Goodreads often embeds metadata in HTML inside <description>.
    soup = BeautifulSoup(description_html, "html.parser")
    text = soup.get_text(" ", strip=True)

    isbn = None
    asin = None

    m = ISBN_RE.search(text)
    if m:
        isbn = m.group(1)

    m2 = ASIN_RE.search(text)
    if m2:
        asin = m2.group(1)

    return isbn, asin


def parse_goodreads_rss(xml_text: str) -> list[dict]:
    """Parse a Goodreads shelf RSS feed.

    Returns a list of dicts with minimal fields:
    - title
    - author
    - isbn (raw-ish, may be isbn10 or isbn13)
    - asin
    - goodreads_book_id (best-effort)

    The parser is defensive: it will skip items missing title/author.
    """

    root = ET.fromstring(xml_text)
    items = root.findall(".//item")

    out: list[dict] = []

    for item in items:
        title = _find_first_text(item, {"title"})
        author = _find_first_text(item, {"author_name", "creator", "author"})
        link = _find_first_text(item, {"link"})
        description = _find_first_text(item, {"description", "encoded"})

        if not title or not author:
            continue

        isbn_from_desc, asin_from_desc = _extract_identifiers_from_description(
            description or ""
        )

        goodreads_book_id = None
        if link and "/book/show/" in link:
            # https://www.goodreads.com/book/show/<id>-...
            try:
                part = link.split("/book/show/", 1)[1]
                goodreads_book_id = part.split("-", 1)[0].strip("/")
            except Exception:
                goodreads_book_id = None

        out.append(
            {
                "title": title.strip(),
                "author": author.strip(),
                "isbn": isbn_from_desc,
                "asin": asin_from_desc,
                "goodreads_book_id": goodreads_book_id,
            }
        )

    return out
</file>

<file path="services/api/app/models/availability_snapshot.py">
from __future__ import annotations

from datetime import datetime
from uuid import uuid4

from app.models.base import Base
from sqlalchemy import DateTime, ForeignKey, Integer, String, UniqueConstraint
from sqlalchemy.orm import Mapped, mapped_column


class AvailabilitySnapshot(Base):
    __tablename__ = "availability_snapshots"

    id: Mapped[str] = mapped_column(
        String(36), primary_key=True, default=lambda: str(uuid4())
    )

    user_id: Mapped[str] = mapped_column(
        String(36),
        ForeignKey("users.id", ondelete="CASCADE"),
        nullable=False,
        index=True,
    )
    catalog_item_id: Mapped[str] = mapped_column(
        String(36),
        ForeignKey("catalog_items.id", ondelete="CASCADE"),
        nullable=False,
        index=True,
    )

    format: Mapped[str] = mapped_column(String(20), nullable=False)  # ebook | audiobook
    status: Mapped[str] = mapped_column(
        String(20), nullable=False
    )  # available | hold | not_owned

    copies_available: Mapped[int | None] = mapped_column(Integer, nullable=True)
    copies_total: Mapped[int | None] = mapped_column(Integer, nullable=True)
    holds: Mapped[int | None] = mapped_column(Integer, nullable=True)

    deep_link: Mapped[str | None] = mapped_column(String(500), nullable=True)

    last_checked_at: Mapped[datetime] = mapped_column(
        DateTime(timezone=True), nullable=False
    )

    __table_args__ = (
        UniqueConstraint(
            "user_id", "catalog_item_id", "format", name="uq_avail_user_item_format"
        ),
    )
</file>

<file path="services/api/app/models/user_settings.py">
from __future__ import annotations

from datetime import datetime, timezone

from app.models.base import Base
from sqlalchemy import JSON, Boolean, DateTime, ForeignKey, String
from sqlalchemy.orm import Mapped, mapped_column, relationship


def utcnow() -> datetime:
    return datetime.now(timezone.utc)


class UserSettings(Base):
    __tablename__ = "user_settings"

    user_id: Mapped[str] = mapped_column(
        String(36),
        ForeignKey("users.id", ondelete="CASCADE"),
        primary_key=True,
    )

    library_system: Mapped[str | None] = mapped_column(String(200), nullable=True)

    preferred_formats: Mapped[list[str]] = mapped_column(
        JSON,
        default=list,
        nullable=False,
    )

    # NEW: per-user notifications toggle
    notifications_enabled: Mapped[bool] = mapped_column(
        Boolean,
        default=True,
        nullable=False,
    )

    updated_at: Mapped[datetime] = mapped_column(
        DateTime(timezone=True),
        default=utcnow,
        onupdate=utcnow,
        nullable=False,
    )

    user = relationship("User", back_populates="settings")
</file>

<file path="services/api/app/models/user.py">
from __future__ import annotations

from datetime import datetime, timezone
from uuid import uuid4

from app.models.base import Base
from sqlalchemy import Boolean, DateTime, String
from sqlalchemy.orm import Mapped, mapped_column, relationship


def utcnow() -> datetime:
    return datetime.now(timezone.utc)


class User(Base):
    __tablename__ = "users"

    id: Mapped[str] = mapped_column(
        String(36), primary_key=True, default=lambda: str(uuid4())
    )
    email: Mapped[str] = mapped_column(
        String(320), unique=True, index=True, nullable=False
    )
    password_hash: Mapped[str] = mapped_column(String(255), nullable=False)

    is_active: Mapped[bool] = mapped_column(Boolean, default=True, nullable=False)

    created_at: Mapped[datetime] = mapped_column(
        DateTime(timezone=True), default=utcnow, nullable=False
    )

    settings = relationship(
        "UserSettings",
        back_populates="user",
        uselist=False,
        cascade="all, delete-orphan",
    )
    shelf_sources = relationship(
        "ShelfSource", back_populates="user", cascade="all, delete-orphan"
    )
    shelf_items = relationship(
        "ShelfItem", back_populates="user", cascade="all, delete-orphan"
    )
    sync_runs = relationship(
        "SyncRun", back_populates="user", cascade="all, delete-orphan"
    )
</file>

<file path="services/api/app/schemas/settings.py">
from __future__ import annotations

from datetime import datetime

from pydantic import BaseModel


class UserSettingsOut(BaseModel):
    library_system: str | None
    preferred_formats: list[str]
    notifications_enabled: bool
    updated_at: datetime

    class Config:
        from_attributes = True


class SettingsPatchIn(BaseModel):
    library_system: str | None = None
    preferred_formats: list[str] | None = None
    notifications_enabled: bool | None = None
</file>

<file path="services/api/app/schemas/shelf.py">
from __future__ import annotations

from datetime import datetime

from pydantic import BaseModel, Field


class ImportErrorOut(BaseModel):
    key: str
    error: str


class ImportSummaryOut(BaseModel):
    created: int
    updated: int
    skipped: int
    errors: list[ImportErrorOut] = Field(default_factory=list)


class ShelfSourceOut(BaseModel):
    id: str
    source_type: str
    provider: str
    source_ref: str
    meta: dict
    is_active: bool
    last_synced_at: datetime | None
    last_sync_status: str | None
    last_sync_error: str | None

    class Config:
        from_attributes = True


class ShelfItemOut(BaseModel):
    id: str
    title: str
    author: str
    isbn10: str | None
    isbn13: str | None
    asin: str | None
    shelf: str | None
    needs_fuzzy_match: bool

    class Config:
        from_attributes = True


class RssConnectIn(BaseModel):
    rss_url: str
    shelf: str | None = "to-read"
    sync_now: bool = True


class SyncEnqueuedOut(BaseModel):
    job_id: str
</file>

<file path="services/api/app/services/goodreads_csv.py">
from __future__ import annotations

import csv
import io

from app.services.normalization import normalize_isbn


class CsvImportError(Exception):
    pass


def parse_goodreads_csv(content: bytes) -> tuple[list[dict], list[dict]]:
    """Return (rows, errors).

    Each row dict is normalized to the same shape consumed by the ingest layer.
    """
    errors: list[dict] = []

    try:
        text = content.decode("utf-8-sig")
    except UnicodeDecodeError:
        raise CsvImportError(
            "CSV must be UTF-8 encoded (Goodreads export is usually UTF-8)."
        )

    reader = csv.DictReader(io.StringIO(text))
    required = {"Title", "Author"}

    if not reader.fieldnames or not required.issubset(set(reader.fieldnames)):
        raise CsvImportError(
            "CSV is missing required columns. Expected at least: Title, Author. "
            "Tip: export from Goodreads: My Books ‚Üí Import and Export ‚Üí Export Library."
        )

    out: list[dict] = []
    for i, row in enumerate(reader, start=2):  # header is line 1
        try:
            title = (row.get("Title") or "").strip()
            author = (row.get("Author") or "").strip()
            if not title or not author:
                errors.append({"line": i, "error": "Missing Title/Author"})
                continue

            external_id = (row.get("Book Id") or "").strip() or None
            isbn10 = normalize_isbn(row.get("ISBN"))
            isbn13 = normalize_isbn(row.get("ISBN13"))
            shelf = (row.get("Exclusive Shelf") or "").strip() or None

            out.append(
                {
                    "external_id": external_id,
                    "title": title,
                    "author": author,
                    "isbn10": isbn10,
                    "isbn13": isbn13,
                    "asin": None,
                    "shelf": shelf,
                }
            )
        except Exception as e:
            errors.append({"line": i, "error": f"Unexpected row error: {e}"})

    return out, errors
</file>

<file path="services/api/app/services/goodreads_rss.py">
from __future__ import annotations

from dataclasses import dataclass
from typing import Iterable
from urllib.parse import urljoin, urlparse
from xml.etree import ElementTree as ET

import httpx
from app.core.config import settings
from app.services.normalization import normalize_isbn


@dataclass(frozen=True)
class GoodreadsRssItem:
    external_id: str | None
    title: str
    author: str
    isbn10: str | None
    isbn13: str | None
    asin: str | None
    shelf: str | None


def _local(tag: str) -> str:
    # "{namespace}name" -> "name"
    return tag.split("}", 1)[-1]


def _child_text(item: ET.Element, wanted: Iterable[str]) -> str | None:
    wanted = {w.lower() for w in wanted}
    for child in item:
        if _local(child.tag).lower() in wanted:
            if child.text:
                return child.text.strip()
    return None


def _split_title_author(title: str) -> tuple[str, str]:
    # Many feeds format title as: "Book Title by Author"
    if " by " in title:
        t, a = title.rsplit(" by ", 1)
        return t.strip(), a.strip()
    return title.strip(), ""


def parse_goodreads_rss(
    xml_text: str, default_shelf: str | None = None
) -> list[GoodreadsRssItem]:
    try:
        root = ET.fromstring(xml_text)
    except ET.ParseError as e:
        raise ValueError(f"Invalid RSS XML: {e}")

    items: list[GoodreadsRssItem] = []

    # RSS can be rss/channel/item OR feed/entry (be tolerant)
    for node in root.iter():
        if _local(node.tag).lower() in {"item", "entry"}:
            title = _child_text(node, ["book_title", "title"]) or ""
            author = _child_text(node, ["author_name", "creator", "dc:creator"]) or ""
            guid = _child_text(node, ["guid", "book_id", "id"])  # prefer stable ID

            isbn = normalize_isbn(_child_text(node, ["isbn"]))
            isbn13 = normalize_isbn(_child_text(node, ["isbn13"]))
            asin = normalize_isbn(_child_text(node, ["asin"]))

            # If title exists but author doesn't, attempt split fallback
            if title and not author:
                t2, a2 = _split_title_author(title)
                title, author = t2, (author or a2)

            if not title:
                # Skip empty items instead of crashing
                continue

            items.append(
                GoodreadsRssItem(
                    external_id=(guid.strip() if guid else None),
                    title=title.strip(),
                    author=author.strip() or "Unknown",
                    isbn10=isbn if isbn and len(isbn) <= 10 else None,
                    isbn13=(
                        isbn13
                        if isbn13
                        else (isbn if isbn and len(isbn) == 13 else None)
                    ),
                    asin=asin,
                    shelf=default_shelf,
                )
            )

    return items


def normalize_rss_input_url(rss_url_or_path: str) -> str:
    s = (rss_url_or_path or "").strip()
    if not s:
        raise ValueError("RSS URL is required")

    # Full URL
    if s.startswith("http://") or s.startswith("https://"):
        u = urlparse(s)
        if not u.scheme or not u.netloc:
            raise ValueError("Invalid RSS URL")
        return s

    # Treat as path relative to base
    base_url = settings.goodreads_base_url
    if not base_url:
        raise ValueError("GOODREADS_BASE_URL is not configured")
    base = base_url.rstrip("/") + "/"
    return urljoin(base, s.lstrip("/"))


async def fetch_rss(url: str) -> str:
    timeout = float(settings.goodreads_fetch_timeout_secs)
    async with httpx.AsyncClient(
        timeout=timeout,
        follow_redirects=True,
        headers={"User-Agent": settings.user_agent},
    ) as client:
        res = await client.get(url)
        res.raise_for_status()
        return res.text
</file>

<file path="services/api/app/services/import_service.py">
from __future__ import annotations

from dataclasses import dataclass

from app.domain.normalize import build_normalized
from app.models.shelf_item import ShelfItem
from sqlalchemy.orm import Session


@dataclass
class ImportSummary:
    created: int
    updated: int
    skipped: int
    errors: list[str]


def upsert_shelf_items(
    *,
    db: Session,
    user_id: str,
    shelf_source_id: str | None,
    items: list[dict],
    errors: list[str] | None = None,
) -> ImportSummary:
    errors_out = list(errors or [])

    def _external_id(item: dict) -> str | None:
        raw = item.get("external_id") or item.get("goodreads_book_id")
        if raw is None:
            return None
        value = str(raw).strip()
        return value or None

    ext_ids = [ext for ext in (_external_id(it) for it in items) if ext]
    existing_by_ext: dict[str, ShelfItem] = {}
    if ext_ids:
        query = db.query(ShelfItem).filter(ShelfItem.user_id == user_id)
        if shelf_source_id is None:
            query = query.filter(ShelfItem.shelf_source_id.is_(None))
        else:
            query = query.filter(ShelfItem.shelf_source_id == shelf_source_id)
        existing = query.filter(ShelfItem.external_id.in_(ext_ids)).all()
        existing_by_ext = {it.external_id: it for it in existing if it.external_id}

    created = updated = skipped = 0

    for it in items:
        title = (it.get("title") or "").strip()
        author = (it.get("author") or "").strip()

        if not title or not author:
            skipped += 1
            errors_out.append("Skipped item missing title/author")
            continue

        isbn13 = it.get("isbn13")
        isbn10 = it.get("isbn10")

        # RSS provides a single "isbn" sometimes; treat as isbn10/13 raw.
        if not isbn13 and not isbn10 and it.get("isbn"):
            raw_isbn = it.get("isbn")
            if raw_isbn and len(str(raw_isbn).strip()) >= 10:
                # Best-effort: put into isbn13 if it looks 13 digits, else isbn10.
                s = str(raw_isbn).strip()
                if len("".join([c for c in s if c.isdigit()])) >= 13:
                    isbn13 = s
                else:
                    isbn10 = s

        asin = it.get("asin")
        external_id = _external_id(it)

        norm = build_normalized(
            title=title,
            author=author,
            isbn13=isbn13,
            isbn10=isbn10,
            asin=asin,
        )

        existing_item = existing_by_ext.get(external_id) if external_id else None
        if existing_item:
            # Update key fields (keep this conservative).
            existing_item.title = title
            existing_item.author = author
            existing_item.isbn13 = norm.isbn13
            existing_item.isbn10 = norm.isbn10
            existing_item.asin = norm.asin
            existing_item.normalized_title = norm.normalized_title
            existing_item.normalized_author = norm.normalized_author
            existing_item.needs_fuzzy_match = norm.needs_fuzzy_match
            existing_item.shelf_source_id = shelf_source_id
            existing_item.external_id = external_id
            existing_item.shelf = it.get("shelf")

            updated += 1
            continue

        new_item = ShelfItem(
            user_id=user_id,
            shelf_source_id=shelf_source_id,
            external_id=external_id,
            title=title,
            author=author,
            isbn13=norm.isbn13,
            isbn10=norm.isbn10,
            asin=norm.asin,
            normalized_title=norm.normalized_title,
            normalized_author=norm.normalized_author,
            shelf=it.get("shelf"),
            needs_fuzzy_match=norm.needs_fuzzy_match,
        )
        db.add(new_item)
        if external_id:
            existing_by_ext[external_id] = new_item
        created += 1

    return ImportSummary(
        created=created, updated=updated, skipped=skipped, errors=errors_out
    )
</file>

<file path="services/api/.env.example">
ENV=local
API_NAME=ShelfSyncAPI

DATABASE_URL=postgresql+psycopg2://shelfsync:shelfsync@localhost:5432/shelfsync
REDIS_URL=redis://localhost:6379/0
OTEL_ENABLED=false

GOODREADS_BASE_URL=http://localhost:4010

# Demo toggles (reserved for local/dev flows)
DEMO_LOGIN_ENABLED=true
DEMO_SEED_ENABLED=false

# pydantic-settings can parse JSON lists; keep it simple in dev
CORS_ORIGINS=["http://localhost:3000"]

AUTH_SECRET_KEY=CHANGE_ME_IN_PROD
AUTH_ACCESS_TOKEN_TTL_MINUTES=10080
AUTH_COOKIE_NAME=access_token
AUTH_COOKIE_SECURE=false
AUTH_COOKIE_SAMESITE=lax
GOODREADS_FETCH_TIMEOUT_SECS=10
USER_AGENT=ShelfSync/0.1

CATALOG_PROVIDER=fixture
FIXTURE_CATALOG_PATH=app/fixtures/catalog_fixture.json
AVAILABILITY_CACHE_TTL_SECS=300
RATE_LIMIT_WINDOW_SECS=60
RATE_LIMIT_DASHBOARD_PER_WINDOW=30
RATE_LIMIT_BOOKS_PER_WINDOW=60
</file>

<file path="Makefile">
.PHONY: infra-up infra-down infra-up-full infra-down-full api-test web-check dev api-alembic

infra-up:
	docker compose -f infra/docker-compose.yml up -d

infra-down:
	docker compose -f infra/docker-compose.yml down

infra-up-full:
	docker compose -f infra/docker-compose.full.yml up -d --build

infra-down-full:
	docker compose -f infra/docker-compose.full.yml down

api-test:
	cd services/api && . .venv/bin/activate && black . && isort . && pytest

web-check:
	cd apps/web && npm run lint && npm run build

dev:
	./scripts/dev.sh

api-alembic:
	cd services/api && ./bin/alembic
</file>

<file path="services/api/alembic/versions/b2c2190162bb_phase3_catalog_matching.py">
"""phase3 catalog matching

Revision ID: b2c2190162bb
Revises: c7a5b2b4af1e
Create Date: 2025-12-23 19:37:31.748233

"""

from typing import Sequence, Union

import sqlalchemy as sa
from alembic import op
from sqlalchemy.dialects import postgresql

# revision identifiers, used by Alembic.
revision: str = "b2c2190162bb"
down_revision: Union[str, Sequence[str], None] = "c7a5b2b4af1e"
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    """Upgrade schema."""
    # ### commands auto generated by Alembic - please adjust! ###
    op.create_table(
        "catalog_items",
        sa.Column("id", sa.String(length=36), nullable=False),
        sa.Column("provider", sa.String(length=40), nullable=False),
        sa.Column("provider_item_id", sa.String(length=160), nullable=False),
        sa.Column("title", sa.String(length=400), nullable=False),
        sa.Column("author", sa.String(length=240), nullable=True),
        sa.Column("isbn10", sa.String(length=10), nullable=True),
        sa.Column("isbn13", sa.String(length=13), nullable=True),
        sa.Column("asin", sa.String(length=20), nullable=True),
        sa.Column("raw", sa.JSON(), nullable=False),
        sa.Column("created_at", sa.DateTime(timezone=True), nullable=False),
        sa.Column("updated_at", sa.DateTime(timezone=True), nullable=False),
        sa.PrimaryKeyConstraint("id"),
        sa.UniqueConstraint(
            "provider", "provider_item_id", name="uq_catalog_items_provider_item"
        ),
    )
    op.create_table(
        "availability_snapshots",
        sa.Column("id", sa.String(length=36), nullable=False),
        sa.Column("user_id", sa.String(length=36), nullable=False),
        sa.Column("catalog_item_id", sa.String(length=36), nullable=False),
        sa.Column("format", sa.String(length=20), nullable=False),
        sa.Column("status", sa.String(length=20), nullable=False),
        sa.Column("copies_available", sa.Integer(), nullable=True),
        sa.Column("copies_total", sa.Integer(), nullable=True),
        sa.Column("holds", sa.Integer(), nullable=True),
        sa.Column("deep_link", sa.String(length=500), nullable=True),
        sa.Column("last_checked_at", sa.DateTime(timezone=True), nullable=False),
        sa.ForeignKeyConstraint(
            ["catalog_item_id"], ["catalog_items.id"], ondelete="CASCADE"
        ),
        sa.ForeignKeyConstraint(["user_id"], ["users.id"], ondelete="CASCADE"),
        sa.PrimaryKeyConstraint("id"),
        sa.UniqueConstraint(
            "user_id", "catalog_item_id", "format", name="uq_avail_user_item_format"
        ),
    )
    op.create_index(
        op.f("ix_availability_snapshots_catalog_item_id"),
        "availability_snapshots",
        ["catalog_item_id"],
        unique=False,
    )
    op.create_index(
        op.f("ix_availability_snapshots_user_id"),
        "availability_snapshots",
        ["user_id"],
        unique=False,
    )
    op.create_table(
        "catalog_matches",
        sa.Column("id", sa.String(length=36), nullable=False),
        sa.Column("user_id", sa.String(length=36), nullable=False),
        sa.Column("shelf_item_id", sa.String(length=36), nullable=False),
        sa.Column("catalog_item_id", sa.String(length=36), nullable=False),
        sa.Column("provider", sa.String(length=40), nullable=False),
        sa.Column("method", sa.String(length=40), nullable=False),
        sa.Column("confidence", sa.Float(), nullable=False),
        sa.Column("evidence", sa.JSON(), nullable=False),
        sa.Column("created_at", sa.DateTime(timezone=True), nullable=False),
        sa.Column("updated_at", sa.DateTime(timezone=True), nullable=False),
        sa.ForeignKeyConstraint(
            ["catalog_item_id"], ["catalog_items.id"], ondelete="CASCADE"
        ),
        sa.ForeignKeyConstraint(
            ["shelf_item_id"], ["shelf_items.id"], ondelete="CASCADE"
        ),
        sa.ForeignKeyConstraint(["user_id"], ["users.id"], ondelete="CASCADE"),
        sa.PrimaryKeyConstraint("id"),
        sa.UniqueConstraint(
            "user_id", "shelf_item_id", name="uq_catalog_match_user_shelf_item"
        ),
    )
    op.create_index(
        op.f("ix_catalog_matches_catalog_item_id"),
        "catalog_matches",
        ["catalog_item_id"],
        unique=False,
    )
    op.create_index(
        op.f("ix_catalog_matches_shelf_item_id"),
        "catalog_matches",
        ["shelf_item_id"],
        unique=False,
    )
    op.create_index(
        op.f("ix_catalog_matches_user_id"), "catalog_matches", ["user_id"], unique=False
    )
    with op.batch_alter_table("shelf_items") as batch_op:
        batch_op.alter_column(
            "isbn10",
            existing_type=sa.VARCHAR(length=10),
            type_=sa.String(length=20),
            existing_nullable=True,
        )
        batch_op.alter_column(
            "isbn13",
            existing_type=sa.VARCHAR(length=13),
            type_=sa.String(length=20),
            existing_nullable=True,
        )
        batch_op.drop_index(op.f("ix_shelf_items_user_normkey"))
        batch_op.drop_constraint(op.f("uq_shelf_item_user_normkey"), type_="unique")
        batch_op.create_index(
            "ix_shelf_items_source_external_unique",
            ["shelf_source_id", "external_id"],
            unique=True,
            postgresql_where=sa.text("external_id IS NOT NULL"),
        )
        batch_op.drop_column("normalized_key")
        batch_op.drop_column("goodreads_book_id")

    with op.batch_alter_table("shelf_sources") as batch_op:
        batch_op.drop_constraint(op.f("uq_shelf_source_user_type_ref"), type_="unique")
        batch_op.drop_column("shelf_name")
        batch_op.drop_column("last_imported_at")
    # ### end Alembic commands ###


def downgrade() -> None:
    """Downgrade schema."""
    # ### commands auto generated by Alembic - please adjust! ###
    op.add_column(
        "shelf_sources",
        sa.Column(
            "last_imported_at",
            postgresql.TIMESTAMP(timezone=True),
            autoincrement=False,
            nullable=True,
        ),
    )
    op.add_column(
        "shelf_sources",
        sa.Column(
            "shelf_name", sa.VARCHAR(length=200), autoincrement=False, nullable=True
        ),
    )
    op.create_unique_constraint(
        op.f("uq_shelf_source_user_type_ref"),
        "shelf_sources",
        ["user_id", "source_type", "source_ref"],
        postgresql_nulls_not_distinct=False,
    )
    op.add_column(
        "shelf_items",
        sa.Column(
            "goodreads_book_id",
            sa.VARCHAR(length=40),
            autoincrement=False,
            nullable=True,
        ),
    )
    op.add_column(
        "shelf_items",
        sa.Column(
            "normalized_key",
            sa.VARCHAR(length=1100),
            autoincrement=False,
            nullable=False,
        ),
    )
    op.drop_index(
        "ix_shelf_items_source_external_unique",
        table_name="shelf_items",
        postgresql_where=sa.text("external_id IS NOT NULL"),
    )
    op.create_unique_constraint(
        op.f("uq_shelf_item_user_normkey"),
        "shelf_items",
        ["user_id", "normalized_key"],
        postgresql_nulls_not_distinct=False,
    )
    op.create_index(
        op.f("ix_shelf_items_user_normkey"),
        "shelf_items",
        ["user_id", "normalized_key"],
        unique=False,
    )
    op.alter_column(
        "shelf_items",
        "isbn13",
        existing_type=sa.String(length=20),
        type_=sa.VARCHAR(length=13),
        existing_nullable=True,
    )
    op.alter_column(
        "shelf_items",
        "isbn10",
        existing_type=sa.String(length=20),
        type_=sa.VARCHAR(length=10),
        existing_nullable=True,
    )
    op.drop_index(op.f("ix_catalog_matches_user_id"), table_name="catalog_matches")
    op.drop_index(
        op.f("ix_catalog_matches_shelf_item_id"), table_name="catalog_matches"
    )
    op.drop_index(
        op.f("ix_catalog_matches_catalog_item_id"), table_name="catalog_matches"
    )
    op.drop_table("catalog_matches")
    op.drop_index(
        op.f("ix_availability_snapshots_user_id"), table_name="availability_snapshots"
    )
    op.drop_index(
        op.f("ix_availability_snapshots_catalog_item_id"),
        table_name="availability_snapshots",
    )
    op.drop_table("availability_snapshots")
    op.drop_table("catalog_items")
    # ### end Alembic commands ###
</file>

<file path="services/api/app/crud/availability.py">
from __future__ import annotations

from dataclasses import dataclass
from datetime import datetime, timezone
from typing import Iterable
from uuid import uuid4

from app.models.availability_snapshot import AvailabilitySnapshot
from app.models.catalog_match import CatalogMatch
from app.models.notification_event import NotificationEvent
from app.models.user_settings import UserSettings
from app.providers.types import AvailabilityResult
from sqlalchemy import select
from sqlalchemy.orm import Session


def utcnow() -> datetime:
    return datetime.now(timezone.utc)


@dataclass(frozen=True)
class NotificationCreated:
    id: str
    shelf_item_id: str
    title: str
    format: str


def upsert_snapshots(
    db: Session, *, user_id: str, results: Iterable[AvailabilityResult]
) -> list[NotificationCreated]:
    """Persist availability snapshots and create notifications when items become available.

    Notes:
    - Notification creation is durable (DB insert).
    - Real-time delivery (Redis/SSE) is best-effort and happens after commit.
    """

    # Read settings once per chunk
    s = db.get(UserSettings, user_id)
    notifications_on = bool(getattr(s, "notifications_enabled", True)) if s else True

    results_list = list(results)
    if not results_list:
        return []

    # Map catalog_item_id -> shelf_item_id for this user
    catalog_ids = {r.catalog_item_id for r in results_list}
    match_rows = db.execute(
        select(CatalogMatch.catalog_item_id, CatalogMatch.shelf_item_id).where(
            CatalogMatch.user_id == user_id,
            CatalogMatch.catalog_item_id.in_(list(catalog_ids)),
        )
    ).all()
    catalog_to_shelf = {cid: sid for cid, sid in match_rows}

    # Preload existing snapshots for quick compare
    existing_rows = (
        db.execute(
            select(AvailabilitySnapshot).where(
                AvailabilitySnapshot.user_id == user_id,
                AvailabilitySnapshot.catalog_item_id.in_(list(catalog_ids)),
            )
        )
        .scalars()
        .all()
    )
    existing_by_key: dict[tuple[str, str], AvailabilitySnapshot] = {
        (row.catalog_item_id, row.format): row for row in existing_rows
    }

    created: list[NotificationCreated] = []
    now = utcnow()

    for r in results_list:
        a = r.availability
        key = (r.catalog_item_id, a.format.value)

        prev = existing_by_key.get(key)
        old_status = prev.status if prev is not None else "unknown"
        new_status = a.status.value

        # Upsert snapshot
        if prev is not None:
            prev.status = new_status
            prev.copies_available = a.copies_available
            prev.copies_total = a.copies_total
            prev.holds = a.holds
            prev.deep_link = a.deep_link
            prev.last_checked_at = now
        else:
            row = AvailabilitySnapshot(
                id=str(uuid4()),
                user_id=user_id,
                catalog_item_id=r.catalog_item_id,
                format=a.format.value,
                status=new_status,
                copies_available=a.copies_available,
                copies_total=a.copies_total,
                holds=a.holds,
                deep_link=a.deep_link,
                last_checked_at=now,
            )
            db.add(row)
            existing_by_key[key] = row

        # Emit notification on transition -> available
        if (
            notifications_on
            and old_status in {"hold", "not_owned"}
            and new_status == "available"
        ):
            shelf_item_id = catalog_to_shelf.get(r.catalog_item_id)
            if shelf_item_id:
                ev = NotificationEvent(
                    id=str(uuid4()),
                    user_id=user_id,
                    shelf_item_id=shelf_item_id,
                    format=a.format.value,
                    old_status=old_status,
                    new_status=new_status,
                    deep_link=a.deep_link,
                    created_at=now,
                )
                db.add(ev)

                created.append(
                    NotificationCreated(
                        id=ev.id,
                        shelf_item_id=shelf_item_id,
                        title="",  # hydrated later (optional)
                        format=a.format.value,
                    )
                )

    return created
</file>

<file path="services/api/app/models/shelf_item.py">
from __future__ import annotations

from datetime import datetime, timezone
from uuid import uuid4

from app.models.base import Base
from sqlalchemy import Boolean, DateTime, ForeignKey, Index, String
from sqlalchemy.orm import Mapped, mapped_column, relationship


def utcnow() -> datetime:
    return datetime.now(timezone.utc)


class ShelfItem(Base):
    __tablename__ = "shelf_items"

    id: Mapped[str] = mapped_column(
        String(36), primary_key=True, default=lambda: str(uuid4())
    )

    user_id: Mapped[str] = mapped_column(
        String(36),
        ForeignKey("users.id", ondelete="CASCADE"),
        index=True,
        nullable=False,
    )

    shelf_source_id: Mapped[str | None] = mapped_column(
        String(36),
        ForeignKey("shelf_sources.id", ondelete="SET NULL"),
        index=True,
        nullable=True,
    )

    # RSS: guid/book_id; CSV: "Book Id" (stringified)
    external_id: Mapped[str | None] = mapped_column(String(120), nullable=True)

    title: Mapped[str] = mapped_column(String(600), nullable=False)
    author: Mapped[str] = mapped_column(String(400), nullable=False)

    isbn10: Mapped[str | None] = mapped_column(String(20), nullable=True)
    isbn13: Mapped[str | None] = mapped_column(String(20), nullable=True)
    asin: Mapped[str | None] = mapped_column(String(20), nullable=True)

    normalized_title: Mapped[str] = mapped_column(String(600), nullable=False)
    normalized_author: Mapped[str] = mapped_column(String(400), nullable=False)

    # A single primary shelf for now (e.g. to-read/read/currently-reading)
    shelf: Mapped[str | None] = mapped_column(String(80), nullable=True)

    needs_fuzzy_match: Mapped[bool] = mapped_column(
        Boolean, nullable=False, default=False
    )

    created_at: Mapped[datetime] = mapped_column(
        DateTime(timezone=True), default=utcnow, nullable=False
    )
    updated_at: Mapped[datetime] = mapped_column(
        DateTime(timezone=True), default=utcnow, onupdate=utcnow, nullable=False
    )

    user = relationship("User", back_populates="shelf_items")
    shelf_source = relationship("ShelfSource", back_populates="items")


# Idempotency: if an external_id exists, it must be unique per source.
# NOTE: Items without external_id can still duplicate; Phase 3 can improve with additional keys.
Index(
    "ix_shelf_items_source_external_unique",
    ShelfItem.shelf_source_id,
    ShelfItem.external_id,
    unique=True,
    postgresql_where=(ShelfItem.external_id.isnot(None)),
)
</file>

<file path="services/api/app/models/shelf_source.py">
from __future__ import annotations

from datetime import datetime, timezone
from uuid import uuid4

from app.models.base import Base
from sqlalchemy import JSON, Boolean, DateTime, ForeignKey, String
from sqlalchemy.orm import Mapped, mapped_column, relationship


def utcnow() -> datetime:
    return datetime.now(timezone.utc)


class ShelfSource(Base):
    __tablename__ = "shelf_sources"

    id: Mapped[str] = mapped_column(
        String(36), primary_key=True, default=lambda: str(uuid4())
    )
    user_id: Mapped[str] = mapped_column(
        String(36),
        ForeignKey("users.id", ondelete="CASCADE"),
        index=True,
        nullable=False,
    )

    # "rss" | "csv"
    source_type: Mapped[str] = mapped_column(String(20), nullable=False)

    # For now: only "goodreads" (later phases can add "libby" etc)
    provider: Mapped[str] = mapped_column(
        String(40), nullable=False, default="goodreads"
    )

    # RSS URL or a logical identifier for CSV imports
    source_ref: Mapped[str] = mapped_column(String(2000), nullable=False)

    # Arbitrary metadata, e.g. {"shelf": "to-read"}
    meta: Mapped[dict] = mapped_column(JSON, nullable=False, default=dict)

    is_active: Mapped[bool] = mapped_column(Boolean, nullable=False, default=True)

    # Sync metadata
    last_synced_at: Mapped[datetime | None] = mapped_column(
        DateTime(timezone=True), nullable=True
    )
    last_sync_status: Mapped[str | None] = mapped_column(
        String(30), nullable=True
    )  # "ok" | "error"
    last_sync_error: Mapped[str | None] = mapped_column(String(2000), nullable=True)

    created_at: Mapped[datetime] = mapped_column(
        DateTime(timezone=True), default=utcnow, nullable=False
    )
    updated_at: Mapped[datetime] = mapped_column(
        DateTime(timezone=True), default=utcnow, onupdate=utcnow, nullable=False
    )

    user = relationship("User", back_populates="shelf_sources")
    items = relationship(
        "ShelfItem", back_populates="shelf_source", cascade="all, delete-orphan"
    )
</file>

<file path="services/api/app/workers/events.py">
from __future__ import annotations

import json
from datetime import datetime
from typing import Any

from app.core.config import settings
from app.core.redis import get_redis


def publish_sync_event(
    *, user_id: str, run_id: str, type_: str, payload: dict[str, Any]
):
    r = get_redis(settings.redis_url)
    channel = f"sync:{user_id}:{run_id}"
    body = {
        "type": type_,
        "payload": payload,
        "ts": datetime.utcnow().isoformat() + "Z",
    }
    r.publish(channel, json.dumps(body))


def publish_notification_event(*, user_id: str, payload: dict[str, Any]) -> None:
    r = get_redis(settings.redis_url)
    channel = f"notify:{user_id}"
    body = {
        "type": "notification",
        "payload": payload,
        "ts": datetime.utcnow().isoformat() + "Z",
    }
    r.publish(channel, json.dumps(body))
</file>

<file path="services/api/app/workers/queue.py">
from __future__ import annotations

from app.core.config import settings
from app.workers.redis_conn import get_redis_connection
from rq import Queue, Retry
from rq.job import Job


def get_queue() -> Queue:
    conn = get_redis_connection()
    return Queue("default", connection=conn)


def enqueue_availability_refresh(*, sync_run_id: str) -> Job:
    q = get_queue()
    return q.enqueue(
        "app.workers.jobs.availability_refresh_job",
        sync_run_id,
        retry=Retry(max=2, interval=[5, 15]),
        job_timeout=settings.worker_job_timeout_secs,
    )
</file>

<file path="services/api/tests/test_auth.py">
def test_signup_sets_cookie_and_returns_user(client):
    resp = client.post(
        "/v1/auth/signup", json={"email": "a@example.com", "password": "password123"}
    )
    assert resp.status_code == 200
    body = resp.json()
    assert body["email"] == "a@example.com"
    assert "set-cookie" in resp.headers


def test_login_and_me(client):
    client.post(
        "/v1/auth/signup", json={"email": "b@example.com", "password": "password123"}
    )

    # logout to clear cookie
    client.post("/v1/auth/logout")

    resp = client.post(
        "/v1/auth/login", json={"email": "b@example.com", "password": "password123"}
    )
    assert resp.status_code == 200

    me = client.get("/v1/auth/me")
    assert me.status_code == 200
    assert me.json()["email"] == "b@example.com"


def test_me_requires_auth(client):
    resp = client.get("/v1/auth/me")
    assert resp.status_code in (401, 403)


def test_login_rejects_bad_password(client):
    client.post(
        "/v1/auth/signup", json={"email": "c@example.com", "password": "password123"}
    )
    resp = client.post(
        "/v1/auth/login", json={"email": "c@example.com", "password": "wrongwrong"}
    )
    assert resp.status_code in (401, 403)


def test_login_accepts_legacy_pbkdf2_hash(client, db_session):
    from app.models.user import User
    from passlib.context import CryptContext

    legacy_context = CryptContext(schemes=["pbkdf2_sha256"], deprecated="auto")
    db_session.add(
        User(
            email="legacy@example.com", password_hash=legacy_context.hash("password123")
        )
    )
    db_session.flush()

    resp = client.post(
        "/v1/auth/login",
        json={"email": "legacy@example.com", "password": "password123"},
    )
    assert resp.status_code == 200


def test_login_auto_creates_demo_user(client):
    resp = client.post(
        "/v1/auth/login", json={"email": "demo@example.com", "password": "password123"}
    )
    assert resp.status_code == 200
    assert resp.json()["email"] == "demo@example.com"


def test_login_auto_creates_demo_user_with_simple_password(client):
    resp = client.post(
        "/v1/auth/login", json={"email": "demo@example.com", "password": "password"}
    )
    assert resp.status_code == 200
    assert resp.json()["email"] == "demo@example.com"
</file>

<file path="services/api/requirements-dev.txt">
-r requirements.txt
black==24.8.0
isort==5.13.2
mypy==1.11.2
pytest==8.3.2
pytest-asyncio==0.24.0
pytest-cov==5.0.0
ruff==0.6.2
types-python-jose
APScheduler==3.11.2
</file>

<file path="pyproject.toml">
[tool.pytest.ini_options]
addopts = "-q"
asyncio_mode = "auto"
asyncio_default_fixture_loop_scope = "function"
pythonpath = ["services/api"]

[tool.black]
line-length = 88

[tool.isort]
profile = "black"
</file>

<file path="apps/web/src/app/dashboard/page.tsx">
"use client";

import Link from "next/link";
import { useEffect, useMemo, useState } from "react";

import { AuthGuard } from "@/components/AuthGuard";
import { NotificationBell } from "@/components/NotificationBell";
import { apiFetch } from "@/lib/api";
import { compareReadNext, readNextTooltip, type ReadNext } from "@/lib/readNext";

type MatchMini = {
  catalog_item_id: string;
  provider: string;
  provider_item_id: string;
  method: string;
  confidence: number;
};

type Availability = {
  format: string;
  status: "available" | "hold" | "not_owned";
  copies_available: number | null;
  copies_total: number | null;
  holds: number | null;
  deep_link: string | null;
  last_checked_at: string;
};

type DashboardRow = {
  shelf_item_id: string;
  title: string;
  author: string | null;
  shelf: string | null;
  needs_fuzzy_match: boolean;
  match: MatchMini | null;
  availability: Availability[];
  read_next: ReadNext;
};

type DashboardResponse = {
  settings: {
    library_system: string | null;
    preferred_formats: string[];
    updated_at: string;
  };
  last_sync: {
    source_type: string | null;
    source_id: string | null;
    last_synced_at: string | null;
    last_sync_status: string | null;
    last_sync_error: string | null;
  };
  page: {
    limit: number;
    offset: number;
    total: number;
  };
  items: DashboardRow[];
};

type SortKey = "read_next" | "availability" | "title";
type FilterKey = "all" | "available" | "hold" | "not_owned";

const DEFAULT_SORT: SortKey = "read_next";
const DEFAULT_FILTER: FilterKey = "all";

function availabilityStatus(row: DashboardRow): FilterKey {
  if (!row.match) return "not_owned";
  if (row.availability.some((a) => a.status === "available")) return "available";
  if (row.availability.some((a) => a.status === "hold")) return "hold";
  return "not_owned";
}

function availabilityRank(row: DashboardRow): number {
  const s = availabilityStatus(row);
  if (s === "available") return 3;
  if (s === "hold") return 2;
  if (s === "not_owned") return 1;
  return 0;
}

export default function DashboardPage() {
  return (
    <AuthGuard>
      <DashboardInner />
    </AuthGuard>
  );
}

function DashboardInner() {
  const [data, setData] = useState<DashboardResponse | null>(null);
  const [error, setError] = useState<string | null>(null);

  useEffect(() => {
    let alive = true;

    (async () => {
      try {
        const res = await apiFetch<DashboardResponse>(`/v1/dashboard?limit=200&offset=0&sort=read_next`);
        if (!alive) return;
        setData(res);
      } catch (e) {
        if (!alive) return;
        setError(e instanceof Error ? e.message : String(e));
      }
    })();

    return () => {
      alive = false;
    };
  }, []);

  const filtered = useMemo(() => {
    if (!data) return [];

    const rows = [...data.items];

    const sorted = rows.sort((a, b) => {
      if (DEFAULT_SORT === "read_next") return compareReadNext(a, b);
      if (DEFAULT_SORT === "availability") {
        const ra = availabilityRank(a);
        const rb = availabilityRank(b);
        if (ra !== rb) return rb - ra;
        return a.title.localeCompare(b.title);
      }
      return a.title.localeCompare(b.title);
    });

    return sorted.filter((r) => {
      if (DEFAULT_FILTER === "all") return true;
      return availabilityStatus(r) === DEFAULT_FILTER;
    });
  }, [data]);

  return (
    <main className="p-6 max-w-6xl mx-auto">
      <div className="flex items-start justify-between gap-4">
        <div>
          <h1 className="text-2xl font-semibold">Dashboard</h1>
          <p className="text-sm text-gray-600">
            Sorted by <span className="font-medium">Read Next</span>. This ranking prioritizes items available now, then
            shorter hold queues, using your format preferences.
          </p>
          <p className="text-xs text-gray-500 mt-1">Tip: hover ‚ìò to see the explanation (tier, format, queue details).</p>
        </div>

        <NotificationBell />

        <Link href="/settings" className="px-3 py-2 rounded border text-sm hover:bg-gray-50">
          Settings
        </Link>
      </div>

      {error ? (
        <div className="mt-4 rounded border border-red-200 bg-red-50 p-3 text-sm">
          <div className="font-medium">Couldn‚Äôt load your dashboard.</div>
          <div className="mt-1 text-gray-700">{error}</div>
        </div>
      ) : null}

      {!data && !error ? <div className="mt-4 rounded border p-4 text-sm text-gray-600">Loading‚Ä¶</div> : null}

      {data ? (
        <div className="mt-4 rounded border overflow-hidden">
          <table className="w-full text-sm">
            <thead className="bg-gray-50 text-gray-700">
              <tr>
                <th className="text-left p-3">Title</th>
                <th className="text-left p-3">Shelf</th>
                <th className="text-left p-3">Availability</th>
                <th className="text-left p-3">Read Next</th>
              </tr>
            </thead>
            <tbody>
              {filtered.map((row) => {
                const status = availabilityStatus(row);
                const rnTitle = readNextTooltip(row.read_next);
                return (
                  <tr key={row.shelf_item_id} className="border-t">
                    <td className="p-3">
                      <div className="flex items-center gap-2">
                        <Link href={`/books/${row.shelf_item_id}`} className="font-medium hover:underline">
                          {row.title}
                        </Link>
                        <span className="text-xs text-gray-500 cursor-help" title={rnTitle} aria-label="Read Next explanation">
                          ‚ìò
                        </span>
                      </div>
                      {row.author ? <div className="text-gray-600">{row.author}</div> : null}
                      {row.needs_fuzzy_match ? <div className="text-xs text-amber-700 mt-1">Needs fuzzy match</div> : null}
                    </td>

                    <td className="p-3">{row.shelf ?? "‚Äî"}</td>

                    <td className="p-3">
                      <div className="capitalize">{status.replace("_", " ")}</div>
                      {row.availability.length ? (
                        <div className="text-xs text-gray-600 mt-1">
                          {row.availability
                            .map((a) => {
                              const bits = [a.format, a.status];
                              if (a.status === "available" && a.copies_available != null) bits.push(`${a.copies_available} available`);
                              if (a.status === "hold" && a.holds != null) bits.push(`${a.holds} holds`);
                              return bits.join(" ‚Ä¢ ");
                            })
                            .join(" | ")}
                        </div>
                      ) : null}
                    </td>

                    <td className="p-3">
                      <div className="capitalize">{row.read_next.tier.replace("_", " ")}</div>
                      <div className="text-xs text-gray-600 mt-1">Score: {row.read_next.score.toFixed(1)}</div>
                      {row.read_next.best_format ? <div className="text-xs text-gray-600">Best: {row.read_next.best_format}</div> : null}
                    </td>
                  </tr>
                );
              })}
            </tbody>
          </table>
        </div>
      ) : null}

      <div className="mt-6 text-xs text-gray-600">
        <p>
          <span className="font-medium">How Read Next works:</span> Available now ‚Üí Holds ‚Üí Not owned. Within a tier, your
          preferred format order matters, and hold queue size breaks ties.
        </p>
      </div>
    </main>
  );
}
</file>

<file path="services/api/alembic/versions/8f008b42145c_phase2_ingestion_fields.py">
"""phase2 ingestion fields

Revision ID: 8f008b42145c
Revises: 8399dceac96e
Create Date: 2025-12-23 10:43:34.266235

"""

from typing import Sequence, Union

import sqlalchemy as sa
from alembic import op

# revision identifiers, used by Alembic.
revision: str = "8f008b42145c"
down_revision: Union[str, Sequence[str], None] = "8399dceac96e"
branch_labels: Union[str, Sequence[str], None] = None
depends_on: Union[str, Sequence[str], None] = None


def upgrade() -> None:
    """Upgrade schema."""
    # ### commands auto generated by Alembic - please adjust! ###
    with op.batch_alter_table("shelf_items") as batch_op:
        batch_op.add_column(sa.Column("isbn13", sa.String(length=13), nullable=True))
        batch_op.add_column(sa.Column("isbn10", sa.String(length=10), nullable=True))
        batch_op.add_column(sa.Column("asin", sa.String(length=20), nullable=True))
        batch_op.add_column(
            sa.Column("goodreads_book_id", sa.String(length=40), nullable=True)
        )
        batch_op.add_column(
            sa.Column("normalized_title", sa.String(length=600), nullable=False)
        )
        batch_op.add_column(
            sa.Column("normalized_author", sa.String(length=400), nullable=False)
        )
        batch_op.add_column(
            sa.Column("normalized_key", sa.String(length=1100), nullable=False)
        )
        batch_op.add_column(
            sa.Column("needs_fuzzy_match", sa.Boolean(), nullable=False)
        )
        batch_op.add_column(
            sa.Column("updated_at", sa.DateTime(timezone=True), nullable=False)
        )
        batch_op.drop_index(op.f("ix_shelf_items_user_title_author"))
        batch_op.create_index(
            "ix_shelf_items_user_normkey",
            ["user_id", "normalized_key"],
            unique=False,
        )
        batch_op.create_unique_constraint(
            "uq_shelf_item_user_normkey", ["user_id", "normalized_key"]
        )

    with op.batch_alter_table("shelf_sources") as batch_op:
        batch_op.add_column(
            sa.Column("shelf_name", sa.String(length=200), nullable=True)
        )
        batch_op.add_column(sa.Column("is_active", sa.Boolean(), nullable=False))
        batch_op.add_column(
            sa.Column("last_imported_at", sa.DateTime(timezone=True), nullable=True)
        )
        batch_op.create_unique_constraint(
            "uq_shelf_source_user_type_ref",
            ["user_id", "source_type", "source_ref"],
        )
    # ### end Alembic commands ###


def downgrade() -> None:
    """Downgrade schema."""
    # ### commands auto generated by Alembic - please adjust! ###
    with op.batch_alter_table("shelf_sources") as batch_op:
        batch_op.drop_constraint("uq_shelf_source_user_type_ref", type_="unique")
        batch_op.drop_column("last_imported_at")
        batch_op.drop_column("is_active")
        batch_op.drop_column("shelf_name")

    with op.batch_alter_table("shelf_items") as batch_op:
        batch_op.drop_constraint("uq_shelf_item_user_normkey", type_="unique")
        batch_op.drop_index("ix_shelf_items_user_normkey")
        batch_op.create_index(
            op.f("ix_shelf_items_user_title_author"),
            ["user_id", "title", "author"],
            unique=False,
        )
        batch_op.drop_column("updated_at")
        batch_op.drop_column("needs_fuzzy_match")
        batch_op.drop_column("normalized_key")
        batch_op.drop_column("normalized_author")
        batch_op.drop_column("normalized_title")
        batch_op.drop_column("goodreads_book_id")
        batch_op.drop_column("asin")
        batch_op.drop_column("isbn10")
        batch_op.drop_column("isbn13")
    # ### end Alembic commands ###
</file>

<file path="services/api/app/models/__init__.py">
from app.models.availability_snapshot import AvailabilitySnapshot
from app.models.base import Base
from app.models.catalog_item import CatalogItem
from app.models.catalog_match import CatalogMatch
from app.models.library import Library
from app.models.notification_event import NotificationEvent
from app.models.shelf_item import ShelfItem
from app.models.shelf_source import ShelfSource
from app.models.sync_run import SyncRun
from app.models.user import User
from app.models.user_settings import UserSettings

__all__ = [
    "Base",
    "User",
    "UserSettings",
    "ShelfSource",
    "ShelfItem",
    "Library",
    "CatalogItem",
    "CatalogMatch",
    "AvailabilitySnapshot",
    "SyncRun",
    "NotificationEvent",
]
</file>

<file path="services/api/requirements.txt">
fastapi
uvicorn[standard]
httpx
requests
beautifulsoup4
selectolax
playwright
selenium
python-dotenv
pydantic

sqlalchemy
alembic
psycopg2-binary
redis
rq
celery
apscheduler
passlib[bcrypt]
bcrypt<4

opentelemetry-api
opentelemetry-sdk
opentelemetry-instrumentation-fastapi
opentelemetry-instrumentation-httpx
opentelemetry-exporter-otlp

python-jose[cryptography]
pydantic-settings
email-validator
python-multipart
APScheduler==3.11.2
</file>

<file path="services/api/app/core/config.py">
from __future__ import annotations

import json
from pathlib import Path
from typing import Annotated, Any, Literal

from pydantic import Field, field_validator
from pydantic_settings import BaseSettings, NoDecode, SettingsConfigDict

# services/api/app/core/config.py -> BASE_DIR == services/api
BASE_DIR = Path(__file__).resolve().parents[2]
APP_DIR = BASE_DIR / "app"
DEFAULT_FIXTURE_CATALOG_PATH = APP_DIR / "fixtures" / "catalog_fixture.json"


class Settings(BaseSettings):
    model_config = SettingsConfigDict(
        env_file=str(BASE_DIR / ".env"),
        extra="ignore",
        env_prefix="",
        case_sensitive=False,
    )

    # Environment
    env: str = Field(default="dev", validation_alias="ENV")

    # Application
    api_name: str = Field(default="shelfsync-api", validation_alias="API_NAME")
    log_level: str = Field(default="INFO", validation_alias="LOG_LEVEL")
    user_agent: str = Field(default="ShelfSync/0.1", validation_alias="USER_AGENT")

    # Database & cache
    database_url: str = Field(
        default="sqlite+pysqlite:///./shelfsync.db",
        validation_alias="DATABASE_URL",
    )
    test_database_url: str | None = Field(
        default=None, validation_alias="TEST_DATABASE_URL"
    )
    redis_url: str = Field(
        default="redis://localhost:6379/0", validation_alias="REDIS_URL"
    )
    availability_cache_ttl_secs: int = Field(
        default=300, validation_alias="AVAILABILITY_CACHE_TTL_SECS"
    )

    # Goodreads / ingestion
    goodreads_base_url: str | None = Field(
        default=None, validation_alias="GOODREADS_BASE_URL"
    )
    goodreads_fetch_timeout_secs: float = Field(
        default=15.0, validation_alias="GOODREADS_FETCH_TIMEOUT_SECS"
    )

    # Auth
    auth_secret_key: str = Field(
        default="dev-secret-key", validation_alias="AUTH_SECRET_KEY"
    )
    auth_algorithm: str = Field(default="HS256", validation_alias="AUTH_ALGORITHM")
    auth_access_token_ttl_minutes: int = Field(
        default=60, validation_alias="AUTH_ACCESS_TOKEN_TTL_MINUTES"
    )
    auth_cookie_name: str = Field(
        default="shelfsync_auth", validation_alias="AUTH_COOKIE_NAME"
    )
    auth_cookie_samesite: Literal["lax", "strict", "none"] = Field(
        default="lax", validation_alias="AUTH_COOKIE_SAMESITE"
    )
    auth_cookie_secure: bool = Field(
        default=False, validation_alias="AUTH_COOKIE_SECURE"
    )

    @field_validator("auth_cookie_samesite", mode="before")
    @classmethod
    def normalize_cookie_samesite(cls, v: Any) -> Literal["lax", "strict", "none"]:
        if v is None:
            return "lax"
        if not isinstance(v, str):
            raise TypeError("AUTH_COOKIE_SAMESITE must be a string")
        s = v.strip().lower()
        if s not in {"lax", "strict", "none"}:
            raise ValueError("AUTH_COOKIE_SAMESITE must be one of: lax, strict, none")
        return s  # type: ignore[return-value]

    # Demo mode
    demo_seed_email: str = Field(
        default="demo@shelfsync.app", validation_alias="DEMO_SEED_EMAIL"
    )
    demo_seed_password: str = Field(
        default="demo", validation_alias="DEMO_SEED_PASSWORD"
    )
    demo_login_enabled: bool = Field(
        default=True, validation_alias="DEMO_LOGIN_ENABLED"
    )

    # CORS
    cors_origins: Annotated[list[str], NoDecode] = Field(
        default_factory=lambda: ["http://localhost:3000"],
        validation_alias="CORS_ORIGINS",
    )

    @field_validator("cors_origins", mode="before")
    @classmethod
    def parse_cors_origins(cls, v: Any) -> list[str]:
        """
        Supported env formats:
          - JSON list: '["http://localhost:3000"]'
          - Bracket list (no quotes): '[http://localhost:3000, http://localhost:5173]'
          - Comma-separated: 'http://localhost:3000, http://localhost:5173'
          - '*' wildcard
        """
        if v is None:
            return []
        if isinstance(v, list):
            return [str(x).strip() for x in v if str(x).strip()]
        if not isinstance(v, str):
            raise TypeError("cors_origins must be a string or list of strings")

        s = v.strip()
        if not s:
            return []
        if s == "*":
            return ["*"]

        # Try JSON first for strings that look like JSON arrays
        if s.startswith("[") and s.endswith("]"):
            try:
                parsed = json.loads(s)
                if isinstance(parsed, list):
                    return [str(x).strip() for x in parsed if str(x).strip()]
            except json.JSONDecodeError:
                # Not JSON, treat as a simple bracket list without quotes
                inner = s[1:-1].strip()
                if not inner:
                    return []
                parts = [p.strip().strip('"').strip("'") for p in inner.split(",")]
                return [p for p in parts if p]

        # Comma-separated list
        parts = [p.strip() for p in s.split(",")]
        return [p for p in parts if p]

    # Catalog provider
    catalog_provider: str = Field(
        default="fixture", validation_alias="CATALOG_PROVIDER"
    )
    fixture_catalog_path: str = Field(
        default=str(DEFAULT_FIXTURE_CATALOG_PATH),
        validation_alias="FIXTURE_CATALOG_PATH",
    )
    google_books_api_key: str | None = Field(
        default=None, validation_alias="GOOGLE_BOOKS_API_KEY"
    )

    # Rate limiting
    rate_limit_window_seconds: int = Field(
        default=60, validation_alias="RATE_LIMIT_WINDOW_SECONDS"
    )
    rate_limit_books_per_window: int = Field(
        default=30, validation_alias="RATE_LIMIT_BOOKS_PER_WINDOW"
    )
    rate_limit_dashboard_per_window: int = Field(
        default=10, validation_alias="RATE_LIMIT_DASHBOARD_PER_WINDOW"
    )

    # Backwards-compat alias for older call sites
    @property
    def rate_limit_window_secs(self) -> int:
        return self.rate_limit_window_seconds

    # Workers
    worker_job_timeout_secs: int = Field(
        default=30, validation_alias="WORKER_JOB_TIMEOUT_SECS"
    )

    # OpenTelemetry
    otel_enabled: bool = Field(default=False, validation_alias="OTEL_ENABLED")
    otel_otlp_endpoint: str = Field(
        default="http://localhost:4317",
        validation_alias="OTEL_OTLP_ENDPOINT",
    )


settings = Settings()
</file>

<file path="README.md">
# ShelfSync

[![Live site](https://img.shields.io/website?label=Live%20site&style=flat-square&url=https%3A%2F%2Fshelfsync-six.vercel.app)](https://shelfsync-six.vercel.app)
[![CI](https://github.com/josuejero/shelfsync/actions/workflows/ci.yml/badge.svg)](https://github.com/josuejero/shelfsync/actions/workflows/ci.yml)

Sync your Goodreads shelves with your public library catalog (Libby/OverDrive-style) availability - plus ‚Äúread next‚Äù recommendations and availability notifications.

> Portfolio note: this repo is intentionally built to demonstrate full-stack + data/ML-adjacent + DevOps fundamentals (typed API contracts, background jobs, caching, tests, CI, security scanning, observability hooks), with a clear path to production integrations.

---

## Table of contents

- [ShelfSync](#shelfsync)
  - [Table of contents](#table-of-contents)
  - [What it does](#what-it-does)
  - [Tech stack](#tech-stack)
  - [Architecture](#architecture)
  - [Project status](#project-status)
  - [Quickstart](#quickstart)
    - [Prereqs](#prereqs)
    - [1) Start infrastructure (Postgres + Redis + Goodreads mock)](#1-start-infrastructure-postgres--redis--goodreads-mock)
    - [2) Run the API](#2-run-the-api)
    - [3) Run the web app](#3-run-the-web-app)
    - [4) Optional: run a background worker (RQ)](#4-optional-run-a-background-worker-rq)
  - [Local development](#local-development)
    - [Import a sample Goodreads shelf](#import-a-sample-goodreads-shelf)
    - [Demo login (local/dev)](#demo-login-localdev)
  - [Configuration](#configuration)
  - [Testing](#testing)
    - [API (pytest + formatting)](#api-pytest--formatting)
    - [Web (lint/build/tests)](#web-lintbuildtests)
  - [Key engineering highlights](#key-engineering-highlights)
    - [Backend engineering](#backend-engineering)
    - [Data + ML-adjacent engineering](#data--ml-adjacent-engineering)
    - [DevOps / cloud engineering](#devops--cloud-engineering)
  - [Repo layout](#repo-layout)
  - [Roadmap](#roadmap)
  - [Docs](#docs)
  - [License](#license)

---

## What it does

ShelfSync helps you answer: ‚ÄúWhich books on my Goodreads shelves are available right now at my library?‚Äù

Core user flows:

- **Import** your shelf from Goodreads (CSV supported; RSS scaffolding exists).
- **Match** each shelf item to a library catalog entry (ISBN, exact metadata, and fuzzy matching with evidence).
- **Track availability** by format (ebook/audiobook), snapshot changes over time, and cache checks.
- **Recommend what to read next** using availability + hold pressure heuristics and your preferred formats.
- **Notify** you when an item becomes available (SSE stream backed by Redis Pub/Sub).

---

## Tech stack

**Frontend**
- Next.js (App Router) + React + TypeScript
- Tailwind CSS

**Backend**
- FastAPI + Pydantic + Uvicorn
- Postgres + SQLAlchemy + Alembic
- Redis + RQ (background jobs) + Redis Pub/Sub (SSE event streams)

**DevOps / Quality**
- Docker + Docker Compose (local infra + API container)
- GitHub Actions CI + CodeQL + Dependabot
- Optional OpenTelemetry instrumentation (FastAPI + httpx)

---

## Architecture

High-level request and data flow:

```txt
Next.js UI
  |
  |  (HTTP + cookies)
  v
FastAPI API  --------------------->  Postgres (normalized entities)
  |  \                                   ^
  |   \                                  |
  |    \-> Redis (cache, job queue, pubsub) 
  |
  +-> Provider adapters (catalog + availability)
        - fixture provider (today)
        - OverDrive/Libby provider (placeholder)
```

Two ‚Äúprovider‚Äù concepts are separated on purpose:

* **Catalog provider**: search and retrieve candidate books for matching.
* **Availability provider**: fetch availability signals and write snapshots.

This keeps matching logic testable and makes it easier to swap real integrations later.

---

## Project status

Implemented (working, tested):

* [x] Cookie-based auth (JWT) with password hashing
* [x] Goodreads CSV parsing + idempotent upsert into `shelf_items`
* [x] Goodreads RSS parsing utilities (fetch + parse)
* [x] Matching engine (ISBN/exact/fuzzy scoring) with match evidence
* [x] Availability snapshots + Redis caching helpers
* [x] ‚ÄúRead next‚Äù scoring
* [x] Notifications model + SSE stream (Redis Pub/Sub)
* [x] CI (web build/lint; API checks) + CodeQL + Dependabot

In progress / intentionally stubbed for phased delivery:

* [ ] Wire background jobs to run RSS sync + matching refresh end-to-end
* [ ] Real OverDrive/Libby integration behind provider interface
* [ ] Normalize all API routes under `/v1` (one route is currently unversioned)

---

## Quickstart

### Prereqs

* Docker + Docker Compose
* Node.js 24+
* Python (the API Docker image uses Python 3.14; local dev works with modern Python 3.x)

### 1) Start infrastructure (Postgres + Redis + Goodreads mock)

From repo root:

```bash
make infra-up
```


Ports (local):

* Postgres: `localhost:5432`
* Redis: `localhost:6379`
* Goodreads mock (Prism): `localhost:4010`

### 2) Run the API

```bash
cd services/api
cp .env.example .env

python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt -r requirements-dev.txt

./bin/alembic upgrade head
uvicorn app.main:app --reload --port 8000
```

Sanity check:

```bash
curl http://localhost:8000/health
```

FastAPI docs:

* Swagger UI: `http://localhost:8000/docs`

### 3) Run the web app

```bash
cd apps/web
cp .env.local.example .env.local

npm ci
npm run dev
```

Open `http://localhost:3000`.

### 4) Optional: run a background worker (RQ)

Some API actions enqueue jobs; to execute them, run a worker in a separate terminal:

```bash
cd services/api
source .venv/bin/activate

# Connect to the same Redis from docker compose:
rq worker -u redis://localhost:6379/0
```

---

## Local development

### Import a sample Goodreads shelf

Use the built-in mock export:

* UI: **Settings -> Goodreads -> Upload CSV**
* File: `mock/goodreads/export.csv`

You should then see imported items listed in the Goodreads settings page.

### Demo login (local/dev)

If `DEMO_LOGIN_ENABLED=true` in `services/api/.env`, you can log in with:

* Email: `demo@example.com`
* Password: anything (local/dev only)

---

## Configuration

API config lives in `services/api/.env` (see `.env.example`).

Common knobs:

* `DATABASE_URL`, `REDIS_URL`
* `AUTH_SECRET_KEY` (change for anything beyond local)
* `CATALOG_PROVIDER=fixture` (demo mode)
* `FIXTURE_CATALOG_PATH=app/fixtures/catalog_fixture.json`
* `AVAILABILITY_CACHE_TTL_SECS=300`
* `OTEL_ENABLED=false` (set true + configure OTLP exporter env vars to enable tracing)

Web config lives in `apps/web/.env.local`:

* `NEXT_PUBLIC_API_BASE_URL=http://localhost:8000`

---

## Testing

### API (pytest + formatting)

```bash
make api-test
```

Or:

```bash
cd services/api
source .venv/bin/activate
pytest
```

### Web (lint/build/tests)

```bash
make web-check
```

Or:

```bash
cd apps/web
npm run lint
npm run build
npm run test
```

---

## Key engineering highlights

This section is written for hiring managers across SWE, ML/data, and DevOps tracks.

### Backend engineering

* Clean separation between **domain services** (matching, scoring, import) and **transport** (FastAPI routes).
* Postgres schema supports idempotent ingestion and historical availability tracking.
* Rate limiting hooks and request tracing hooks are built in.

### Data + ML-adjacent engineering

* Matching supports:

  * Identifier matches (ISBN)
  * Exact title/author
  * Fuzzy scoring with explainable evidence (useful for debugging + future model evaluation)
* ‚ÄúRead next‚Äù scoring uses availability tiers + hold pressure heuristics with user-preferred formats.

### DevOps / cloud engineering

* Dockerized API with local Compose dependencies.
* CI + security scanning:

  * GitHub Actions workflow(s)
  * CodeQL for JS + Python
  * Dependabot for npm/pip/actions
* Optional OpenTelemetry instrumentation to export traces to an OTLP collector.

---

## Repo layout

```txt
apps/web        Next.js UI
services/api    FastAPI backend
infra           Docker Compose (Postgres, Redis, mock services, API container)
docs            Architecture notes + ADRs
mock            Sample Goodreads RSS/CSV fixtures
```

---

## Roadmap

Near-term (most valuable for real users):

* RSS sync: implement the end-to-end worker job using the existing fetch + parse utilities.
* Matching refresh: connect the job to the existing matching + persistence services.
* OverDrive/Libby provider: implement real catalog + availability calls behind the provider interfaces.
* Production deployment: add deployment manifests and secrets strategy.

---

## Docs

* `docs/architecture.md`
* `docs/adr/0001-tech-stack.md`
* `docs/adr/0002-goodreads-ingestion.md`

---

## License

See `LICENSE`.
</file>

<file path="services/api/app/workers/jobs.py">
from __future__ import annotations

import asyncio
import importlib
import inspect
import logging
from datetime import datetime, timezone
from typing import Any

from app.crud.availability import upsert_snapshots
from app.crud.shelf_items import list_shelf_items_for_user
from app.crud.sync_runs import (
    get_sync_run,
    set_sync_run_failed,
    set_sync_run_running,
    set_sync_run_succeeded,
    update_progress,
)
from app.db.session import SessionLocal
from app.models.shelf_item import ShelfItem
from app.providers.factory import get_provider as get_availability_provider
from sqlalchemy import select
from sqlalchemy.orm import Session

logger = logging.getLogger(__name__)


def utcnow() -> datetime:
    return datetime.now(timezone.utc)


def _try_publish(fn_name: str, **kwargs: Any) -> None:
    """
    Publish events if an implementation exists, without hard imports.

    Tries modules in order, and no-ops if not found. This keeps mypy happy and
    avoids runtime import explosions when optional publishing code isn't present.
    """
    for module_name in ("app.workers.events", "app.workers.publish"):
        try:
            mod = importlib.import_module(module_name)
        except Exception:
            continue
        fn = getattr(mod, fn_name, None)
        if callable(fn):
            try:
                fn(**kwargs)
            except Exception:
                logger.exception(
                    "publish failed", extra={"fn": fn_name, "module": module_name}
                )
            return


def publish_notification_event(*, user_id: str, payload: dict[str, Any]) -> None:
    _try_publish("publish_notification_event", user_id=user_id, payload=payload)


def publish_sync_event(
    *, user_id: str, run_id: str | None, type_: str, payload: dict[str, Any]
) -> None:
    _try_publish(
        "publish_sync_event",
        user_id=user_id,
        run_id=run_id,
        type_=type_,
        payload=payload,
    )


def availability_refresh_job(sync_run_id: str) -> None:
    db: Session = SessionLocal()
    run = None
    try:
        run = get_sync_run(db, run_id=sync_run_id)
        if run is None:
            raise RuntimeError("sync run not found")

        items = list_shelf_items_for_user(db, user_id=run.user_id)
        total = len(items)

        set_sync_run_running(db, run=run, total=total)

        provider = get_availability_provider(db, user_id=run.user_id)

        processed = 0
        batch_size = 50

        for i in range(0, total, batch_size):
            chunk = items[i : i + batch_size]
            results = provider.availability_bulk(chunk)

            created = upsert_snapshots(db, user_id=run.user_id, results=results)
            processed += len(chunk)

            update_progress(db, run=run, current=processed)  # typically commits

            if created:
                # Hydrate titles for nicer live notifications
                ids = [c.shelf_item_id for c in created]
                rows = (
                    db.execute(
                        select(ShelfItem.id, ShelfItem.title).where(
                            ShelfItem.id.in_(ids)
                        )
                    )
                    .tuples()
                    .all()
                )
                id_to_title: dict[str, str] = {sid: title for sid, title in rows}

                for c in created:
                    publish_notification_event(
                        user_id=run.user_id,
                        payload={
                            "id": c.id,
                            "shelf_item_id": c.shelf_item_id,
                            "title": id_to_title.get(c.shelf_item_id, ""),
                            "format": c.format,
                        },
                    )

            publish_sync_event(
                user_id=run.user_id,
                run_id=run.id,
                type_="availability_progress",
                payload={"current": processed, "total": total},
            )

        set_sync_run_succeeded(db, run=run)
        publish_sync_event(
            user_id=run.user_id,
            run_id=run.id,
            type_="availability_succeeded",
            payload={"current": processed, "total": total},
        )

    except Exception as e:
        logger.exception("availability_refresh_job failed")
        try:
            if run is not None:
                set_sync_run_failed(db, run=run, message=str(e))
                publish_sync_event(
                    user_id=run.user_id,
                    run_id=run.id,
                    type_="availability_failed",
                    payload={"error": str(e)},
                )
        except Exception:
            logger.exception("failed to mark sync run as failed")
        raise
    finally:
        db.close()


def refresh_matching_for_user(user_id: str) -> dict[str, int]:
    """
    TODO: Implement matching refresh using app/services/matching/*.

    This function is kept so imports and job registration remain stable,
    but matching refresh is currently a no-op.
    """
    db: Session = SessionLocal()
    try:
        items = list_shelf_items_for_user(db, user_id=user_id)
        total = len(items)
        return {"matched": 0, "total": total}
    finally:
        db.close()


def sync_goodreads_rss(source_id: str) -> None:
    """
    TODO: Implement RSS sync through your ingestion pipeline.

    Kept as a stub so job registration doesn't break.
    """
    # If you later add an ingestion entrypoint, call it here.
    return
</file>

<file path="services/api/tests/conftest.py">
from __future__ import annotations

import os
from pathlib import Path
from typing import Generator

import pytest
from alembic.command import upgrade
from alembic.config import Config
from app.models import Base
from fastapi.testclient import TestClient
from sqlalchemy import create_engine, event, text
from sqlalchemy.orm import Session, sessionmaker


def _guess_test_database_url() -> str:
    """Prefer TEST_DATABASE_URL; fall back to DATABASE_URL; if Postgres, append _test."""
    test_url = os.getenv("TEST_DATABASE_URL")
    if test_url:
        return test_url

    db_url = os.getenv("DATABASE_URL")
    if not db_url:
        return "sqlite+pysqlite:///:memory:"

    if db_url.endswith("_test") or "shelfsync_test" in db_url:
        return db_url

    if db_url.startswith("postgresql") and "/" in db_url.rsplit("@", 1)[-1]:
        prefix, dbname = db_url.rsplit("/", 1)
        if dbname:
            return f"{prefix}/{dbname}_test"

    return db_url


_TEST_DB_URL = _guess_test_database_url()
os.environ.setdefault("DATABASE_URL", _TEST_DB_URL)


def _alembic_cfg(db_url: str) -> Config:
    base_dir = Path(__file__).resolve().parents[1]  # services/api
    alembic_ini = base_dir / "alembic.ini"
    cfg = Config(str(alembic_ini))
    cfg.set_main_option("sqlalchemy.url", db_url)
    return cfg


def _truncate_all_tables(engine) -> None:
    """Delete rows from every model table so tests always start clean."""
    with engine.begin() as conn:
        if engine.dialect.name == "sqlite":
            conn.execute(text("PRAGMA foreign_keys = OFF"))

        for table in reversed(Base.metadata.sorted_tables):
            conn.execute(table.delete())

        if engine.dialect.name == "sqlite":
            conn.execute(text("PRAGMA foreign_keys = ON"))


@pytest.fixture(scope="session")
def engine():
    url = _TEST_DB_URL

    if url.startswith("sqlite"):
        from sqlalchemy.pool import StaticPool

        eng = create_engine(
            url,
            connect_args={"check_same_thread": False},
            poolclass=StaticPool,
        )
    else:
        eng = create_engine(url, pool_pre_ping=True)

    connection = eng.connect()
    cfg = _alembic_cfg(url)
    cfg.attributes["connection"] = connection
    try:
        upgrade(cfg, "head")
    finally:
        connection.close()

    yield eng
    eng.dispose()


@pytest.fixture()
def db_session(engine) -> Generator[Session, None, None]:
    _truncate_all_tables(engine)
    connection = engine.connect()
    transaction = connection.begin()

    TestingSessionLocal = sessionmaker(
        bind=connection, autoflush=False, autocommit=False
    )
    session: Session = TestingSessionLocal()

    session.begin_nested()

    @event.listens_for(session, "after_transaction_end")
    def _restart_savepoint(sess: Session, trans) -> None:  # type: ignore[no-redef]
        if (
            trans.nested and not trans._parent.nested
        ):  # pyright: ignore[reportPrivateUsage]
            sess.begin_nested()

    try:
        yield session
    finally:
        session.close()
        transaction.rollback()
        connection.close()


@pytest.fixture()
def client(db_session: Session) -> Generator[TestClient, None, None]:
    from app.db.session import get_db
    from app.main import app

    def _override_get_db() -> Generator[Session, None, None]:
        yield db_session

    app.dependency_overrides[get_db] = _override_get_db
    with TestClient(app) as c:
        yield c
    app.dependency_overrides.clear()
</file>

<file path="services/api/app/main.py">
from __future__ import annotations

from app.api.router import api_router
from app.api.routes.notifications import router as notifications_router
from app.core.config import settings
from app.core.otel import init_otel
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware

app = FastAPI(title=settings.api_name)

app.add_middleware(
    CORSMiddleware,
    allow_origins=settings.cors_origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

app.include_router(api_router)
app.include_router(notifications_router)

init_otel(app)
</file>

</files>
